{
  "title": "å³æ’å³ç”¨å‹ã€åŸºäºç›¸å…³æ€§å‰ªæã€‘å®Œæ•´ä»£ç ",
  "description": "é¦–å…ˆæ„Ÿè°¢JG21054å’Œ KZ79256 å’Œ  HQ17963 ä»¥åŠ EC12049 ä¸‰ä½å¤§ä½¬çš„æºç ï¼Œæ€è·¯å’Œåˆ†äº«ã€‚å…·ä½“çš„æ–‡ç« å°±ä¸æ”¾é“¾æ¥äº†ã€‚å› ä¸ºæˆ‘æ˜¯å°ç™½ çœ‹å‰è¾ˆçš„ä»£ç ä¸èƒ½ç›´æ¥ä½¿ç”¨ï¼Œä¹Ÿçœ‹åˆ°è®ºå›å¥½å¤šå’Œæˆ‘ä¸€æ ·çš„ åªçœ‹åˆ°ä»£ç ä¸èƒ½ç›´æ¥è·‘èµ·æ¥ï¼Œæˆ‘ç»“åˆå„ä½å¤§ä½¬çš„ä»£ç æ•´åˆäº†ä¸‹ ...",
  "post_body": "é¦–å…ˆæ„Ÿè°¢JG21054å’Œ KZ79256 å’Œ Â HQ17963 ä»¥åŠ EC12049 ä¸‰ä½å¤§ä½¬çš„æºç ï¼Œæ€è·¯å’Œåˆ†äº«ã€‚å…·ä½“çš„æ–‡ç« å°±ä¸æ”¾é“¾æ¥äº†ã€‚å› ä¸ºæˆ‘æ˜¯å°ç™½ çœ‹å‰è¾ˆçš„ä»£ç ä¸èƒ½ç›´æ¥ä½¿ç”¨ï¼Œä¹Ÿçœ‹åˆ°è®ºå›å¥½å¤šå’Œæˆ‘ä¸€æ ·çš„ åªçœ‹åˆ°ä»£ç ä¸èƒ½ç›´æ¥è·‘èµ·æ¥ï¼Œæˆ‘ç»“åˆå„ä½å¤§ä½¬çš„ä»£ç æ•´åˆäº†ä¸‹Â  å¯ä»¥æ­£å¸¸è·‘èµ·æ¥ï¼Œå¦‚æœ‰ä¸å¯¹çš„åœ°æ–¹æ¬¢è¿æ‰¹è¯„æŒ‡æ­£ã€‚\nimport pandas as pd\nimport time\nimport os\nimport logging as logger Â # ä¿æŒæ—¥å¿—æ¨¡å—åˆ«å\nimport requests\nfrom typing import List, Dict, Any, Optional\n# å…³é”®ä¿®æ”¹ï¼šä»…ä»machine_libå¯¼å…¥loginï¼Œï¼‰\nfrom machine_lib import login\n# -------------------------- 1. ç§»é™¤åŸæœ‰çš„cfgç±»å’Œè‡ªå®šä¹‰login()å‡½æ•° --------------------------\n# -------------------------- 2. Alphaæ•°æ®è·å–æ¨¡å—ï¼ˆä¿æŒä¸å˜ï¼‰ --------------------------\ndef fetch_alphas(url: str, session: requests.Session, max_retry: int = 3) -> List[Dict[str, Any]]:\n\"\"\"è¯·æ±‚Alphaæ•°æ®ï¼Œå¸¦é‡è¯•é€»è¾‘ï¼Œè¿”å›ç»“æœåˆ—è¡¨\"\"\"\nfor retry in range(max_retry):\nresponse = session.get(url)\nif response.status_code == 200:\nbreak\nlogger.warning(f\"âŒ è·å–æ•°æ®å¤±è´¥ (URL: {url})ï¼ŒçŠ¶æ€ç : {response.status_code}ï¼Œé‡è¯•æ¬¡æ•°: {retry + 1}\")\ntime.sleep(3)\nelse:\nlogger.error(f\"âŒ è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retry})ï¼Œè·å–æ•°æ®å¤±è´¥\")\nreturn []\nreturn response.json().get(\"results\", [])\ndef process_alpha(alpha: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n\"\"\"å¤„ç†å•ä¸ªAlphaæ•°æ®ï¼šè¿‡æ»¤æ— æ•ˆæ•°æ®ã€è°ƒæ•´è¡¨è¾¾å¼ä¸decay\"\"\"\nrequired_fields = [\"id\", \"dateCreated\", \"is\", \"settings\", \"regular\"]\nfor field in required_fields:\nif field not in alpha:\nlogger.warning(f\"âš ï¸ Alphaæ•°æ®ç¼ºå°‘å­—æ®µ {field}ï¼Œè·³è¿‡å¤„ç†\")\nreturn None\nalpha_id = alpha[\"id\"]\ndate_created = alpha[\"dateCreated\"]\nsharpe = alpha[\"is\"].get(\"sharpe\", 0)\nfitness = alpha[\"is\"].get(\"fitness\", 0)\nturnover = alpha[\"is\"].get(\"turnover\", 0)\nmargin = alpha[\"is\"].get(\"margin\", 0)\nlong_count = alpha[\"is\"].get(\"longCount\", 0)\nshort_count = alpha[\"is\"].get(\"shortCount\", 0)\ndecay = alpha[\"settings\"].get(\"decay\", 0)\nexpression = alpha[\"regular\"].get(\"code\", \"\")\n# è¿‡æ»¤æŒä»“æ•°é‡è¿‡å°‘çš„Alpha\nif (long_count + short_count) <= 100:\nlogger.debug(f\"âš ï¸ Alpha {alpha_id} æŒä»“æ•°é‡è¿‡å°‘ ({long_count + short_count} <= 100)ï¼Œè·³è¿‡\")\nreturn None\n# è°ƒæ•´è´ŸSharpeçš„è¡¨è¾¾å¼\nif sharpe < 0:\nexpression = f\"-{expression}\"\n# æ„å»ºåŸºç¡€è®°å½•\nrecord: Dict[str, Any] = {\n\"alpha_id\": alpha_id,\n\"expression\": expression,\n\"sharpe\": sharpe,\n\"turnover\": turnover,\n\"fitness\": fitness,\n\"margin\": margin,\n\"date_created\": date_created,\n\"decay\": decay\n}\n# æ ¹æ®æ¢æ‰‹ç‡è°ƒæ•´decay\nif turnover > 0.7:\nrecord[\"decay\"] = decay * 4\nelif turnover > 0.6:\nrecord[\"decay\"] = decay * 3 + 3\nelif turnover > 0.5:\nrecord[\"decay\"] = decay * 3\nelif turnover > 0.4:\nrecord[\"decay\"] = decay * 2\nelif turnover > 0.35:\nrecord[\"decay\"] = decay + 4\nelif turnover > 0.3:\nrecord[\"decay\"] = decay + 2\nreturn record\ndef get_alphas(\nsession: requests.Session,\nstart_time: str,\nend_time: str,\nsharpe_th: float,\nfitness_th: float,\nregion: str = \"\",\nmargin: str = \"\",\nreturns: str = \"\",\nturnover: str = \"\",\norder: str = \"-dateCreated\",\npage_size: int = 100,\nmax_page: int = -1,\nfilter_failed: bool = False,\nenable_opt: bool = False\n) -> List[List[Any]]:\n\"\"\"æ‰¹é‡è·å–å¹¶å¤„ç†Alphaæ•°æ®ï¼Œè¿”å›ç»“æ„åŒ–åˆ—è¡¨\"\"\"\nbase_url = \"https://api.worldquantbrain.com/users/self/alphas\"\nquery_list: List[str] = [\n\"status=UNSUBMITTED%1FIS_FAIL\",\n\"hidden=false\",\n\"type=REGULAR\",\n# åŒ—äº¬æ—¶é—´è½¬çº½çº¦æ—¶é—´ï¼ˆç¡®ä¿æ—¶åŒºæ­£ç¡®ï¼‰\nf'dateCreated>={pd.Timestamp(start_time).floor(\"s\").tz_localize(\"Asia/Shanghai\").tz_convert(\"America/New_York\").isoformat()}',\nf'dateCreated<{pd.Timestamp(end_time).floor(\"s\").tz_localize(\"Asia/Shanghai\").tz_convert(\"America/New_York\").isoformat()}',\nf\"limit={page_size}\"\n]\n# è¿½åŠ è¿‡æ»¤æ¡ä»¶ï¼ˆéç©ºæ‰æ·»åŠ ï¼‰\nif region:\nquery_list.append(f\"settings.region={region}\")\nif order:\nquery_list.append(f\"order={order}\")\nif margin:\nquery_list.append(f\"is.margin>{margin}\")\nif returns:\nquery_list.append(f\"is.returns>{returns}\")\nif turnover:\nquery_list.append(f\"is.turnover>{turnover}\")\n# è¿½åŠ Sharpe/Fitnessè¿‡æ»¤ä¸åˆ†é¡µå‚æ•°\nquery_list.extend([\n\"is.sharpe{sharpe_op}{sharpe_th}\",\n\"is.fitness{fitness_op}{fitness_th}\",\n\"offset={offset}\"\n])\nquery_url = f\"{base_url}?{'&'.join(query_list)}\"\nconditions = {\n\"gt\": {\"sharpe_op\": \">\", \"sharpe_th\": f\"{sharpe_th}\", \"fitness_op\": \">\", \"fitness_th\": f\"{fitness_th}\"},\n\"lt\": {\"sharpe_op\": \"<\", \"sharpe_th\": f\"-{sharpe_th}\", \"fitness_op\": \"<\", \"fitness_th\": f\"-{fitness_th}\"}\n}\nrecord_list: List[Dict[str, Any]] = []\nfor key in conditions.keys():\nif not enable_opt and key == \"lt\":\ncontinue\noffset = 0\nn_page = 0\nwhile True:\nn_page += 1\nlogger.info(f\"ğŸ“„ å¤„ç†ç¬¬ {n_page} é¡µï¼Œåç§»é‡: {offset}\")\n# æ ¼å¼åŒ–è¯·æ±‚URL\nurl = query_url.format(offset=offset, **conditions[key])\nlogger.debug(f\"ğŸ” è¯·æ±‚URL: {url}\")\n# è·å–Alphaåˆ—è¡¨\nalpha_list = fetch_alphas(url, session)\nif not alpha_list:\nlogger.info(f\"ğŸ“Œ æ— æ›´å¤šæ•°æ®ï¼ˆç¬¬ {n_page} é¡µä¸ºç©ºï¼‰\")\nbreak\n# è¶…è¿‡æœ€å¤§é¡µæ•°é™åˆ¶\nif max_page > 0 and n_page >= max_page:\nlogger.info(f\"ğŸ“Œ å·²è¾¾åˆ°æœ€å¤§é¡µæ•° {max_page}ï¼Œåœæ­¢æŸ¥è¯¢\")\nbreak\n# å¤„ç†Alphaæ•°æ®\nalpha_df = pd.DataFrame(alpha_list)\nif filter_failed:\n# è¿‡æ»¤ISé˜¶æ®µæ£€æŸ¥å¤±è´¥çš„Alpha\nchecks_mask = alpha_df[\"is\"].apply(\nlambda x: not any(pd.DataFrame(x[\"checks\"])[\"result\"] == \"FAIL\")\n)\nalpha_df = alpha_df[checks_mask]\nlogger.info(f\"âš¡ è¿‡æ»¤åå‰©ä½™Alphaæ•°é‡: {len(alpha_df)}\")\nif not alpha_df.empty:\nprocessed_records = alpha_df.apply(process_alpha, axis=1).dropna()\nrecord_list.extend(processed_records.tolist())\nlogger.info(f\"âœ… ç´¯è®¡å¤„ç†æœ‰æ•ˆAlphaæ•°é‡: {len(record_list)}\")\noffset += page_size\n# æ•´ç†è¾“å‡ºæ ¼å¼\nfields = [\"alpha_id\", \"expression\", \"sharpe\", \"turnover\", \"fitness\", \"margin\", \"date_created\", \"decay\"]\noutput = [\n[rec[field] for field in fields]\nfor rec in record_list\nif rec is not None\n]\nlogger.info(f\"ğŸ‰ æœ€ç»ˆè·å–æœ‰æ•ˆAlphaæ€»æ•°: {len(output)}\")\nreturn output\n# -------------------------- 3. PNLåˆæ³•æ€§æ£€æµ‹æ¨¡å—ï¼ˆä¿æŒä¸å˜ï¼‰ --------------------------\ndef wait_get(url: str, sess: requests.Session, max_retries: int = 10) -> requests.Response:\n\"\"\"å¸¦é‡è¯•ä¸Retry-Afterç­‰å¾…çš„GETè¯·æ±‚\"\"\"\nretries = 0\nwhile retries < max_retries:\nresponse = sess.get(url)\nretry_after = response.headers.get(\"Retry-After\", 0)\nif retry_after != 0:\nlogger.info(f\"â³ æœåŠ¡ç«¯è¦æ±‚ç­‰å¾… {retry_after} ç§’ï¼Œé‡è¯•ä¸­...\")\ntime.sleep(float(retry_after))\ncontinue\nif response.status_code < 400:\nreturn response\nelse:\nwait_time = 2 ** retries\nlogger.warning(f\"âŒ è¯·æ±‚å¤±è´¥ï¼ˆçŠ¶æ€ç : {response.status_code}ï¼‰ï¼Œ{wait_time} ç§’åé‡è¯•\")\ntime.sleep(wait_time)\nretries += 1\nlogger.error(f\"âŒ è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œè¯·æ±‚å¤±è´¥: {url}\")\nraise requests.exceptions.HTTPError(f\"Max retries exceeded for URL: {url}\")\ndef check_consecutive_non_zero_values(alpha_id: str, data: List[List[Any]], required_streak: int = 200) -> bool:\n\"\"\"æ£€æŸ¥PNLæ•°æ®æ˜¯å¦åˆæ³•ï¼šæ— è¿ç»­200ä¸ªéé›¶å€¼ + æ— å…¨é›¶åˆ—\"\"\"\nif not data or len(data) < required_streak:\nreturn True\ndef _check_column(column_data: List[Any]) -> bool:\n\"\"\"æ£€æŸ¥å•åˆ—æ˜¯å¦æœ‰è¿ç»­è¶…æ ‡éé›¶å€¼\"\"\"\nif len(column_data) < required_streak:\nreturn True\ncurrent_streak = 0\ncurrent_value = None\nfor val in column_data:\nif val != 0:\nif val == current_value:\ncurrent_streak += 1\nelse:\ncurrent_value = val\ncurrent_streak = 1\nelse:\ncurrent_value = None\ncurrent_streak = 0\nif current_streak >= required_streak:\nreturn False\nreturn True\n# æå–å…³é”®åˆ—æ•°æ®\ncol1, col2 = [], []\nfor row in data:\nif len(row) >= 3:\ncol1.append(row[1])\ncol2.append(row[2])\n# æ£€æŸ¥å…¨é›¶åˆ—\nif col1 and col2:\nif all(v == 0 for v in col1) or all(v == 0 for v in col2):\nlogger.warning(f\"âŒ Alpha {alpha_id} PNLå­˜åœ¨å…¨é›¶åˆ—ï¼Œåˆ¤å®šä¸åˆæ³•\")\nreturn False\n# æ£€æŸ¥è¿ç»­éé›¶å€¼\nif not _check_column(col1) or not _check_column(col2):\nlogger.warning(f\"âŒ Alpha {alpha_id} PNLå­˜åœ¨è¿ç»­{required_streak}ä¸ªç›¸åŒéé›¶å€¼ï¼Œåˆ¤å®šä¸åˆæ³•\")\nreturn False\nlogger.info(f\"âœ… Alpha {alpha_id} PNLæ£€æµ‹é€šè¿‡\")\nreturn True\ndef get_alpha_pnl_legal(alpha_id: str, sess: requests.Session) -> bool:\n\"\"\"å•ä¸ªAlphaçš„PNLåˆæ³•æ€§æ£€æµ‹\"\"\"\npnl_url = f\"https://api.worldquantbrain.com/alphas/{alpha_id}/recordsets/pnl\"\ntry:\nresponse = wait_get(pnl_url, sess)\npnl_data = response.json()\nrecords = pnl_data.get(\"records\", [])\nreturn check_consecutive_non_zero_values(alpha_id, records)\nexcept Exception as e:\nlogger.error(f\"âš ï¸ Alpha {alpha_id} PNLæ£€æµ‹å¼‚å¸¸: {str(e)}ï¼Œæš‚åˆ¤å®šä¸åˆæ³•\")\nreturn False\ndef get_alpha_pnl_legal_list(fo_tracker: List[List[Any]], sess: requests.Session) -> List[List[Any]]:\n\"\"\"æ‰¹é‡æ£€æµ‹Alphaçš„PNLåˆæ³•æ€§ï¼Œè¿”å›åˆæ³•åˆ—è¡¨\"\"\"\nlogger.info(f\"ğŸ“Š å¼€å§‹PNLåˆæ³•æ€§æ£€æµ‹ï¼Œå¾…æ£€æµ‹Alphaæ•°é‡: {len(fo_tracker)}\")\nlegal_alphas = [fo for fo in fo_tracker if get_alpha_pnl_legal(fo[0], sess)]\nlogger.info(f\"ğŸ‰ PNLæ£€æµ‹å®Œæˆï¼Œåˆæ³•Alphaæ•°é‡: {len(legal_alphas)}ï¼Œå‰”é™¤æ•°é‡: {len(fo_tracker) - len(legal_alphas)}\")\nreturn legal_alphas\n# -------------------------- 4. ä¸»æ‰§è¡Œæµç¨‹ï¼ˆç™»å½•é€»è¾‘ä¸è„šæœ¬Aä¸€è‡´ï¼‰ --------------------------\nif __name__ == \"__main__\":\n# å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨machine_lib.login()è·å–ä¼šè¯ï¼ˆä¸è„šæœ¬Açš„s = login()å®Œå…¨ä¸€è‡´ï¼‰\nsession = login()\n# åˆå§‹åŒ–æ—¥å¿—ï¼ˆç¡®ä¿åœ¨ç™»å½•åã€ä½¿ç”¨æ—¥å¿—å‰é…ç½®ï¼Œé¿å…é‡å¤é…ç½®ï¼‰\nif not logger.getLogger().hasHandlers():\nlogger.basicConfig(level=logger.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\nif not session:\nlogger.error(\"âŒ ç™»å½•å¤±è´¥ï¼Œè„šæœ¬ç»ˆæ­¢\")\nexit(1)\ntry:\n# æ­¥éª¤2ï¼šè·å–Alphaæ•°æ®ï¼ˆå‚æ•°å¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰\nfo_tracker = get_alphas(\nsession=session, Â  Â  Â  Â  Â  Â # ä¼ å…¥machine_lib.login()è¿”å›çš„ä¼šè¯\nstart_time=\"2025-07-12\", Â  Â  # å¼€å§‹æ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰\nend_time=\"2025-08-18\", Â  Â  Â  # ç»“æŸæ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰\nsharpe_th=1.0, Â  Â  Â  Â  Â  Â  Â  # Sharpeé˜ˆå€¼\nfitness_th=0.57, Â  Â  Â  Â  Â  Â  Â # Fitnessé˜ˆå€¼\nregion=\"GLB\", Â  Â  Â  Â  Â  Â  Â  Â # åœ°åŒºè¿‡æ»¤ï¼ˆUSAï¼‰\npage_size=200, Â  Â  Â  Â  Â  Â  Â  # æ¯é¡µæ•°é‡\nmax_page=-1, Â  Â  Â  Â  Â  Â  Â  Â  # æœ€å¤§é¡µæ•°ï¼ˆ-1ä¸ºå…¨éƒ¨ï¼‰\nfilter_failed=True, Â  Â  Â  Â  Â # è¿‡æ»¤ISé˜¶æ®µå¤±è´¥é¡¹\nenable_opt=False Â  Â  Â  Â  Â  Â  # ä¸å¼€å¯è´ŸSharpeä¼˜åŒ–\n)\n# æ­¥éª¤3ï¼šPNLåˆæ³•æ€§æ£€æµ‹\nif not fo_tracker:\nlogger.warning(\"âš ï¸ æ— æœ‰æ•ˆAlphaæ•°æ®ï¼Œè·³è¿‡PNLæ£€æµ‹\")\nelse:\nf_num = len(fo_tracker)\nprint(f\"\\nğŸ“¢ {f_num} ä¸ªAlpha è¿›è¡ŒPNLåˆæ³•æ£€æµ‹ï¼Œè¯·è€å¿ƒç­‰å¾…...\")\nfo_tracker = get_alpha_pnl_legal_list(fo_tracker, session)\nprint(f\"ğŸ“¢ {f_num - len(fo_tracker)} ä¸ªä¸åˆæ³•çš„PNLå·²è¢«å‰”é™¤ï¼Œå‰©ä½™åˆæ³•Alphaæ•°é‡: {len(fo_tracker)}\")\nexcept Exception as e:\nlogger.error(f\"âŒ è„šæœ¬æ‰§è¡Œå¼‚å¸¸: {str(e)}\", exc_info=True)\nfinally:\n# å…³é—­ä¼šè¯\nsession.close()\nlogger.info(\"ğŸ”š ä¼šè¯å·²å…³é—­ï¼Œè„šæœ¬æ‰§è¡Œç»“æŸ\")",
  "post_comments": []
}