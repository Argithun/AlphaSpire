{
  "title": "【有奖】Alpha模板征文：分享你独到的Idea和Implementation！",
  "description": "一句话总结该活动：直接在评论区评论，分享你的模板。  被审核通过者将获得BRAIN纪念品一份（下图背包）优秀分享更有机会将获得50USD的一次性津贴。  活动时间：即日起至8.31日23：59（以服务器时间为准）...",
  "post_body": "一句话总结该活动：直接在评论区评论，分享你的模板。\n被审核通过者将获得BRAIN纪念品一份（下图背包）\n优秀分享更有机会将获得50USD的一次性津贴。\n活动时间：即日起至8.31日23：59（以服务器时间为准）\n活动要求：参赛同学可发布多个模板参赛，\n必须展示下列所有元素\n。同一人可发布多条评论参赛，一个评论仅能放1个idea。同一人不可领多份奖励，但被发出的评论越多会更容易获得较多点赞。\n模板\n模板中的变量必须使用< />进行声明，不符合语法规则的评论无法参评\n需阐述具体变量赋值，如operator类别、数据集id、地区等\n阐述搜索空间的大小\n阐述模板的idea，implement细节（即哪步是数据处理，哪步是主信号）\n产出效果（回测：Alpha数量，可以仅展示比率）\n建议其他顾问未来尝试的探索方向\n再次强调，必须至少展示上述所有信息👆先到先得！有些模板过于类似的将不再被批复，建议大家快快抓住机会！",
  "post_comments": [
    "通过质量检查的背包获奖者：\nJB71859\n,\nWL27618\n,\nYK49234\n,\nCC85858\n, ZS59763,\nGZ81958\n, XX25820,\nAL13375\n,\nXC66172\n,\nKD86036\n,\nLM81527\n,\nSQ15289\n,\nYH82809\n,\nQX52484\n,\nJB53978\n,\nAK76468\n,\nAM12075\n,\nSG46247\n,\nPX70901\n,\nJG21054\n,\nTJ14150\n,\nSC23342\n,\nBA51127\n,\nML42552\n优胜奖获得者：\nJR23144, LH44620,\nYZ70114\n,\nLL49894\n,\nJL98779\n,\nLR93609\n示例：\n模板\niv_difference =\n<option_call/>\n-\n<option_put/>\n;\nstd_group = bucket(rank(historical_volatility_120), range=\"0.1,1,0.1\");\ndecorrelator = 1 / (1 + abs(ts_delta(iv_difference, 5)));\n<group_compareOP/>\n(iv_difference, std_group)  * decorrelator\n模板中的变量使用\n< />\n进行了声明，总共三个变量。\n具体变量1：<option_call/>: 可以包含所有和call有关的期权字段\n具体变量2：<option_put/>: 可以包含所有和call有关的期权字段\n具体变量3：<group_compareOP/>: 可以对组间进行计算的分组运算符，例如group_rank, group_zscore, group_neutralize等\n搜索空间的大小：以数据集option4为例的话，大约是230*230*9\n模板的idea，implement细节（即哪步是数据处理，哪步是主信号）：<group_compareOP/>(iv_difference, std_group) * decorrelator：对iv_difference在std_group内做分组运算（如group_rank），再乘以decorrelator，得到最终信号。其中，std_group = bucket(rank(historical_volatility_120), range=\"0.1,1,0.1\")：对历史波动率分rank后分桶，形成分组标签。decorrelator = 1 / (1 + abs(ts_delta(iv_difference, 5)))：用5期的iv_difference变化幅度做装饰，降低信号自相关性。\n产出效果：我尝试了X次回测，有20个可以提交的Alpha.\n建议其他顾问未来尝试的探索方向: 在不同市场、不同品种（如ETF期权、商品期权）上做迁移和泛化测试。\n如您无意参赛，也欢迎留言参与讨论：\n随手分享一个来自热心顾问的小小数据探索模板，他说这个称不上完整，但是很适合探索新数据。且不参与征文，故对格式就不严格要求了。👇代发内容如下\nzscore(ts_delta(rank(ts_zscore(datafield,60)),5))\n上文中的模板op可以进行替换，时间参数也应该针对不同数据集进行合理替换.",
    "Alpha模板Name：《多重平滑排名信号》\n模板\nranked_signal = ts_rank(\n<Earnings_data_field/>\n,\n<decay_1/>\n);\n<ts_statistical/>\n(ranked_signal,\n<decay_2/>\n)\n具体变量1：<Earnings_data_field/>: 可以包含所有EARNINGS的数据字段进行遍历\n具体变量2：<decay_1/>: 第一层平滑ts_rank的decay可以是5，10，20，60，120\n具体变量3：<decay_2/>: 第二层平滑ts_mean的decay可以是5，10，20，60，120\n具体变量4：<ts_statistical/>: 尝试了ts_mean, 但是觉得还有很多可以尝试，例如median，sum, max, last change\n搜索空间的大小：以数据集earnings2为例的话，大约是12 * 5 * 5\n模板的idea，implement细节：ts_mean(ts_rank(<Earnings_data_field/>, <decay_1/>), <decay_2/>)：首先ts_rank(<Earnings_data_field/>,<decay_1/>) 对原始EARNINGS数据进行时间序列排名（第一重平滑），将绝对数值转换为相对排名，消除量纲影响；然后ts_mean(ranked_signal, <decay_2/>) 对排名序列进行时间窗口均值平滑（第二重平滑），进一步降低信号噪声，确保输出稳定的Alpha信号。\n产出效果：我在尝试了2个数据集大概2000次的回测后，点亮了USA中earnings的塔，通过该模板提交了3个Atom的Alpha。\n该模板还未在EUR、ASI和GLB中运行，存在一定的潜力.第一步的ts_rank其实也可以抽象成其他的ts相关的运算符。感觉这个模板适合一些已经处理好的数据，因为如果数据不处理的话，第一步得到的东西会很抽象且不好用。建议大家在放入第一步前处理一下数据。\n也可以跟其他处理数据的模板结合在一起使用。",
    "模板\n<exceed op/>(<Arithmetic op/>(<option_call/>) , <Arithmetic op/>(<option_put/>))\n模板中的变量使用\n< />\n进行了声明，总共4个变量。\n具体变量1：<option_call/>: 可以包含所有和call有关的期权字段\n具体变量2：<option_put/>: 可以包含所有和call有关的期权字段\n具体变量3：<\nArithmetic op\n/>: 任何eltwise的变换operator(比如-1/x, log, signed_power, s_log_1p, tanh, arc_tan)等\n具体变量4：<\nexceed op\n/>: 任何表示前一个超出后一个的运算.(比如\"({data1})/({data2})\", \"({data1}-({data2}))\", \"({data1}-({data2}))/({data2})\",\n# 相对变化率, data1, data2是正数的变量时使用 \"({data1}-{data2})/({data1}/2+{data2}/2), ({data1}-{data2})/ts_std_dev({data2},252)”)等,\n搜索空间的大小：option8, option4 , pair个数 x arithmetic op个数 x 外面套的1,2,3阶运算符的个数\n模板的idea，implement细节（即哪步是数据处理，哪步是主信号）：\nArithmetic op的目的\n就是想尽量扩大搜索范围, call,put分别套了arithmetic op 后correlation下降不少, 有些信号看着还挺好\n产出效果： 可能option4数据集信号就是比较强, 随便跑都有很多产出\n建议其他顾问未来尝试的探索方向\n: AI\n我尝试过不同希腊字母\n,\n比如\ntheta vega\n配对\n,\n但是效果一般\n,\n感觉还是应该尝试不同的希腊字母组合\n.",
    "动量分歧因子\n模版：\n<ts_delta/ts_mean>\n(ts_zscore(\n<feild/>\n,\n<window day1/>\n),\n<window day1/>\n)-\n<ts_delta/ts_mean>\n(ts_zscore(\n<feild/>\n,\n<window day2/>\n),\n<window day2/>\n)\n具体变量1\n:\n<ts_delta/ts_mean>\n表示俩个操作符二选一\n具体变量2\n:\n<feild/>\n表示数据字段，前后俩个\n<feild/>\n相同，表示同一个数据字段，模版对\n<feild/>\n数据字段类型适配度比较广泛，没有太大限制\n具体变量3\n:\n<window day1/>\n&\n<window day2/>\n俩个不同的时间窗口 俩个窗口的比例要尽量大 例如\n<window day1>\n= 100 ;\n<window day2/>\n= 400\n搜素空间大小\n：以\npv13\nmatrix ，短期+长期窗口 【5，10，20】、【200，300，400】 为例 28*2*3*3\n模版的idea\n：捕捉同一个数据字段 短期与长期动量的分歧  并保证模版普适性\n产出效果：以1sharp、0.6fitness为“有信号“阈值：\nfundamental\n（asi minvol1m）出信号率约4%\npv\n（asi minvol1m）出信号率约6%   20000次回测我最后选择并提交了4个alpha\n建议其他顾问未来尝试的探索方向: 此模版适配性较强，还可以尝试用组合数据字段进行回测",
    "YK49234\n对field的描述请详细些，补充一下，这两个field有没有什么约束条件",
    "基于oth455 网络因子差值动量模板\n模板\n<ts_operator_op/> ( <oth455_xxxx_fact1_value/> <math_op/> <oth455_xxxx_fact2_value/>,<days_op/>)\n1. 模板中的变量使用< />进行了声明，总共五个变量。\n具体变量1：<ts_operator_op/> 可以是\nts_rank, ts_zscore, ts_delta, ts_sum, ts_delay, ts_std_dev, ts_mean, ts_arg_min, ts_arg_max, ts_scale, ts_quantile  等\n具体变量2：<oth455_xxxx_fact1_value/> 可以是\noth455_relation_roam_w3_pca_fact1_value, oth455_relation_roam_w4_pca_fact1_value, oth455_relation_roam_w5_pca_fact1_value, oth455_partner_roam_w5_pca_fact1_value 等\n具体变量3：<math_op/> 可以是\n+ ，-，* , / 等\n具体变量4：<oth455_xxxx_fact2_value/> 可以是\noth455_relation_roam_w3_pca_fact2_value, oth455_relation_roam_w4_pca_fact2_value, oth455_relation_roam_w5_pca_fact2_value, oth455_partner_roam_w5_pca_fact2_value 等\n具体变量5：<days_op/> 可以是\n20,60,120,240 等\n2. 搜索空间的大小\n以oth455数据集为例，因子配对（如fact1-fact2, fact1- fact3, fact2-fact3,fact1* fact2）有3种核心组合。数据集本身有约11\n(时间序列算子）\n* 4\n(数学运算)\n* 300\n(基础设定)\n= 13,200 种不同的“关系-算法-窗口”基础设定 。 数据量非常大，有巨大的挖掘潜力 。\n3. 模板的idea，implement细节（即哪步是数据处理，哪步是主信号）\nIdea\n: oth455的PCA因子（fact1, fact2, fact3）捕捉了公司在关系网络中不同层面的特征。fact1通常代表“主流/规模”特征，而fact2、fact3代表更“小众/特色”的特征。本模板的核心思想是，通过计算不同因子间的\n差值或比率\n，来寻找网络地位**“失衡”\n的公司，例如那些“主流地位”不强但“特色地位”突出的“隐形冠军”。然后，通过时间序列算子捕捉这种“失衡”特征的\n持续性或变化动量**。\nImplement细节\n:\n<oth455_xxxx_fact1_value/> <math_op/> <oth455_xxxx_fact2_value/>;\n这是\n数据处理\n步骤。它将两个独立的网络因子合成为一个具有更清晰经济学意义的“差值”信号。\n<ts_operator_op/>(factor_difference, <days_op/>)\n这是\n主信号生成\n步骤。它作用于处理后的“差值”信号，将其转化为一个捕捉长期累积效应（如ts_sum）或短期变化趋势（如ts_delta）的最终Alpha。\n4. 产出效果\nEUR 市场中 Other模块\n乘数\n是\n最高\n的，不过也是能找到很多alpha的。给大家展示一个提交了的alpha，使用 ts_sum(oth455_xxxx_fact2_value - oth455_xxxx_fact1_value, 240) 这个具体表达式，从附图的回测结果看，虽然Fitness为0.6，还有提升空间，但其有0.07 的margin 并且operator 少，还有Risk neutralized 也是紧贴着的所以我交了。\n5. 建议其他顾问未来尝试的探索方向\n系统性挖掘\n: 全面遍历我上面提到的\n<ts_operator_op/> ， <oth455_xxxx_fact1_value/> ，<math_op/> ，<oth455_xxxx_fact2_value/>，<days_op/>\n的组合，大概率能发现比我这个\nts_sum\n版本表现更优的Alpha\n。\n构建二阶Alpha 三阶Alpha\n: 这个模板产出的信号还是有的，非常适合作为**“基石”或“风控”因子**。可以该模板去套培训用的二阶三阶模版，相信也是会有更好表现的。\n跨关系比较\n: 我模板中比较的是同一关系内的fact1和fact2。一个更有趣的方向是进行\n跨关系比较\n，例如 customer_fact1 - competitor_fact1，这可以构建一个衡量公司“网络护城河”的强大信号。",
    "模板：\nsigned_power(normalize(ts_backfill(trade_when(pcr_oi_<days1/>>1, implied_volatility_call_<days2/>-implied_volatility_put_<days3/>, -1),5)), 2)\n模板中的变量使用< />进行了声明，总共三个变量。\n• 具体变量1：<days1/>: 10、20、30、60、90、120、150、180、270、360、720、1080\n• 具体变量2：<days2/>: 10、20、30、60、90、120、150、180、270、360、720、1080\n• 具体变量3：<days3/>: 10、20、30、60、90、120、150、180、270、360、720、1080\n• 搜索空间的大小：使用option8，option9大约是12*12*12\n• 模板的idea，implement细节：捕捉短期看跌情绪pcr_oi_<days1/>>1的极端变化，通过数据平滑和信号增强，生成衡量市场恐慌/贪婪强度的方向性指标。\n比如说当20天期权出现看跌情绪主导时，对比6个月看涨期权与1年看跌期权的隐含波动率差异，他的极端值反映了投资者对中期风险与长期保护的定价分歧，再通过标准化normalize和非线性放大signed_power后提取显著的交易信号。\nsigned_power(normalize(ts_backfill(trade_when(pcr_oi_<days1/>>1, implied_volatility_call_<days2/>-implied_volatility_put_<days3/>, -1),5)), 2)：首先通过条件触发（当PCR_OI大于1时）计算隐含波动率差，然后对无效值回填，再标准化，最后取符号保留的平方得到最终信号。其中，trade_when(pcr_oi_180>1, iv_diff, -1)：当days天期权成交量的看跌看涨比大于1时，取隐含波动率差（call IV - put IV），否则置为-1；ts_backfill(series,5)：用最近5期内的有效值向前填充无效值；normalize(series)：将序列标准化；signed_power(x,2)：计算sign(x)*x^2，以保留方向并增强信号强度。\n• 产出效果：我尝试了一万次回测，有15个可以提交的优质Alpha（sharp超过2，fitness超过3，return超过40%的alpha也不在少数）.\n• 建议其他顾问未来尝试的探索方向: 在不同的universal（比如top3000、ILLIQUID_MINVOL1M）以及不同的操作符（比如去除ts_backfill，改为\ngroup_neutralize（x，industry），或者将sign_power改为\nts_decay_linear（x,5))。trade_when中的条件也可尝试\nvolume >ts_mean(volume, 60)*2，\nts_regression(returns, ts_step(5), 20, lag = 0, rettype = 2) > 0，\nrank(rp_css_business) > 0.8，option9中其他比率型的数据字段以及\n使用三阶模板中的一些条件也都有不错的表现。\n而且特别重要的一点，这个模板跑出的alpha符合ppac，这意味着如果不看重生产相关性的话每个人都可以交\n）",
    "波动率分歧因子（适用于rsk数据 类型）\n模版：\n<group_op>(power(<ts_op>(abs(<data>),<day1>),2)-power(<ts_op>(<data>,<day1>),2),<group>)\n具体变量：\n<group_op>：'group_neutralize','group_zscore','group_rank','group_normalize'\n<ts_op>：'ts_std_dev','ts_rank','ts_sum','ts_zscore'\n<data>：与风险有关的数据集 字段 ，例如rsk70\n<day1>：常见的时间窗口 ，5,20,120,252均可，模版中默认是5\n<group>：中性化组，模版中默认使用四个基础的中性化组\n搜索空间大小：\n按照默认参数，单个字段搜索空间为4*4*4*1=64个因子=7个槽\n产出效果：\n实验在usa地区按照使用人数倒序选取前100个rsk类型数据，目前跑了2234个因子，按照sharpe1和fit0.6进行筛选，得到114个因子，产出率为5%\n模版的idea：\n衡量过去\n<day1>\n天内数据的波动幅度与趋势的偏离程度，可以用来衡量风险的稳定性或者风险的突变。\n探索方向：\n在其他类型数据中实验模版\nabs(data)=data的问题，weijie老师给出的建议是把abs的处理方式换成在数轴上的分布，从而按照分布进行处理，初步的想法是按照ts_mean作为分布中心点，以if_else的方式代替abs处理，可以基于此进一步优化模版",
    "JR23144\n很漂亮",
    "YL41226\n请不要拿AI生成的东西不经思索就上传，对参数、字段约束、模板效果的描述没有提供有效细节（如截图），请参考前人优秀作品进行彻底修改，您的旧评论将在今日被删除。\n同时，您历史发布的所有帖子与评论将会在本日进行人工审核，疑似AI的帖子或评论都将被删除，社区分数亦会被相应降低。",
    "CC85858\n感谢分享。建议继续补充一些关于模板拓展性的内容或者关于trade_when条件的讨论，使其可以拓展到更多类似数据集。",
    "模板\n(<field1/> - ts_mean(<field1/>, <day/>)) / ts_mean(<field1/>, <day/>)\n模板中的变量使用\n< />\n进行了声明，总共2个变量。\n具体变量1：<field1/>: 是任何连贯的数据字段，要求是一定要数据丰富，数据连贯，每天都有数据的是最好的。\n具体变量2：<day/>: 看你要考察当前因子Field值与多长时间范围的平均值的偏离程度。可以是20一个月，也可以是252一年，all OK。\n搜索空间的大小：以数据集analyst69为例的话，大约是226 * 4 = 904\n模板的idea，implement细节（即哪步是数据处理，哪步是主信号）：\n1. 数据处理阶段\n输入字段准备\n：\n确保\n<field1/>\n是一个连贯且数据丰富的字段，且每天都有数据。\n确保\n<day/>\n是一个合理的时间窗口（如20天、252天等），根据分析需求选择适当的时间跨度。\n时间序列均值计算\n：\n对\n<field1/>\n进行滚动窗口计算，计算过去\n<day/>\n天的均值\nts_mean(<field1/>, <day/>)\n。\n滚动均值的计算是数据处理的核心步骤，确保均值计算的窗口长度与\n<day/>\n一致。\n2. 主信号生成阶段\n偏离程度计算\n：\n计算\n<field1/>\n当前值与其滚动均值的偏离程度\n该公式的分子部分捕捉了当前值与历史均值的差异，分母部分通过标准化消除量纲影响，使得信号具有可比性。\n信号输出\n：\n生成的信号是一个标准化的偏离值，范围通常在正负区间内（例如，正值表示当前值高于均值，负值表示当前值低于均值）。\n一般而言，这个是一个反转因子，所以在最后要加上一个\n负号！！！\n产出效果：我尝试了900次回测，有28个可以提交的Alpha.\n建议其他顾问未来尝试的探索方向: 任何数据集全都可以产出，这是一个通用模板。",
    "signal = <dataprocess_op/>(-group_backfill(vec_max(<nws77/>),country,60, std = 4.0));\nts_decay_linear(<ts_Statistical_op/>(signal, 90),5, dense = false)\n模板中的变量使用\n< />\n进行了声明，总共三个变量。\n具体变量1：<nws77/>: EUR region TOP2500 universe, DataSet ID：\nnws77\n. DataFields Type: Vector.\n具体变量2：<dataprocess_op/>: quantile(x)、winsorize(x)等，对\n数据进行整理\n的函数。\n具体变量3：<ts_Statistical_op/>: ts_ir(x,days) 、ts_zscore(x,days)、ts_entropy(x,days)等，计算信号\n复合统计指标\n的函数。\n注：ts_decay_linear也可以替换成jump_decay和ts_decay_exp_window(x, d, factor = f)这些\nTime Series: Decay, Smoothing, and Turnover Control (时间序列：衰减、平滑和周转控制)\n的函数\n。\n因为这些函数的参数数量不一致，为了简洁，该模板就写死了ts_decay_linear。\n搜索空间的大小：以数据集\nnws77\n为例的话，大约是29*2*3=179个。\nnws77\n有29个数据字段。\nnws77数据特征分析：\n总结一下\nnws77\n数据集中代表字段\nnws77_sentiment_impact_projection\n的数据分布，并用ai加以整理后就是：分布特征，数据分布高度集中且呈负偏态（左偏），众数显著地出现在-0.18附近。数值以负值为主，这表明整体的情绪预测偏向谨慎或看跌。稀疏性与覆盖率，该数据集极其稀疏，存在大量NaN（缺失）值。然而，数据覆盖率（即非缺失值的数量）自2013年以来呈现强劲的上升趋势，并表现出明显的季度性规律，这与企业财报的发布周期相符。时间序列性质，该数据为事件驱动型，而非连续的时间序列。针对特定实体的数据更新频率较低，且对应于财报电话会议等离散事件。非缺失值（Non-NaN）与非缺失且非零值（Non-NaN and Non-Zero）的数量几乎完全相同，说明有效的数值极少为零。群组行为，群组层面（如按国家分组）的均值表现出强烈的协同变动性，这表明存在一个共同的宏观或整体市场因素在影响着市场情绪。（\nDistribution: The data has a highly concentrated and negatively skewed distribution, with a prominent mode around -0.18. The predominantly negative values suggest an overall cautious or bearish sentiment projection.Sparsity & Coverage: The dataset is extremely sparse with a high prevalence of NaN values. However, data coverage (the count of non-NaN entries) shows a strong upward trend since 2013 and exhibits clear quarterly seasonality, consistent with corporate earnings reporting cycles.Time Series Nature: The data is event-driven, not a continuous time series. Updates for a given entity are infrequent and correspond to discrete events like earnings calls. The Non-NaN and Non-NaN and Non-Zero counts are nearly identical, indicating that valid data points are rarely zero.Group Behavior: Group-level averages (e.g., by country) show strong co-movement, suggesting a common macro or market-wide influence on sentiment.\n）可以直接复制用在最新的APP AI数据集探索的提示词当中。\n模板的idea，implement细节：signal = <dataprocess_op/>(-group_backfill(vec_max(<nws77/>),country,60, std = 4.0))部分是对信号进行处理。vec_max选出vector中值最大元素。group_backfill则负责填充缺失值，减少意外平仓的情况。<dataprocess_op/>则主要用来调整数据分布，减少极值，起到增强信号的作用。ts_decay_linear(<ts_Statistical_op/>(signal, 90),5, dense = false)则是主信号部分，用来提取经过初步处理后的数据中的时序信息。ts_decay_linear则用来进一步平滑和增强信号。\n产出效果：我尝试了174次回测，有1个可以提交的单数据字段Alpha。但是剩下的alpha几乎都有明显的信号，可以证明模版在nws77数据集的泛用性。需要进一步针对每个数据字段不同的特征进行进一步的信息提取，来得到可以提交的alpha。\n以下是探索模版的分阶段表现（消融实验）：\n建议其他顾问未来尝试的探索方向: 这个数据集中turnover普遍较高（不是某些时间的极端值，而是从时间上看都很高），margin较低。",
    "LH44620\n漂亮，漂亮到我觉得50USD都少了",
    "<group_compareOP/>\n(\n<risk_field/>\n,\n<std_group/>\n)\n模板中的变量使用\n< />\n进行了声明，总共三个变量。\n具体变量1：\n<group_compareOP/>\n: 可以对组间进行计算的分组运算符，例如group_rank, group_zscore, group_neutralize等\n具体变量2：\n<risk_field/>\n: 可以包含所有和 GLB risk 的数据集，vec数据需要套上vec_op\n具体变量3：\n<std_group/>\n: 可以包含pv,other 数据集中 group类型的数据\n搜索空间的大小：<group_compareOP/>使用group_zscore ，\n<std_group/>\n选5个，所有GLB三个region , 6 种中性化，risk 下的数据集为例的话，一共是6750个\n模板的idea，implement细节（即哪步是数据处理，哪步是主信号）：\n使用labs 查看这个这个数据集，在回测后发现该字段本身就是有信号的，使用不同组间运算符增强组间信号\n发现sharpe增加，fitness增加不多\n尝试再外面套ts_op效果不理想\n产出效果：我尝试了6750次回测，有的sharpe是<0的，需要表达式前加负号，最终回测比这数值高，有339个sharpe>1的alpha,目前已选中几个提交点亮的py\n建议其他顾问未来尝试的探索方向: 其他region",
    "AH18340\n模板似乎与课堂示例没有太多不同，没有原创性，没有针对数据集进行深入探索，深度不足。请重新思考并修改。",
    "模板\n<ts_op1/>(<ts_op2/>(<oth_datafields/>,<days1/>),<days2/>)\n模板中的变量使用\n< />\n进行了声明，总共三个变量。\n具体变量1：<ts_op1/>: 可以包含时间序列中所有字段，如ts_returns、ts_rank、ts_zscore等\n具体变量2：<ts_op2/>: 可以包含时间序列中所有字段，但两个字段不可以重复，建议这里选择ts_mean、ts_max、ts_delta等字段\n具体变量3：<days1/>: 时间窗口，可选为20，60，120，252，504等\n具体变量4：<days2/>: 时间窗口，可选为5，20，60，120，252等，并且days2要大于days1\n搜索空间的大小：以数据集other143为例的话，大约是84*3*3*15=11340\n模板的idea，implement细节：该双重时间序列模板是利用不同时间窗口的数据差异，分析数据的周期性内在联系，其中ts_op2的作用是为了对数据做预处理，对于oth_datafields的选择为覆盖率大于等于80%以保证处理效果，再用ts_op1获得主信号，灵感来自于lab上对于数据字段周期性的研究，认为PNL的变化在一定尺度上的变化幅度是符合周期性的，未来也可以根据周期做相应预测。\n产出效果：我尝试了3000次回测，有24个可以提交的Alpha，并且近三年表现都较好且有递增趋势，其中一个数据如下（无法上传图片，在这里简单说明一下产出alpha的特点：pnl后期无下滑且一直向上，sharpe也是如此）。\nYearSharpeTurnoverFitnessReturnsDrawdownMarginLong CountShort Count\n2021 1.37 13.05% 1.08 8.08% 4.13% 12.39‱ 1520 1482\n2022 2.34 11.37% 2.38 12.88% 3.14% 22.65‱ 1539 1523\n2023 8.86 10.52% 13.63 29.57% 0.22% 56.21‱ 1553 1525\n建议其他顾问未来尝试的探索方向: 该模板为基础模板，可以嵌入到其他模板中代替单字段使用，后续建议在anl、mdl、fnd等覆盖率较高的数据集中使用（或者手动选择覆盖率高的字段）。",
    "alpha = alpha; #有信号的一阶表达式\nvector_neut(alpha,ts_backfill(\n<risk70/>\n,120)) #是否要ts_backfill可自行决定\n模板中的变量使用\n< />\n进行了声明，总共一个变量。\n具体变量1：\n<risk70/>\n: dataset risk70;\n搜索空间的大小：以ASI - D1 - MINVOL1M为例，一共30个字段\n模板的idea，implement细节：alpha是一阶表达式作为主信号，vector_neut(alpha,ts_backfill(<risk70/>,120))对于主信号做风险因子的中性化处理（类似于neutralization里的RAM,statistical等)，相当于对一阶因子做二阶处理\n产出效果：我回测了几个有信号的一阶表达式套上模板，回测结果都和原来的表达式信号大致相同，同时也是鲁棒性测试的一个补充检验（和neutralization类似，一般而言returns可能比原信号稍微低一点，但同时drawdown也会降低）\n注意事项：alpha推荐是有信号的一阶表达式，假设是二阶表达式（这时可能已经用了两个数据集包含pv),则再套用以上模板则此表达式最终有3个数据集，虽然可以提交但无法点亮任何金字塔。所以注意alpha应该只包含一个数据集的字段。\n应用场景：\n（1）假设一阶表达式刚好卡在pc（0.72）或者sc边缘（0.52），套用该模板后有可能降低相关性得以提交\n（2）假设RISK pyramid还没有点亮，提交此类因子有助于点亮RISK pyramid\n（3）vector_neut算是一个比较少用的operator, 可增加六维里的operator count\nReference:\nhttps://support.worldquantbrain.com/hc/en-us/community/posts/27157459347991-Getting-started-with-Risk-Datasets?_gl=1*1orlfhc*_gcl_au*ODM4Njg5MjI5LjE3NDg1MjY5NDQ.*_ga*MTc0ODUyNjk0MTk2OTRmNjhiNDBjMGVlZTk.*_ga_9RN6WVT1K1*czE3NTMxOTg1NTgkbzI4OSRnMSR0MTc1MzE5OTYyNyRqNTYkbDAkaDA\n.\n以上模板来自于官方推荐，亲测有效。\nExample Alpha ideas:\nUse\nvector_neut(Alpha, factor)\nto neutralize your Alpha's exposure to a chosen risk factor. Iterate over different factors to determine whether your Alpha's returns are influenced by any of them. This approach helps you maintain diversity in your Alpha pool and avoid over-reliance on a few specific factors.\n例子：\n原来因子（alpha)\nts_backfill(oth176_maltahc_eq_score, 120)\n套用模板后因子\nalpha =ts_backfill(oth176_maltahc_eq_score, 120);\nvector_neut(alpha,ts_backfill(rsk70_mfm2_asetrd_cntadj,120))\n可以看到数据基本有所提升（除returns外），\nsharpe: 1.59 -> 1.72; fitness:1.46->1.57; Returns:10.58%->10.47%;Margin:43.3 ->50.92",
    "WL13229 谢谢老师的认可",
    "XC66172\n内容OK，但请补充一些具体应用，例如你使用过的因子，放进去的效果。",
    "AL13375\n\"产出效果：我尝试了3000次回测，有24个可以提交的Alpha，并且近期表现都较为优异。\"\n请补充说明、细节",
    "WL13229 无法上传图片，已补充文字说明，请老师点评指正！",
    "@WL13229  谢谢WEIJIE老师点评，我已经在原来的帖子基础上补充了例子。",
    "模板:去除组内平均效应的相对排名信号\ns = <vec_op/>(<risk_data/>) - group_mean(<vec_op/>(<risk_data/>), <weight/>, <group/>);\n<group_op/>(<cross_op/>(s)，<group2/>)\n模板中的变量使用< />进行了声明，总共7个变量。\n具体变量1：<vec_op/>: vector 数据处理 例如：vec_avg 等\n具体变量2：<risk_data/>: risk 数据集推荐USA-D1 的risk60 的  rsk60_crowding, rsk60_offer\n具体变量3：<weight/>: group_mean 的weight 定义，可以放 1 或者adv20 等pv1 的字段\n具体变量4：<group/>: 中性化组，pv1例如:sector, market, industry, subindustry\n具体变量5：<group_op/>:使用 group_rank group_zscore 等做排名\n具体变量6：<cross_op/>横截面 运算符 例如：zscore, rank, quantile 等\n具体变量7：<group2/>: 中性化组，pv1例如:sector, market, industry, subindustry\n搜索空间的大小：以数据集risk60为例的话，大约是(SearchSpace: 1 × 2 × 1 × 4 × 3 × 3 × 4 = 288)\n模板的idea，implement细节（即哪步是数据处理，哪步是主信号）：\n核心思想是构建一个去除组内平均效应的相对排名信号。\nsign = <vec_op/>(<risk_data/>) - group_mean(...) - 核心主信号，通过减去组内均值来获得相对强度\n<group_op/>(<cross_op/>(sign), <group2/>) - 对信号进行截面处理和二次分组排名\n该模板是之前kunqi 老师分享的组平均差去做的实操，我直接用于risk60做的尝试，直接是先用组分差看是否有信号，再一步步套其他操作符号产生的，在这里做了一个web app 的template 版本，希望能给大家有所启发。\n跑了之后我去分析其中的原理:\nrsk60_crowding 的字段说明：\nCrowding is a daily score for shorting and covering activity on the security.\nScores of 7 and greater represent significant shorting activity for a given day.\nScores of 5 to 0 show notable shorting activity for a given day.\nNegative scores represent covering on a security.\nScores of -7 and less represent significant covering activity for a given day.\nScores of 0 to -5 show notable covering activity for a given day.\n可得rsk60_crowding 是衡量个股每天\n卖空拥挤度\n的打分指标\n≥ 7 显著做空压力\n1 ~ 6 可观察到做空行为\n0 中性，无明显变化\n-1 ~ -6 可观察到回补行为\n≤ -7 强烈的空头回补\n因子则是衡量一只股票的空头交易活跃度相对于市场平均的偏离程度\n排名靠前（空头很集中）:可能市场过度看空，反弹潜力大\n排名靠后（大家都在回补）: 市场开始转向就顺势做多，情绪刚转暖\n行业内部比较剔除行业因素，看相对强弱，精选同类股里“风向变”的那只\n建议其他顾问未来尝试的探索方向:\n1.权重的控制可以探索其他值\n2.其他地区的数据是否也能好的效果？",
    "YZ70114\n变量描述的地方似乎缺少了 下列变量的描述\ncross_op",
    "WL13229\n不好意思，写漏了一个操作符，已补上",
    "SL95036\n您的发言偏离主题，在本贴无贡献。请不要将AI的内容不假思索进行粘贴，您的历史论坛活动将被人工审核，疑似AI的帖子或评论都将被删除，社区分数亦会被相应降低。",
    "模板：\nts_corr(ts_zscore(<vec_op/>(<snt27_data/>),20), ts_zscore(<vec_op/>(<snt27_data/>),20), 90)\n模板中的变量使用< />进行了声明，总共两个变量\n具体变量1：<snt27_data/>: USA TOP3000 的sentiment27数据中的两两组合字段。这个数据集只有429个 user使用，Pyramid 1.4。\n具体变量2：<vec_op/>:vec_sum 或者 vec_avg，处理vector类型的数据\n搜索空间的大小：272 × 4 = 1088\n模板的idea：这个数据集是针对热门网站的排名指标，通过计算两个经过标准化处理的网站相关指标在过去 90 天内的时间序列相关系数，衡量这两个指标同步变化趋势，更好的反映网站的市场表现、用户偏好或竞争格局的内在关联。最后得到做多或者做空的信号。\nimplement细节：\n首先用vec操作符号对处理vector类数据，\n然后用ts_zscore对数据进行标准化处理，统一量纲\n最后用ts_corr计算两者的关联程度，这里选择的90天，计算中长期的关联程度，如 “网站排名” 与 “相对受欢迎度” 是否在 90 天内同步上升 / 下降）。作为最终的主信号\n产出效果：我尝试了1088次回测，有6个可以提交的Alpha。提交标准为PPAC。该模板为单数集字段。\n建议其他顾问未来尝试的探索方向：可以扩展到多数据集计算两两的相关程度，在时间的选择上面也可以修改，比如252天，60天和20天，捕捉长期，短期交易信号。\n以下是其中一个alpha：使用的字段为\nsnt27_relpopularity08_11：Relative popularity calculated with alpha=0.8\nsnt27_avgranking_19：Average of rankings\nsnt27_relpopularity08_11反应了市场短期情绪或消费者即时偏好\nsnt27_avgranking_19 简单平均排名则代表长期稳定表现，不受短期波动影响\n当两者相关性较高时，说明短期热度与长期质量表现一致；若相关性低，则可能表示 “昙花一现” 的炒作或未被市场及时发现的优质产品。计算两者的ts_corr相关程度，可以生成做空以及做多的信号。该alpha表现如下。\n在五月份成为条件顾问以后，第一VF更新为0.95。因为snt27这个数据集做的人较少，所以自己单独尝试了一下，鉴于该数据集只有2019年以后的数据，我并不太确定这个alpha的稳定性。所以没有交，也想请weijie老师指点一下。谢谢老师！\n如下图所示，已补充数据分布以及更新频率，基本上每天或者3-4天就会更新，这个更新频率应该比较符合sentiment的。\n该alpha换手率极低，我感觉可能是和90天长窗口有关系，我先测试了ts_zscore的窗口，换手率没有变化，估计只要两个序列的相对波动节奏一致，它们的 z-score 变化方向可能同步，相关性未必剧烈变化。\n我继续测试，调整ts_corr的窗口为5天，10天，20天如图所示：\n短期关系易反转，换手率突然上升，长窗口的话，更能看出两者之间的稳定关系，所以\nts_corr 是 “信号变化频率” 的核心开关\n，而 ts_zscore 更多影响信号的 “波动幅度”，这也是为什么调整 ts_corr 窗口后换手率变化更显著。虽然这几个窗口都能提交，估计是需要找到一个临界点。我个人感觉10天的窗口比较合适。请weijie老师再看一下。",
    "LL49894\n抽空可以看看数据字段本身的更新频率。目前这个Alpha的更新频率很低，感觉上来说，一个sentiment的数据不应该这么低的更新频率。如果更新频率匹配且相关性低的话，不失为一个好的Alpha",
    "## 消除市值干扰后的分析师数据横截面比较\n### 模版:\n```\nfinancial_data = ts_backfill(<analyst_field/>, 66);\nsignal = <group_operator/>(financial_data, <group/>);\nsignal - regression_proj(signal, rank(cap))\n```\n### 变量:\n- <analyst_field/> 分析师数据，如果是向量的话用 vec_sum / vec_mean 处理\n- <group_operator/>: group_zscore, group_scale, group_neutralize\n- <group/>: 分组我是直接使用 mechine_lib 里面 USA 定义的分组，加上 industry, sector 这些\n### 搜索空间大小：\n以 analyst4 为例：\nmatrix数据 202 + vector数据 148 * 2 = 498; 分组操作符3个; 分组数据没有太细究，直接从 machine lib 上复制了大概20个。\n因此：一共大概 500 * 3 * 20 = 30000 搜索空间。\n### Idea & Implement\n开始尝试直接对分析师数据数据做时间序列处理后，发现 turnover 特别高，而且信号感觉不强。因此，开始直接进行组件横截面比较，信号不错。而后想到一般而言，市值越高的公司会越受到分析师关注，所以高市值公司分析师数据会有更多偏见，正好可以用使用 `regression_proj` 把原来信号和市值进行回归，然后再减去结果，从而得到更佳纯粹的信号。\n以 `anl4_rd_exp_low` 数据为例：一般来讲市值越高的公司 R&D 数据会越高，且分析师更愿意去分析考虑高市值的公司。通过回归可以很好消除这个影响，这个信号我测试如果不回归的话，sharpe 值会下降到1.03。除了这个例子，我发现分析师横截面比较的信号，剔除市值影响后，都会有一定程度的提升。\n### 产出效果\n在 anl4 遍历了一万多，能交 ppa 的有不少。\n### 后续\n1. 有可能会有比【市值】更好衡量这种偏见的字段，也就是替代 rank(cap)；2. 或者再去套 trade when。我也想知道有没有办法能再去提升这个信号，如果大家有什么想法也可以一起交流谢谢🙏"
  ]
}