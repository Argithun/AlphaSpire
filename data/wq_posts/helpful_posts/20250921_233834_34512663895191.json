{
  "title": "即插即用型【基于相关性剪枝】完整代码",
  "description": "首先感谢JG21054和 KZ79256 和  HQ17963 以及 EC12049 三位大佬的源码，思路和分享。具体的文章就不放链接了。因为我是小白 看前辈的代码不能直接使用，也看到论坛好多和我一样的 只看到代码不能直接跑起来，我结合各位大佬的代码整合了下 ...",
  "post_body": "首先感谢JG21054和 KZ79256 和  HQ17963 以及 EC12049 三位大佬的源码，思路和分享。具体的文章就不放链接了。因为我是小白 看前辈的代码不能直接使用，也看到论坛好多和我一样的 只看到代码不能直接跑起来，我结合各位大佬的代码整合了下  可以正常跑起来，如有不对的地方欢迎批评指正。\nimport pandas as pd\nimport time\nimport os\nimport logging as logger  # 保持日志模块别名\nimport requests\nfrom typing import List, Dict, Any, Optional\n# 关键修改：仅从machine_lib导入login，）\nfrom machine_lib import login\n# -------------------------- 1. 移除原有的cfg类和自定义login()函数 --------------------------\n# -------------------------- 2. Alpha数据获取模块（保持不变） --------------------------\ndef fetch_alphas(url: str, session: requests.Session, max_retry: int = 3) -> List[Dict[str, Any]]:\n\"\"\"请求Alpha数据，带重试逻辑，返回结果列表\"\"\"\nfor retry in range(max_retry):\nresponse = session.get(url)\nif response.status_code == 200:\nbreak\nlogger.warning(f\"❌ 获取数据失败 (URL: {url})，状态码: {response.status_code}，重试次数: {retry + 1}\")\ntime.sleep(3)\nelse:\nlogger.error(f\"❌ 超过最大重试次数 ({max_retry})，获取数据失败\")\nreturn []\nreturn response.json().get(\"results\", [])\ndef process_alpha(alpha: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n\"\"\"处理单个Alpha数据：过滤无效数据、调整表达式与decay\"\"\"\nrequired_fields = [\"id\", \"dateCreated\", \"is\", \"settings\", \"regular\"]\nfor field in required_fields:\nif field not in alpha:\nlogger.warning(f\"⚠️ Alpha数据缺少字段 {field}，跳过处理\")\nreturn None\nalpha_id = alpha[\"id\"]\ndate_created = alpha[\"dateCreated\"]\nsharpe = alpha[\"is\"].get(\"sharpe\", 0)\nfitness = alpha[\"is\"].get(\"fitness\", 0)\nturnover = alpha[\"is\"].get(\"turnover\", 0)\nmargin = alpha[\"is\"].get(\"margin\", 0)\nlong_count = alpha[\"is\"].get(\"longCount\", 0)\nshort_count = alpha[\"is\"].get(\"shortCount\", 0)\ndecay = alpha[\"settings\"].get(\"decay\", 0)\nexpression = alpha[\"regular\"].get(\"code\", \"\")\n# 过滤持仓数量过少的Alpha\nif (long_count + short_count) <= 100:\nlogger.debug(f\"⚠️ Alpha {alpha_id} 持仓数量过少 ({long_count + short_count} <= 100)，跳过\")\nreturn None\n# 调整负Sharpe的表达式\nif sharpe < 0:\nexpression = f\"-{expression}\"\n# 构建基础记录\nrecord: Dict[str, Any] = {\n\"alpha_id\": alpha_id,\n\"expression\": expression,\n\"sharpe\": sharpe,\n\"turnover\": turnover,\n\"fitness\": fitness,\n\"margin\": margin,\n\"date_created\": date_created,\n\"decay\": decay\n}\n# 根据换手率调整decay\nif turnover > 0.7:\nrecord[\"decay\"] = decay * 4\nelif turnover > 0.6:\nrecord[\"decay\"] = decay * 3 + 3\nelif turnover > 0.5:\nrecord[\"decay\"] = decay * 3\nelif turnover > 0.4:\nrecord[\"decay\"] = decay * 2\nelif turnover > 0.35:\nrecord[\"decay\"] = decay + 4\nelif turnover > 0.3:\nrecord[\"decay\"] = decay + 2\nreturn record\ndef get_alphas(\nsession: requests.Session,\nstart_time: str,\nend_time: str,\nsharpe_th: float,\nfitness_th: float,\nregion: str = \"\",\nmargin: str = \"\",\nreturns: str = \"\",\nturnover: str = \"\",\norder: str = \"-dateCreated\",\npage_size: int = 100,\nmax_page: int = -1,\nfilter_failed: bool = False,\nenable_opt: bool = False\n) -> List[List[Any]]:\n\"\"\"批量获取并处理Alpha数据，返回结构化列表\"\"\"\nbase_url = \"https://api.worldquantbrain.com/users/self/alphas\"\nquery_list: List[str] = [\n\"status=UNSUBMITTED%1FIS_FAIL\",\n\"hidden=false\",\n\"type=REGULAR\",\n# 北京时间转纽约时间（确保时区正确）\nf'dateCreated>={pd.Timestamp(start_time).floor(\"s\").tz_localize(\"Asia/Shanghai\").tz_convert(\"America/New_York\").isoformat()}',\nf'dateCreated<{pd.Timestamp(end_time).floor(\"s\").tz_localize(\"Asia/Shanghai\").tz_convert(\"America/New_York\").isoformat()}',\nf\"limit={page_size}\"\n]\n# 追加过滤条件（非空才添加）\nif region:\nquery_list.append(f\"settings.region={region}\")\nif order:\nquery_list.append(f\"order={order}\")\nif margin:\nquery_list.append(f\"is.margin>{margin}\")\nif returns:\nquery_list.append(f\"is.returns>{returns}\")\nif turnover:\nquery_list.append(f\"is.turnover>{turnover}\")\n# 追加Sharpe/Fitness过滤与分页参数\nquery_list.extend([\n\"is.sharpe{sharpe_op}{sharpe_th}\",\n\"is.fitness{fitness_op}{fitness_th}\",\n\"offset={offset}\"\n])\nquery_url = f\"{base_url}?{'&'.join(query_list)}\"\nconditions = {\n\"gt\": {\"sharpe_op\": \">\", \"sharpe_th\": f\"{sharpe_th}\", \"fitness_op\": \">\", \"fitness_th\": f\"{fitness_th}\"},\n\"lt\": {\"sharpe_op\": \"<\", \"sharpe_th\": f\"-{sharpe_th}\", \"fitness_op\": \"<\", \"fitness_th\": f\"-{fitness_th}\"}\n}\nrecord_list: List[Dict[str, Any]] = []\nfor key in conditions.keys():\nif not enable_opt and key == \"lt\":\ncontinue\noffset = 0\nn_page = 0\nwhile True:\nn_page += 1\nlogger.info(f\"📄 处理第 {n_page} 页，偏移量: {offset}\")\n# 格式化请求URL\nurl = query_url.format(offset=offset, **conditions[key])\nlogger.debug(f\"🔍 请求URL: {url}\")\n# 获取Alpha列表\nalpha_list = fetch_alphas(url, session)\nif not alpha_list:\nlogger.info(f\"📌 无更多数据（第 {n_page} 页为空）\")\nbreak\n# 超过最大页数限制\nif max_page > 0 and n_page >= max_page:\nlogger.info(f\"📌 已达到最大页数 {max_page}，停止查询\")\nbreak\n# 处理Alpha数据\nalpha_df = pd.DataFrame(alpha_list)\nif filter_failed:\n# 过滤IS阶段检查失败的Alpha\nchecks_mask = alpha_df[\"is\"].apply(\nlambda x: not any(pd.DataFrame(x[\"checks\"])[\"result\"] == \"FAIL\")\n)\nalpha_df = alpha_df[checks_mask]\nlogger.info(f\"⚡ 过滤后剩余Alpha数量: {len(alpha_df)}\")\nif not alpha_df.empty:\nprocessed_records = alpha_df.apply(process_alpha, axis=1).dropna()\nrecord_list.extend(processed_records.tolist())\nlogger.info(f\"✅ 累计处理有效Alpha数量: {len(record_list)}\")\noffset += page_size\n# 整理输出格式\nfields = [\"alpha_id\", \"expression\", \"sharpe\", \"turnover\", \"fitness\", \"margin\", \"date_created\", \"decay\"]\noutput = [\n[rec[field] for field in fields]\nfor rec in record_list\nif rec is not None\n]\nlogger.info(f\"🎉 最终获取有效Alpha总数: {len(output)}\")\nreturn output\n# -------------------------- 3. PNL合法性检测模块（保持不变） --------------------------\ndef wait_get(url: str, sess: requests.Session, max_retries: int = 10) -> requests.Response:\n\"\"\"带重试与Retry-After等待的GET请求\"\"\"\nretries = 0\nwhile retries < max_retries:\nresponse = sess.get(url)\nretry_after = response.headers.get(\"Retry-After\", 0)\nif retry_after != 0:\nlogger.info(f\"⏳ 服务端要求等待 {retry_after} 秒，重试中...\")\ntime.sleep(float(retry_after))\ncontinue\nif response.status_code < 400:\nreturn response\nelse:\nwait_time = 2 ** retries\nlogger.warning(f\"❌ 请求失败（状态码: {response.status_code}），{wait_time} 秒后重试\")\ntime.sleep(wait_time)\nretries += 1\nlogger.error(f\"❌ 超过最大重试次数 ({max_retries})，请求失败: {url}\")\nraise requests.exceptions.HTTPError(f\"Max retries exceeded for URL: {url}\")\ndef check_consecutive_non_zero_values(alpha_id: str, data: List[List[Any]], required_streak: int = 200) -> bool:\n\"\"\"检查PNL数据是否合法：无连续200个非零值 + 无全零列\"\"\"\nif not data or len(data) < required_streak:\nreturn True\ndef _check_column(column_data: List[Any]) -> bool:\n\"\"\"检查单列是否有连续超标非零值\"\"\"\nif len(column_data) < required_streak:\nreturn True\ncurrent_streak = 0\ncurrent_value = None\nfor val in column_data:\nif val != 0:\nif val == current_value:\ncurrent_streak += 1\nelse:\ncurrent_value = val\ncurrent_streak = 1\nelse:\ncurrent_value = None\ncurrent_streak = 0\nif current_streak >= required_streak:\nreturn False\nreturn True\n# 提取关键列数据\ncol1, col2 = [], []\nfor row in data:\nif len(row) >= 3:\ncol1.append(row[1])\ncol2.append(row[2])\n# 检查全零列\nif col1 and col2:\nif all(v == 0 for v in col1) or all(v == 0 for v in col2):\nlogger.warning(f\"❌ Alpha {alpha_id} PNL存在全零列，判定不合法\")\nreturn False\n# 检查连续非零值\nif not _check_column(col1) or not _check_column(col2):\nlogger.warning(f\"❌ Alpha {alpha_id} PNL存在连续{required_streak}个相同非零值，判定不合法\")\nreturn False\nlogger.info(f\"✅ Alpha {alpha_id} PNL检测通过\")\nreturn True\ndef get_alpha_pnl_legal(alpha_id: str, sess: requests.Session) -> bool:\n\"\"\"单个Alpha的PNL合法性检测\"\"\"\npnl_url = f\"https://api.worldquantbrain.com/alphas/{alpha_id}/recordsets/pnl\"\ntry:\nresponse = wait_get(pnl_url, sess)\npnl_data = response.json()\nrecords = pnl_data.get(\"records\", [])\nreturn check_consecutive_non_zero_values(alpha_id, records)\nexcept Exception as e:\nlogger.error(f\"⚠️ Alpha {alpha_id} PNL检测异常: {str(e)}，暂判定不合法\")\nreturn False\ndef get_alpha_pnl_legal_list(fo_tracker: List[List[Any]], sess: requests.Session) -> List[List[Any]]:\n\"\"\"批量检测Alpha的PNL合法性，返回合法列表\"\"\"\nlogger.info(f\"📊 开始PNL合法性检测，待检测Alpha数量: {len(fo_tracker)}\")\nlegal_alphas = [fo for fo in fo_tracker if get_alpha_pnl_legal(fo[0], sess)]\nlogger.info(f\"🎉 PNL检测完成，合法Alpha数量: {len(legal_alphas)}，剔除数量: {len(fo_tracker) - len(legal_alphas)}\")\nreturn legal_alphas\n# -------------------------- 4. 主执行流程（登录逻辑与脚本A一致） --------------------------\nif __name__ == \"__main__\":\n# 关键修改：使用machine_lib.login()获取会话（与脚本A的s = login()完全一致）\nsession = login()\n# 初始化日志（确保在登录后、使用日志前配置，避免重复配置）\nif not logger.getLogger().hasHandlers():\nlogger.basicConfig(level=logger.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\nif not session:\nlogger.error(\"❌ 登录失败，脚本终止\")\nexit(1)\ntry:\n# 步骤2：获取Alpha数据（参数可根据需求调整）\nfo_tracker = get_alphas(\nsession=session,            # 传入machine_lib.login()返回的会话\nstart_time=\"2025-07-12\",     # 开始时间（ISO格式）\nend_time=\"2025-08-18\",       # 结束时间（ISO格式）\nsharpe_th=1.0,               # Sharpe阈值\nfitness_th=0.57,              # Fitness阈值\nregion=\"GLB\",                # 地区过滤（USA）\npage_size=200,               # 每页数量\nmax_page=-1,                 # 最大页数（-1为全部）\nfilter_failed=True,          # 过滤IS阶段失败项\nenable_opt=False             # 不开启负Sharpe优化\n)\n# 步骤3：PNL合法性检测\nif not fo_tracker:\nlogger.warning(\"⚠️ 无有效Alpha数据，跳过PNL检测\")\nelse:\nf_num = len(fo_tracker)\nprint(f\"\\n📢 {f_num} 个Alpha 进行PNL合法检测，请耐心等待...\")\nfo_tracker = get_alpha_pnl_legal_list(fo_tracker, session)\nprint(f\"📢 {f_num - len(fo_tracker)} 个不合法的PNL已被剔除，剩余合法Alpha数量: {len(fo_tracker)}\")\nexcept Exception as e:\nlogger.error(f\"❌ 脚本执行异常: {str(e)}\", exc_info=True)\nfinally:\n# 关闭会话\nsession.close()\nlogger.info(\"🔚 会话已关闭，脚本执行结束\")",
  "post_comments": []
}