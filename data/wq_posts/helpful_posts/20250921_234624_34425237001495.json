{
  "title": "利用Brain Lab进行数据探索系列一：单data field数据特征研究、数据预处理，以及MCP Prompt构建",
  "description": "概览： 一、为什么要进行数据特征研究及数据预处理？ 二、结合Brain lab分析可能存在的数据问题，以及如何进行数据预处理 三、需要关注的数据特征 四、数据预处理整体流程梳理 五、构建MCP prompt对数据进行预处理   正文：...",
  "post_body": "概览：\n一、为什么要进行数据特征研究及数据预处理？\n二、结合\nBrain lab\n分析可能存在的数据问题，以及如何进行数据预处理\n三、需要关注的数据特征\n四、数据预处理整体流程梳理\n五、构建\nMCP prompt\n对数据进行预处理\n正文：\n一、为什么要进行数据特征研究及数据预处理？\n“因子”是基于\nRoss\n的\nAPT\n理论\n，\nFama\n和\nFrench\n通过三因子模型所开创的领域，\n是属于实证金融而非理论金融的范畴。实证金融的基础是统计学，具体到因子分析上，构成因子的每个金融工具特征在截面上的同质性和因子在时间序列上的平稳性是重要假设前提。\n具体到\nBrain\n平台上的\nAlpha generation\n，\n数据预处理的目的有二\n：\n1.\n通过数据预处理\n保证数据的同质及平稳\n，可以更大程度保障\nIS\n及\nOS\n的一致性，使我们对\nOS\n表现更有信心。\n2.\n对于涉及多个数据字段的\nalpha\n，我们需要\n保证多个字段的可比性\n。\n需要注意的问题是，\n原始数据的平稳性与因子平稳性是不一样的\n。我们\n更关注的是因子的平稳性\n。举个例子，\nnews\n数据通常是脉冲式的，随机出现的，因此\nnews\n数据段一般是不平稳的。但是，如果我们基于\nnews\n数据构建一个事件驱动因子，我们对每一个新闻执行同样的交易策略，那我们的因子数据的事件空间就是所有的有新闻的时间，而非整个交易时间段，那么因子数据就很可能是平稳的。\n二、结合\nBrain lab\n分析可能存在的数据问题，以及如何进行数据预处理\n首先，要理解\n原始数据是一个面板\n，从数据每个截面上看是不同的金融工具在同一日期的不同特征表现；从每个时间序列上看是同一金融工具在不同日期的特征表现。 因此，在横截面上和时间序列上都需要对数据可能出现的问题进行研究和处理。\n进一步，考虑数据可能出现的、对同质性和平稳性有影响的问题。基于\nBrain lab\n给出的案例，数据可能出现的问题有如下几类：\n缺失数据\n(missing data)\n。在\ncross section\n和\ntime series\n上都可能出现该问题，如果缺失数据比例较大，则因子的统计有效性是存疑的。另外，缺失数据可能被命名为\n-1\n或者\n99\n，如果不处理会扭曲真实数据情况。\n周期性数据\n(cyclical data)\n。如果数据在\ntime series\n上出现周期性特征，则数据是不平稳的，可能存在自相关，那么得出的统计量是不可靠的。\n极值\n(outlier)\n。会扭曲数据的真实分布，使得统计量不可靠。\n非正态分布\n(non normal distribution)\n。非正态分布一般不会产生重大问题。平台关注的统计量基本都是参数估计，但如果数据本身是非正态分布的，那么很多经典的参数检验都是无效的，对判断统计量的有效性产生一定难度。\n还有一些虽然不影响因子有效性，但处理数据时应当小心的问题：\n向量数据\n(vector data)\n。应该了解\nvector data\n原始数据的状态，再进一步考虑如何使用具体\nvector operator\n进行处理。\n非可比数据\n(non-comparable data)\n。如果涉及两个或两个以上字段进行分析，那么这些字段只有在单位上可比才能联合分析。\n以下均使用\nUSA, Delay1, TOP3000, dividend\n数据字段\n做示例。\n1.\n通过数据可视化初步判断数据可能出现的问题\n缺失数据：\n通过分布图和数据覆盖图判断是否有缺失值，缺失值是否应该处理，以及应该如何处理\n情况一：大量、无规律缺失。处理方式：首先判断数据是否可有用于构建事件驱动策略，如可能，则不处理；如果不能，则放弃该字段。\n情况二：大量、有规律缺失。处理方式：方法一：构建事件驱动策略因子；方法二：通过自回归、周期加总使数据平稳；方法三：简单缺失值回填。\n情况三少量缺失：处理方式：方法一：简单缺失值回填；方法二：不处理。\n示例：\n通过\nVisualize Field\n查看数据分布以及覆盖\n从覆盖图上可以看出，数据\n90%\n以上为\n0\n，没有\nnan\n数据。这时候我们需要注意，这个\n0\n数据到底是缺失值被回填为\n0\n了，还是它本来就是\n0\n？我们放大数据可以看出，覆盖数据在时间上具有一定周期性。\n再看\nturnover\n数据，周期性非常明显。再结合这个字段是股利数据，那么，我们可以大胆判断，这个数据的缺失属于情况二，大量、有规律缺失。（类似的字段还有财务报表的原始数据）我们可以再结合个股数据再次验证。\n再看频率分布图，大量的数据集中在\n0\n，对\ny\n轴采用\nlog\n，可以看出数据有少量的非零数据，非零数据具有尖峰厚尾的统计特征。另外我们注意到，数据并没有把缺失值表示为\n-1, 999\n之类。\n极值\n通过统计量判断是否存在极值，极值应到如何处理\n情况一：与策略相关，如捕捉异常财务数据，因为亏损、会计调整等等出现财务指标出现异常数据，那么要么不处理，要么捕捉这些异常后再进行处理。\n情况二：去极值，使数据同质、平稳。\n示例：\n根据统计量分布图查看百分位数的分布\n从统计量分布图可以看出\nmax\n值相对于\n75\n分位值有\nscale\n上的差异，因此这个数据集的极值是会扭曲数据分布的，因为\ndividend\n的数据本来就少，所以预计基于异常值很难组成有效因子，所以对于这个字段最好的处理方式就是去极值。\n非正态分布\n通过频率分布图查看数据是否属于正态分布。\n从之前的分布图可以看出数据明显左偏，也就是出现极值的概率大于正态分布。自变量不一定需要调整成正态分布，主要还是看使用数据的目的。比如担心极值会扭曲数据关系，或者希望数据预测更加稳健，可以将数据调整成正态分布。\n数据可比性\n通过频率分布图对相关数据进行单位及分布进行判断，调整数据使相关数据字段可比。\n2.\n可以采用的数据预处理方式\n以下是一些示例，采用mcp后会提供更多的数据预处理方式\n缺失数据：\n将数值转化为nan：pasteurize, purify, to_nan, nan_out\n回填nan数据：nan_mask, ts_backfill, kth_element\n极值：\n去极值：winsorize, truncate\n非正态分布：\n分布调整：log, sigmoid, tanh\n三、需要关注的数据特征\n连续还是离散\n缺失值类型及比例\n数据频率\n数据单位和分布\n四、数据预处理整体流程梳理\n第一步：\n点击\nVisualize Field\n初步查看数据特征，了解缺失值、异常值和分布\n数据覆盖图\n+turnover\n图\n+\n个股数据图判断数据频率、周期性及数据分布\n数据覆盖图\n+\n频率分布图查看缺失值\n统计特征图查看极值\n第二步：\n建立关键统计量定律描述数据概况（利用Brain lab进行分析）\n1.\n零值和\nnan\n值比例\n2.\n数据频率及周期。\n3.\n关键统计量\n分别计算\n原始非零数据的关键统计量\n和\n去极值后的数据的关键统计量\n。\n第三步：\n构建\nMCP\n数据分析的\nprompt\n，进行数据预处理。\n五、\nprompt\n模板\nBefore we start the alpha generation, we need to preprocess the raw data to improve the quality of the alpha data.\nEach data field in the Brain Platform database is panel data, i.e., it contains aggregated time-series data of all the financial instruments in the universe in interest.\nThe following is the statistical summary of the data field. After you digesting the data info, you will generate content I later ask you to provide.\nData Summary:\n1. data field\n{“delay”: 1,\n“\nuniverse”: “USA”,\n“\ndata set id”: “pv1”,\n“\ndata field id”: “dividend”}\n2.\nmeasurement scale\nThe value in the data field is continuous, not discrete.\n3. data coverage\nFrom the time series perspective, the proportion of zero values is\n{\"mean\": 0.9920,\n\"min\": 0.9455,\n\"25%\": 0.9906,\n\"50%\": 0.9943,\n\"75%\": 0.9968,\n\"max\": 1, }.\nThe proportion of nan values in the time series is 0.\n4. data frequency\nThe data has pulse-like periodicity for each instrument in the time series perspective.\nFor each financial instrument, calculate the average days between adjacent non-zero values in its time series, and then conduct statistical analysis on all financial instruments, with the conclusions: {\"mean\": 87,\n\"min\": 1.5,\n\"25%\": 1.5,\n\"50%\": 91,\n\"75%\": 91,\n\"max\": 1650\n}\n5. data distribution\nStatistical analysis on all non-zero data shows:\n{\"mean\": 0.3541,\n\"min\": 0.000003,\n\"25%\": 0.12,\n\"50%\": 0.23,\n\"75%\": 0.4025,\n\"max\": 103.75,\n\"skewness\": 49.54,\n\"kurtosis\": 5101.7}\nAfter cross-sectional data winsorization applying the IQR rule, the statistical analysis results are: {\"mean\": 0.2993,\n\"min\": 0.000003,\n\"25%\": 0.12,\n\"50%\": 0.23,\n\"75%\": 0.40,\n\"max\": 6.5,\n\"skewness\": 1.89,\n\"kurtosis\": 10.16}.\nNow please provide multiple data preprocessing expressions based on your understanding of the operators in the Brain Platform:\nStep 1. Handling missing data.\nStep 2. Handling outliers.\nOptional step. Adjust data distribution for better statistical results.\nAdditional requirement:\n1. Employ as many as possible operators in the platform\n以下是\n分析结果示例：\n这是单数据字段的分析示例，可以进一步改进prompt，了解数据单位和分布特征后，数据预处理可以实现多个数据字段的对齐，使之可比，是alpha质量更高。\n另：也在此提问，怎么去理解lab字段分析中的turnover？我理解是abs（单日数据-长期平均值之间差异）/长期平均值，不知道理解是否正确",
  "post_comments": [
    "学习，还是看得一头雾水",
    "手动点赞，谢谢大佬的分享。\n========================================================================================================================================================================",
    "有深度，有帮助，期待更多优秀分享",
    "turnover 的计算：diff(1).abs().sum() / abs().sum()",
    "MCP提示词的数据，用lab公式获取后，是手动输入的，还是有其他自动化方案，请大佬给指点一下。"
  ]
}