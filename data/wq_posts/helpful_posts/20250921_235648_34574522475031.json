{
  "title": "ä½¿ç”¨å›¾è®ºæœ€å¤§å›¢ç®—æ³•æ·±å…¥åˆ†æAlphaç›¸å…³æ€§ï¼Œè¿›è¡ŒAlphaå»é‡åŠå›¢å†…Alphaå¯è§†åŒ–æŒ‘é€‰çš„å·¥å…·",
  "description": "å¤§å®¶å¥½ï¼Œ æ ¹æ®åå­å“¥çš„è‡ªç›¸å…³ä»£ç ä»¥åŠæ¯›è€å¸ˆæå‡ºçš„æœ€å¤§å›¢æ€è·¯, æˆ‘åšäº†ä¸€ä¸ªåŸºäºå›¾è®ºä¸­ â€œæœ€å¤§å›¢ï¼ˆMaximal Cliqueï¼‰â€ ç®—æ³•çš„å¯è§†åŒ–åˆ†æå·¥å…·. ç®—æ³•æ€è·¯å°±ä¸èµ˜è¿°äº†, ç®€å•æ¥è¯´å°±æ˜¯é’ˆå¯¹0.5ä»¥ä¸‹è‡ªç›¸å…³çš„alpha,å°†ä»–ä»¬åˆ†æˆå›¢,åŒä¸€ä¸ªå›¢å†…çš„alpha,...",
  "post_body": "å¤§å®¶å¥½ï¼Œ\næ ¹æ®åå­å“¥çš„è‡ªç›¸å…³ä»£ç ä»¥åŠæ¯›è€å¸ˆæå‡ºçš„æœ€å¤§å›¢æ€è·¯, æˆ‘åšäº†ä¸€ä¸ªåŸºäºå›¾è®ºä¸­ â€œæœ€å¤§å›¢ï¼ˆMaximal Cliqueï¼‰â€ ç®—æ³•çš„å¯è§†åŒ–åˆ†æå·¥å…·.\nç®—æ³•æ€è·¯å°±ä¸èµ˜è¿°äº†, ç®€å•æ¥è¯´å°±æ˜¯é’ˆå¯¹0.5ä»¥ä¸‹è‡ªç›¸å…³çš„alpha,å°†ä»–ä»¬åˆ†æˆå›¢,åŒä¸€ä¸ªå›¢å†…çš„alpha, æäº¤äº†å…¶ä¸­ä¸€ä¸ªä»¥å, å…¶ä»–çš„alphaæ— æ³•é€šè¿‡PPAçš„è‡ªç›¸å…³æ£€æµ‹, ä½¿ç”¨å¯è§†åŒ–å·¥å…·,æ–¹ä¾¿æ‰¾å‡ºè¿™æ‰¹alphaé‡Œé¢æ›²çº¿èµ°åŠ¿ä¸æ¯å¹´æŒ‡æ ‡ç»¼åˆæ›´ä¼˜è´¨çš„é€‰æ‹©.å½“ç„¶ä¹Ÿå¯ä»¥åšæˆç›¸å…³æ€§çš„å‰ªæ, åªå¯¹å›¢å†…éƒ¨åˆ†å› å­è¿›è¡Œè¿›ä¸€æ­¥çš„å…¶ä»–å¤„ç†, å‡å°‘æ— æ•ˆçš„åŒè´¨åŒ–æ“ä½œ.\nåŠŸèƒ½å’Œäº®ç‚¹ï¼š\n1. 1.\nåˆ©ç”¨ networkx åº“è¿›è¡Œå›¾çš„æ„å»ºå’Œæœ€å¤§å›¢æŸ¥æ‰¾ï¼Œèƒ½æ›´ç³»ç»Ÿåœ°æ­ç¤ºAlphaé—´çš„å¤æ‚å…³è”ç»“æ„ã€‚\n2. 2.\näº¤äº’å¼å¯è§†åŒ–ä»ªè¡¨ç›˜ ï¼šåŸºäº Streamlit å’Œ Plotly æ„å»ºäº†Webç•Œé¢ã€‚ä½ å¯ä»¥ï¼š\n- æŸ¥çœ‹è¯†åˆ«å‡ºçš„æ‰€æœ‰Alphaå›¢ã€‚\n- ç‚¹å‡»ä»»ä½•ä¸€ä¸ªå›¢ï¼Œç«‹å³çœ‹åˆ°å›¢å†…æ‰€æœ‰Alphaçš„ç´¯ç§¯PNLæ›²çº¿å¯¹æ¯”å›¾ã€‚\n- è·å–å›¢å†…Alphaçš„å¹´åº¦è¡¨ç°ç»Ÿè®¡ï¼ˆSharpeã€Returnsã€Drawdownç­‰ï¼‰ï¼Œå¿«é€Ÿè¯„ä¼°å…¶æ•´ä½“è´¨é‡ã€‚\n- æŸ¥çœ‹æ¯ä¸ªAlphaçš„è¯¦ç»†å‚æ•°ï¼ˆregion, neutralization, turnoverç­‰ï¼‰ã€‚\n3. 3.\nçµæ´»çš„æ•°æ®æºæ”¯æŒ ï¼š\n- æ”¯æŒç›´æ¥ä» WorldQuant Brain API è·å–Alphaçš„PNLå’Œè¯¦ç»†æ•°æ®ã€‚\n- ä¹Ÿæ”¯æŒè¿æ¥æœ¬åœ° MySQLæ•°æ®åº“ ï¼Œæ–¹ä¾¿è¿›è¡Œç¦»çº¿åˆ†æå’Œå›æµ‹ã€‚\n4. 4.\nç¨³å¥çš„APIäº¤äº’ ï¼šå†…ç½®äº†å®Œæ•´çš„WQ Brain APIä¼šè¯ç®¡ç†ã€è¯·æ±‚é‡è¯•å’Œè¶…æ—¶å¤„ç†æœºåˆ¶ï¼Œç¡®ä¿åœ¨å¤§é‡æ‹‰å–æ•°æ®æ—¶çš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚\n5. 5.\nä¸€ç«™å¼åˆ†æ ï¼šä»æ•°æ®è·å–ã€ç›¸å…³æ€§è®¡ç®—ã€æœ€å¤§å›¢æ£€æµ‹åˆ°ç»“æœå¯è§†åŒ–å’ŒæŒ‡æ ‡åˆ†æï¼Œæ•´ä¸ªæµç¨‹è¢«æ•´åˆåœ¨ä¸€ä¸ªå·¥å…·ä¸­ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„Alphaå…³è”æ€§åˆ†æã€‚\nè¿™ä¸ªå·¥å…·åœ¨å®è·µä¸­å¯ä»¥å¸®åŠ©æˆ‘ä»¬ï¼š\n- Alphaå»é‡ä¸ç­›é€‰ ï¼šå½“ä¸€ä¸ªâ€œå›¢â€è¢«è¯†åˆ«å‡ºæ¥åï¼Œæˆ‘ä»¬å¯ä»¥è®¤ä¸ºè¿™ä¸ªå›¢å†…çš„Alphaæ•æ‰çš„æ˜¯ç›¸ä¼¼çš„ä¿¡å·ã€‚æˆ‘ä»¬å¯ä»¥ä»ä¸­é€‰æ‹©ä¸€ä¸¤ä¸ªæœ€å…·ä»£è¡¨æ€§ï¼ˆä¾‹å¦‚Sharpeæœ€é«˜ã€æœ€ç¨³å®šï¼‰çš„Alphaï¼Œè€Œå°†å…¶ä»–å†—ä½™çš„Alphaå­˜æ¡£ï¼Œä»è€Œå¤§å¤§ç®€åŒ–æˆ‘ä»¬çš„Alphaåº“ã€‚\n- æå‡ç»„åˆå¤šæ ·æ€§ ï¼šåœ¨æ„å»ºæŠ•èµ„ç»„åˆæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ„è¯†åœ°ä»ä¸åŒçš„â€œå›¢â€ä¸­å„æŒ‘é€‰ä¸€ä¸ªä»£è¡¨ï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿æˆ‘ä»¬çš„æœ€ç»ˆç»„åˆæ˜¯ç”±ä¸€ç³»åˆ—ä½ç›¸å…³çš„ç­–ç•¥æ„æˆçš„ï¼Œä»è€Œæå‡å¤šæ ·æ€§ã€‚\n- é£é™©æš´éœ²åˆ†æ ï¼šåˆ†ææŸä¸ªâ€œå›¢â€å†…Alphaçš„å…±åŒç‰¹å¾ï¼ˆä¾‹å¦‚ï¼Œå®ƒä»¬æ˜¯å¦éƒ½äº¤æ˜“æŸä¸€ç±»è‚¡ç¥¨ï¼Œæˆ–è€…éƒ½ä½¿ç”¨äº†æŸä¸ªç‰¹å®šçš„æ•°æ®å­—æ®µï¼‰ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£ç»„åˆæ½œåœ¨çš„é£é™©æš´éœ²ã€‚\n- æ¿€å‘æ–°æ€è·¯ ï¼šä¸ºä»€ä¹ˆè¿™ä¸ªâ€œå›¢â€é‡Œçš„Alphaè¡¨ç°å¦‚æ­¤ç›¸ä¼¼ï¼Ÿå®ƒä»¬èƒŒåå…±åŒçš„é€»è¾‘æ˜¯ä»€ä¹ˆï¼Ÿå¯¹è¿™äº›é—®é¢˜çš„æ¢ç´¢ï¼Œæœ‰æ—¶èƒ½å¸®åŠ©æˆ‘ä»¬æç‚¼å‡ºæ›´ç¨³å®šã€æ›´å¼ºå¤§çš„æ–°Alphaå› å­ã€‚ æ€»ç»“\næ€»è€Œè¨€ä¹‹ï¼Œ maximal_clique_analyzer.py æ˜¯ä¸€ä¸ªå°†å›¾è®ºç®—æ³•ä¸é‡åŒ–æŠ•ç ”å®è·µç›¸ç»“åˆçš„å°è¯•ã€‚å®ƒæä¾›äº†ä¸€ä¸ªæ–°é¢–çš„è§†è§’æ¥å®¡è§†æˆ‘ä»¬æ‰‹ä¸­çš„Alphaå®åº“ï¼Œå¸®åŠ©æˆ‘ä»¬å»ç²—å–ç²¾ï¼Œæ„å»ºæ›´ç¨³å¥ã€æ›´å¤šæ ·åŒ–çš„æŠ•èµ„ç»„åˆã€‚\nè¿™ä¸ªå·¥å…·çš„è®¾è®¡çµæ„Ÿæ¥æºäºå®é™…å·¥ä½œä¸­çš„ç—›ç‚¹ï¼Œå¸Œæœ›èƒ½å¯¹å¤§å®¶æœ‰æ‰€å¸®åŠ©ã€‚æ¬¢è¿å„ä½ä¸‹è½½è¯•ç”¨ã€äº¤æµæƒ³æ³•ï¼Œä¹ŸæœŸå¾…å¤§å®¶çš„å®è´µæ„è§å’Œå»ºè®®ï¼\nå®é™…æ•ˆæœå›¾:\nä»£ç æ¯”è¾ƒé•¿æ”¾è¯„è®ºåŒºå§",
  "post_comments": [
    "å…·ä½“ä»£ç å¦‚ä¸‹:\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nMaximal Clique Alpha Analyzer\næœ€å¤§å›¢Alphaåˆ†æå·¥å…·\næœ¬å·¥å…·åŸºäºå›¾è®ºä¸­çš„æœ€å¤§å›¢ç®—æ³•ï¼Œç”¨äºåˆ†æAlphaä¹‹é—´çš„ç›¸å…³æ€§ç»“æ„ã€‚\nä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š\n1. åŸºäºç›¸å…³æ€§é˜ˆå€¼æ„å»ºAlphaå…³è”å›¾\n2. ä½¿ç”¨æœ€å¤§å›¢ç®—æ³•è¯†åˆ«é«˜åº¦ç›¸å…³çš„Alphaç»„åˆ\n3. å¯è§†åŒ–å›¢å†…Alphaçš„PNLè¡¨ç°æ›²çº¿\n4. æä¾›äº¤äº’å¼æ•°æ®å±•ç¤ºå’Œå¹´åº¦æŒ‡æ ‡ç»Ÿè®¡\n5. æ”¯æŒæ•°æ®åº“å’ŒæœåŠ¡å™¨APIä¸¤ç§æ•°æ®æº\næŠ€æœ¯å®ç°ï¼š\n- ä½¿ç”¨NetworkXåº“è¿›è¡Œå›¾æ„å»ºå’Œæœ€å¤§å›¢æ£€æµ‹\n- åŸºäºPlotlyå®ç°äº¤äº’å¼æ•°æ®å¯è§†åŒ–\n- é€šè¿‡Streamlitæ„å»ºWebç•Œé¢\n- æ”¯æŒWorldQuant Brain APIæ•°æ®è·å–\n\"\"\"\nimport pandas as pd\nimport numpy as np\nimport json\nimport logging\nimport pymysql\nfrom typing import List, Dict, Tuple, Optional\nimport networkx as nx\nimport datetime\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport streamlit as st\nimport requests\nimport time\nimport os\nfrom time import sleep\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n# å…¨å±€sessionç®¡ç†å™¨\nclass SessionManager:\n_instance = None\n_session = None\n_last_login_time = None\n_session_timeout = 3600 Â # 1å°æ—¶sessionè¶…æ—¶\ndef __new__(cls):\nif cls._instance is None:\ncls._instance = super(SessionManager, cls).__new__(cls)\nreturn cls._instance\ndef _login(self):\n\"\"\"å†…éƒ¨ç™»å½•æ–¹æ³•ï¼Œå‡å°‘æ—¥å¿—è¾“å‡º\"\"\"\nusername = os.environ.get('wq_username')\npassword = os.environ.get('wq_password')\nretry_strategy = Retry(\ntotal=3,\nbackoff_factor=1,\nstatus_forcelist=[500, 502, 503, 504]\n)\nadapter = HTTPAdapter(max_retries=retry_strategy)\ns = requests.Session()\ns.mount(\"https://\", adapter)\ns.mount(\"http://\", adapter)\ns.auth = (username, password)\nmax_try = 3\nretry = 0\nwhile retry < max_try:\ntry:\nresponse = s.post('\nhttps://api.worldquantbrain.com/authentication\n')\nif response.status_code in [200, 201]:\n# åªåœ¨é¦–æ¬¡ç™»å½•æˆ–é‡æ–°ç™»å½•æ—¶æ‰“å°ç®€åŒ–ä¿¡æ¯\nauth_data = response.json()\nuser_id = auth_data.get('user', {}).get('id', 'æœªçŸ¥')\nlogger.info(f\"Sessionè®¤è¯æˆåŠŸï¼Œç”¨æˆ·ID: {user_id}\")\nself._last_login_time = time.time()\nreturn s\nexcept Exception as e:\nlogger.warning(f\"ç™»å½•å°è¯•å¤±è´¥: {e}\")\nsleep(2)\nretry += 1\nlogger.error(\"ç™»å½•å¤±è´¥ï¼Œè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°\")\nreturn None\ndef get_session(self):\n\"\"\"è·å–sessionï¼Œè‡ªåŠ¨å¤„ç†å¤ç”¨å’Œé‡æ–°ç™»å½•\"\"\"\ncurrent_time = time.time()\n# æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°ç™»å½•\nif (self._session is None or\nself._last_login_time is None or\ncurrent_time - self._last_login_time > self._session_timeout):\nlogger.info(\"åˆ›å»ºæ–°çš„sessionæˆ–sessionå·²è¿‡æœŸï¼Œé‡æ–°ç™»å½•...\")\nself._session = self._login()\nreturn self._session\n# å…¨å±€sessionç®¡ç†å™¨å®ä¾‹\nsession_manager = SessionManager()\ndef get_session():\n\"\"\"è·å–WorldQuant Brain session\"\"\"\ns = session_manager.get_session()\nif s:\nreturn s\n# å¦‚æœç™»å½•å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªåŸºæœ¬çš„requests session\nsession = requests.Session()\nsession.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'})\nreturn session\n# é…ç½®æ—¥å¿—\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\nclass Config:\n\"\"\"é…ç½®ç±»\"\"\"\nDB_CONFIG = {\n'host': 'localhost',\n'user': 'root',\n'password': '123456',\n'database': 'worldquant',\n'port': 3306,\n'charset': 'utf8mb4'\n}\nCORRELATION_THRESHOLD = 0.5 Â # ç›¸å…³æ€§é˜ˆå€¼\nclass DatabaseManager:\n\"\"\"æ•°æ®åº“ç®¡ç†ç±»\"\"\"\ndef __init__(self):\nself.config = Config.DB_CONFIG\ndef get_connection(self):\n\"\"\"è·å–æ•°æ®åº“è¿æ¥\"\"\"\ntry:\nconnection = pymysql.connect(**self.config, cursorclass=pymysql.cursors.DictCursor)\nreturn connection\nexcept Exception as e:\nlogger.error(f\"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}\")\nreturn None\ndef get_pnl_data(self, alpha_id: str) -> Optional[dict]:\n\"\"\"ä»æ•°æ®åº“è·å–PNLæ•°æ®\"\"\"\nconn = self.get_connection()\nif not conn:\nreturn None\ntry:\nwith conn.cursor() as cursor:\nsql = \"SELECT pnl_data FROM alpha_pnl WHERE alpha_id = %s\"\ncursor.execute(sql, (alpha_id,))\nresult = cursor.fetchone()\nreturn json.loads(result['pnl_data']) if result else None\nexcept Exception as e:\nlogger.error(f\"è·å–PNLæ•°æ®å¤±è´¥ {alpha_id}: {e}\")\nreturn None\nfinally:\nconn.close()\ndef get_alpha_data(self, alpha_id: str) -> Optional[dict]:\n\"\"\"ä»alpha_dataè¡¨è·å–alphaæ•°æ®\"\"\"\nconn = self.get_connection()\nif not conn:\nreturn None\ntry:\nwith conn.cursor() as cursor:\nsql_data = \"\"\"SELECT * FROM alpha_data WHERE id = %s\"\"\"\ncursor.execute(sql_data, (alpha_id,))\nalpha_data = cursor.fetchone()\nif not alpha_data:\nreturn None\nresult = dict(alpha_data)\n# å¤„ç†datetimeå¯¹è±¡\nfor key, value in result.items():\nif isinstance(value, datetime.datetime):\nresult[key] = value.strftime('%Y-%m-%d %H:%M:%S')\nelif isinstance(value, datetime.date):\nresult[key] = value.strftime('%Y-%m-%d')\n# è®¾ç½®é»˜è®¤å­—æ®µ\nif 'name' not in result:\nresult['name'] = alpha_id\nif 'neutralization' not in result:\nresult['neutralization'] = 'CROWDING'\nreturn result\nexcept Exception as e:\nlogger.error(f\"è·å–alphaæ•°æ®å¤±è´¥ {alpha_id}: {e}\")\nreturn None\nfinally:\nconn.close()\ndef get_alpha_data_from_server(self, alpha_id: str, session, timeout_seconds: int = 3600) -> Optional[dict]:\n\"\"\"ä»æœåŠ¡å™¨è·å–alphaæ•°æ®ï¼Œæ”¯æŒ1å°æ—¶è¶…æ—¶é™åˆ¶\nArgs:\nalpha_id (str): Alpha ID\nsession: ä¼šè¯å¯¹è±¡\ntimeout_seconds (int): æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤3600ç§’ï¼ˆ1å°æ—¶ï¼‰\nReturns:\nOptional[dict]: Alphaæ•°æ®ï¼Œè¶…æ—¶æˆ–å¤±è´¥è¿”å›None\n\"\"\"\nstart_time = time.time()\ntry:\n# æ£€æŸ¥è¶…æ—¶\nif time.time() - start_time > timeout_seconds:\nlogger.warning(f\"Alpha {alpha_id} è·å–æ•°æ®è¶…æ—¶ï¼Œè·³è¿‡\")\nreturn None\n# ä½¿ç”¨ç‰¹å®šalphaçš„APIç«¯ç‚¹\nurl = f\"\nhttps://api.worldquantbrain.com/alphas/{alpha_id}\n\"\n# è®¡ç®—å‰©ä½™è¶…æ—¶æ—¶é—´\nremaining_timeout = timeout_seconds - (time.time() - start_time)\nif remaining_timeout <= 0:\nlogger.warning(f\"Alpha {alpha_id} å‰©ä½™æ—¶é—´ä¸è¶³ï¼Œè·³è¿‡\")\nreturn None\nresponse = session.get(url, timeout=min(30, remaining_timeout))\nif response.status_code == 404:\nlogger.warning(f\"Alpha {alpha_id} ä¸å­˜åœ¨ï¼Œè·³è¿‡\")\nreturn None\nelif response.status_code != 200:\nlogger.error(f\"è·å–alpha {alpha_id} å¤±è´¥: {response.status_code}\")\nreturn None\nresult_data = response.json()\nalpha_info = result_data.get('is', {})\nsettings = result_data.get('settings', {})\n# æå–éœ€è¦çš„å­—æ®µ\nresult = {\n'id': alpha_id,\n'sharpe': alpha_info.get('sharpe'),\n'returns': alpha_info.get('returns'),\n'drawdown': alpha_info.get('drawdown'),\n'turnover': alpha_info.get('turnover'),\n'fitness': alpha_info.get('fitness'),\n'margin': alpha_info.get('margin'),\n'long_count': alpha_info.get('longCount'),\n'short_count': alpha_info.get('shortCount'),\n'region': settings.get('region'),\n'neutralization': settings.get('neutralization', 'CROWDING'),\n'name': result_data.get('name', alpha_id)\n}\nlogger.info(f\"æˆåŠŸä»æœåŠ¡å™¨è·å–alphaæ•°æ®: {alpha_id}, region: {result.get('region')}\")\nreturn result\nexcept requests.exceptions.Timeout:\nlogger.warning(f\"Alpha {alpha_id} è¯·æ±‚è¶…æ—¶ï¼Œè·³è¿‡\")\nreturn None\nexcept Exception as e:\nlogger.error(f\"ä»æœåŠ¡å™¨è·å–alphaæ•°æ®å¤±è´¥ {alpha_id}: {e}\")\nreturn None\nclass CorrelationCalculator:\n\"\"\"ç›¸å…³æ€§è®¡ç®—ç±»\"\"\"\ndef __init__(self, db_manager: DatabaseManager, use_server_data: bool = False):\nself.db_manager = db_manager\nself.use_server_data = use_server_data\nself.session = None\nif use_server_data:\nself.session = get_session()\ndef wait_get(self, url: str, max_retries: int = 10, timeout_seconds: int = 3600):\n\"\"\"\nå‘é€å¸¦æœ‰é‡è¯•æœºåˆ¶çš„ GET è¯·æ±‚ï¼Œç›´åˆ°æˆåŠŸæˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°æˆ–è¶…æ—¶ã€‚\næ­¤å‡½æ•°ä¼šæ ¹æ®æœåŠ¡å™¨è¿”å›çš„ `Retry-After` å¤´ä¿¡æ¯è¿›è¡Œç­‰å¾…ï¼Œå¹¶åœ¨é‡åˆ° 401 çŠ¶æ€ç æ—¶é‡æ–°åˆå§‹åŒ–é…ç½®ã€‚\nArgs:\nurl (str): ç›®æ ‡ URLã€‚\nmax_retries (int, optional): æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º 10ã€‚\ntimeout_seconds (int, optional): æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸º 3600ï¼ˆ1å°æ—¶ï¼‰ã€‚\nReturns:\nResponse: è¯·æ±‚çš„å“åº”å¯¹è±¡ï¼Œå¦‚æœè¶…æ—¶è¿”å›Noneã€‚\n\"\"\"\nstart_time = time.time()\nretries = 0\nwhile retries < max_retries:\n# æ£€æŸ¥æ˜¯å¦è¶…è¿‡æ€»è¶…æ—¶æ—¶é—´\nif time.time() - start_time > timeout_seconds:\nlogger.warning(f\"è¯·æ±‚è¶…æ—¶ï¼ˆ{timeout_seconds}ç§’ï¼‰ï¼Œè·³è¿‡: {url}\")\nreturn None\nwhile True:\ntry:\nsimulation_progress = self.session.get(url, timeout=30)\nif simulation_progress.headers.get(\"Retry-After\", 0) == 0:\nbreak\nretry_after = float(simulation_progress.headers[\"Retry-After\"])\n# æ£€æŸ¥ç­‰å¾…æ—¶é—´æ˜¯å¦ä¼šå¯¼è‡´æ€»è¶…æ—¶\nif time.time() - start_time + retry_after > timeout_seconds:\nlogger.warning(f\"ç­‰å¾…æ—¶é—´ä¼šå¯¼è‡´è¶…æ—¶ï¼Œè·³è¿‡: {url}\")\nreturn None\ntime.sleep(retry_after)\nexcept requests.exceptions.Timeout:\nlogger.warning(f\"è¯·æ±‚è¶…æ—¶ï¼Œé‡è¯•: {url}\")\nbreak\nexcept Exception as e:\nlogger.error(f\"è¯·æ±‚å¼‚å¸¸: {e}\")\nbreak\nif simulation_progress.status_code < 400:\nbreak\nelse:\nwait_time = min(2 ** retries, 60) Â # æœ€å¤§ç­‰å¾…60ç§’\n# æ£€æŸ¥ç­‰å¾…æ—¶é—´æ˜¯å¦ä¼šå¯¼è‡´æ€»è¶…æ—¶\nif time.time() - start_time + wait_time > timeout_seconds:\nlogger.warning(f\"é‡è¯•ç­‰å¾…ä¼šå¯¼è‡´è¶…æ—¶ï¼Œè·³è¿‡: {url}\")\nreturn None\ntime.sleep(wait_time)\nretries += 1\nreturn simulation_progress\ndef get_pnl_from_server(self, alpha_id: str, timeout_seconds: int = 3600) -> Optional[dict]:\n\"\"\"ä»æœåŠ¡å™¨è·å–PNLæ•°æ®ï¼Œæ”¯æŒ1å°æ—¶è¶…æ—¶é™åˆ¶\nArgs:\nalpha_id (str): Alpha ID\ntimeout_seconds (int): æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤3600ç§’ï¼ˆ1å°æ—¶ï¼‰\nReturns:\nOptional[dict]: PNLæ•°æ®ï¼Œè¶…æ—¶æˆ–å¤±è´¥è¿”å›None\n\"\"\"\ntry:\nresponse = self.wait_get(f\"\nhttps://api.worldquantbrain.com/alphas/{alpha_id}/recordsets/pnl\n\", timeout_seconds=timeout_seconds)\nif response is None:\nlogger.error(f\"ä»æœåŠ¡å™¨è·å–PNLæ•°æ®å¤±è´¥ {alpha_id}: è¯·æ±‚è¶…æ—¶æˆ–ç½‘ç»œé”™è¯¯\")\nreturn None\nelif response.status_code == 200:\nreturn response.json()\nelif response.status_code == 404:\nlogger.warning(f\"Alpha {alpha_id} ä¸å­˜åœ¨\")\nreturn None\nelse:\nlogger.error(f\"ä»æœåŠ¡å™¨è·å–PNLæ•°æ®å¤±è´¥ {alpha_id}: HTTP {response.status_code}\")\nreturn None\nexcept Exception as e:\nlogger.error(f\"ä»æœåŠ¡å™¨è·å–PNLæ•°æ®å¼‚å¸¸ {alpha_id}: {e}\")\nreturn None\ndef get_alpha_returns(self, alpha_id: str, timeout_seconds: int = 3600) -> Optional[pd.Series]:\n\"\"\"è·å–alphaçš„æ”¶ç›Šç‡åºåˆ—ï¼Œæ”¯æŒ1å°æ—¶è¶…æ—¶é™åˆ¶\"\"\"\n# æ ¹æ®é…ç½®é€‰æ‹©æ•°æ®æº\nif self.use_server_data:\npnl_data = self.get_pnl_from_server(alpha_id, timeout_seconds=timeout_seconds)\nelse:\npnl_data = self.db_manager.get_pnl_data(alpha_id)\nif not pnl_data:\nreturn None\ntry:\nrecords = pnl_data['records']\ncolumns = [\"date\", \"pnl\"] + [f\"col{i}\" for i in range(2, len(records[0]))]\ndf = pd.DataFrame(records, columns=columns)\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.set_index('date').sort_index()\npnl_series = df['pnl']\nreturns = pnl_series.pct_change().dropna()\n# è¿‡æ»¤æœ€è¿‘4å¹´æ•°æ®\ncutoff_date = returns.index.max() - pd.DateOffset(years=4)\nreturns = returns[returns.index > cutoff_date]\nreturn returns\nexcept Exception as e:\nlogger.error(f\"è®¡ç®—æ”¶ç›Šç‡å¤±è´¥ {alpha_id}: {e}\")\nreturn None\ndef get_alpha_pnl_series(self, alpha_id: str, timeout_seconds: int = 3600) -> Optional[pd.Series]:\n\"\"\"è·å–alphaçš„PNLåºåˆ—ï¼ˆç”¨äºç»˜å›¾ï¼‰ï¼Œæ”¯æŒ1å°æ—¶è¶…æ—¶é™åˆ¶\"\"\"\n# æ ¹æ®é…ç½®é€‰æ‹©æ•°æ®æº\nif self.use_server_data:\npnl_data = self.get_pnl_from_server(alpha_id, timeout_seconds=timeout_seconds)\nelse:\npnl_data = self.db_manager.get_pnl_data(alpha_id)\nif not pnl_data:\nreturn None\ntry:\nrecords = pnl_data['records']\ncolumns = [\"date\", \"pnl\"] + [f\"col{i}\" for i in range(2, len(records[0]))]\ndf = pd.DataFrame(records, columns=columns)\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.set_index('date').sort_index()\nreturn df['pnl']\nexcept Exception as e:\nlogger.error(f\"è·å–PNLåºåˆ—å¤±è´¥ {alpha_id}: {e}\")\nreturn None\ndef calculate_correlation_matrix(self, alpha_ids: List[str], progress_callback=None) -> pd.DataFrame:\n\"\"\"è®¡ç®—alphaé—´çš„ç›¸å…³æ€§çŸ©é˜µ\"\"\"\nlogger.info(f\"å¼€å§‹è®¡ç®— {len(alpha_ids)} ä¸ªalphaçš„ç›¸å…³æ€§çŸ©é˜µ\")\nreturns_data = {}\ntotal_alphas = len(alpha_ids)\nfor i, alpha_id in enumerate(alpha_ids, 1):\nif progress_callback:\nprogress_callback(f\"æ­£åœ¨è·å–ç¬¬ {i}/{total_alphas} ä¸ªAlphaçš„PNLæ•°æ®: {alpha_id}\")\nreturns = self.get_alpha_returns(alpha_id)\nif returns is not None and len(returns) > 0:\nreturns_data[alpha_id] = returns\nif len(returns_data) < 2:\nlogger.warning(\"æœ‰æ•ˆalphaæ•°é‡ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§\")\nreturn pd.DataFrame()\nreturns_df = pd.DataFrame(returns_data)\nreturns_df = returns_df.dropna()\nif returns_df.empty:\nlogger.warning(\"æ²¡æœ‰é‡å çš„æ—¶é—´åºåˆ—æ•°æ®\")\nreturn pd.DataFrame()\ncorrelation_matrix = returns_df.corr()\nlogger.info(f\"ç›¸å…³æ€§çŸ©é˜µè®¡ç®—å®Œæˆï¼Œç»´åº¦: {correlation_matrix.shape}\")\nreturn correlation_matrix\nclass MaximalCliqueDetector:\n\"\"\"æœ€å¤§å›¢æ£€æµ‹ç±»\"\"\"\ndef __init__(self, correlation_threshold: float = Config.CORRELATION_THRESHOLD):\nself.correlation_threshold = correlation_threshold\ndef build_correlation_graph(self, correlation_matrix: pd.DataFrame) -> nx.Graph:\n\"\"\"æ ¹æ®ç›¸å…³æ€§çŸ©é˜µæ„å»ºå›¾\"\"\"\nG = nx.Graph()\nalpha_ids = correlation_matrix.index.tolist()\nG.add_nodes_from(alpha_ids)\nfor i, alpha1 in enumerate(alpha_ids):\nfor j, alpha2 in enumerate(alpha_ids[i+1:], i+1):\ncorrelation = correlation_matrix.loc[alpha1, alpha2]\nif abs(correlation) > self.correlation_threshold:\nG.add_edge(alpha1, alpha2, weight=correlation)\nlogger.info(f\"æ„å»ºç›¸å…³æ€§å›¾å®Œæˆ: {G.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {G.number_of_edges()} æ¡è¾¹\")\nreturn G\ndef find_maximal_cliques(self, G: nx.Graph) -> List[List[str]]:\n\"\"\"å¯»æ‰¾ä¸é‡å çš„æœ€å¤§å›¢\"\"\"\n# è·å–æ‰€æœ‰æœ€å¤§å›¢\nall_cliques = list(nx.find_cliques(G))\n# è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰å¤§å°æ’åºï¼ˆä»å¤§åˆ°å°ï¼‰\nall_cliques = [list(clique) for clique in all_cliques if len(clique) > 1]\nall_cliques.sort(key=len, reverse=True)\n# è´ªå¿ƒç®—æ³•é€‰æ‹©ä¸é‡å çš„å›¢\nselected_cliques = []\nused_nodes = set()\nfor clique in all_cliques:\n# æ£€æŸ¥è¿™ä¸ªå›¢æ˜¯å¦ä¸å·²é€‰æ‹©çš„å›¢æœ‰é‡å \nif not any(node in used_nodes for node in clique):\nselected_cliques.append(clique)\nused_nodes.update(clique)\n# æ·»åŠ ç‹¬ç«‹èŠ‚ç‚¹ï¼ˆæ²¡æœ‰è¿æ¥ä¸”æœªè¢«åŒ…å«åœ¨ä»»ä½•å›¢ä¸­çš„èŠ‚ç‚¹ï¼‰\nisolated_nodes = list(nx.isolates(G))\nfor node in isolated_nodes:\nif node not in used_nodes:\nselected_cliques.append([node])\nused_nodes.add(node)\n# æ·»åŠ å‰©ä½™æœªåˆ†ç»„çš„èŠ‚ç‚¹ä½œä¸ºå•ç‹¬çš„å›¢\nall_nodes = set(G.nodes())\nremaining_nodes = all_nodes - used_nodes\nfor node in remaining_nodes:\nselected_cliques.append([node])\nlogger.info(f\"å‘ç° {len(selected_cliques)} ä¸ªä¸é‡å çš„å›¢ï¼Œå…¶ä¸­ {len(isolated_nodes)} ä¸ªç‹¬ç«‹èŠ‚ç‚¹\")\nreturn selected_cliques\ndef detect_cliques(self, correlation_matrix: pd.DataFrame) -> List[List[str]]:\n\"\"\"æ£€æµ‹å›¢ç»“æ„çš„ä¸»å‡½æ•°\"\"\"\nif correlation_matrix.empty:\nreturn []\nG = self.build_correlation_graph(correlation_matrix)\ncliques = self.find_maximal_cliques(G)\nreturn cliques\nclass PNLAnalyzer:\n\"\"\"PNLåˆ†æç±» - ä½¿ç”¨WorldQuant Brainå®˜æ–¹yearly-stats API\"\"\"\ndef __init__(self, db_manager: DatabaseManager):\nself.db_manager = db_manager\nself.brain_api_url = \"\nhttps://api.worldquantbrain.com\n\"\n# æ·»åŠ å¹´åº¦ç»Ÿè®¡æ•°æ®ç¼“å­˜ï¼Œé¿å…é‡å¤APIè°ƒç”¨\nself._yearly_stats_cache = {}\n# æ·»åŠ æ°¸ä¹…å¤±è´¥ç¼“å­˜ï¼Œé¿å…å¯¹æ˜ç¡®æ— æ•°æ®çš„Alphaé‡å¤è¯·æ±‚\nself._permanent_failures = set()\n# æ·»åŠ æš‚æ—¶å¤±è´¥è®°å½•ï¼Œå…è®¸åç»­é‡è¯•\nself._temp_failure_count = {}\ndef get_alpha_yearly_stats(self, alpha_id: str, timeout_seconds: int = 3600, use_cache: bool = True) -> pd.DataFrame:\n\"\"\"\nç›´æ¥ä»WorldQuant Brain APIè·å–å¹´åº¦ç»Ÿè®¡æ•°æ®ï¼Œæ”¯æŒ1å°æ—¶è¶…æ—¶é™åˆ¶\nArgs:\nalpha_id (str): Alpha ID\ntimeout_seconds (int): æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤3600ç§’ï¼ˆ1å°æ—¶ï¼‰\nuse_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤True\nReturns:\npd.DataFrame: å¹´åº¦ç»Ÿè®¡æ•°æ®ï¼Œè¶…æ—¶æˆ–å¤±è´¥è¿”å›ç©ºDataFrame\n\"\"\"\n# æ£€æŸ¥ç¼“å­˜\nif use_cache and alpha_id in self._yearly_stats_cache:\nlogger.info(f\"Alpha {alpha_id} ä½¿ç”¨ç¼“å­˜çš„å¹´åº¦ç»Ÿè®¡æ•°æ®\")\nreturn self._yearly_stats_cache[alpha_id]\n# æ£€æŸ¥æ˜¯å¦ä¸ºæ°¸ä¹…å¤±è´¥ï¼ˆæ˜ç¡®æ— æ•°æ®çš„Alphaï¼‰\nif use_cache and alpha_id in self._permanent_failures:\nlogger.info(f\"Alpha {alpha_id} å·²çŸ¥æ— å¹´åº¦ç»Ÿè®¡æ•°æ®ï¼Œè·³è¿‡\")\nreturn pd.DataFrame()\n# æ£€æŸ¥æš‚æ—¶å¤±è´¥æ¬¡æ•°ï¼Œå¦‚æœå¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œæš‚æ—¶è·³è¿‡\ntemp_failures = self._temp_failure_count.get(alpha_id, 0)\nif temp_failures >= 3: Â # è¿ç»­3æ¬¡æš‚æ—¶å¤±è´¥åï¼Œæš‚åœä¸€æ¬¡\nlogger.info(f\"Alpha {alpha_id} è¿ç»­å¤±è´¥{temp_failures}æ¬¡ï¼Œæš‚åœè¯·æ±‚\")\nreturn pd.DataFrame()\nstart_time = time.time()\ns = get_session()\nmax_retries = 10 Â # å¢åŠ é‡è¯•æ¬¡æ•°åˆ°10æ¬¡ï¼Œæé«˜æˆåŠŸç‡\nmax_wait_time = 60 Â # å•æ¬¡æœ€å¤§ç­‰å¾…æ—¶é—´\nretry_count = 0\nwhile retry_count < max_retries:\n# æ£€æŸ¥æ˜¯å¦è¶…è¿‡æ€»è¶…æ—¶æ—¶é—´\nelapsed_time = time.time() - start_time\nif elapsed_time > timeout_seconds:\nlogger.warning(f\"Alpha {alpha_id} è·å–å¹´åº¦ç»Ÿè®¡æ•°æ®è¶…æ—¶ï¼ˆ{timeout_seconds}ç§’ï¼‰ï¼Œè·³è¿‡\")\nreturn pd.DataFrame()\ntry:\n# è®¡ç®—å‰©ä½™è¶…æ—¶æ—¶é—´\nremaining_timeout = min(30, timeout_seconds - elapsed_time)\nif remaining_timeout <= 0:\nlogger.warning(f\"Alpha {alpha_id} å‰©ä½™æ—¶é—´ä¸è¶³ï¼Œè·³è¿‡\")\nreturn pd.DataFrame()\nresult = s.get(\nf\"{self.brain_api_url}/alphas/{alpha_id}/recordsets/yearly-stats\",\ntimeout=remaining_timeout\n)\nif \"retry-after\" in result.headers:\nretry_after = float(result.headers[\"Retry-After\"])\n# ä¼˜å…ˆä½¿ç”¨æœåŠ¡å™¨æŒ‡å®šçš„é‡è¯•æ—¶é—´ï¼ŒæœåŠ¡å™¨æœ€æ¸…æ¥šè‡ªå·±çš„è´Ÿè½½æƒ…å†µ\nwait_time = min(retry_after, max_wait_time)\n# æ£€æŸ¥ç­‰å¾…æ—¶é—´æ˜¯å¦ä¼šå¯¼è‡´æ€»è¶…æ—¶\nif elapsed_time + wait_time > timeout_seconds:\nlogger.warning(f\"Alpha {alpha_id} ç­‰å¾…æ—¶é—´ä¼šå¯¼è‡´è¶…æ—¶ï¼Œè·³è¿‡\")\n# è®°å½•ä¸ºæš‚æ—¶å¤±è´¥\nif use_cache:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nreturn pd.DataFrame()\nlogger.info(f\"Alpha {alpha_id} éœ€è¦ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•ï¼ˆç¬¬ {retry_count + 1}/{max_retries} æ¬¡ï¼‰\")\ntime.sleep(wait_time)\nretry_count += 1\ncontinue\nelse:\nbreak\nexcept requests.exceptions.Timeout:\nlogger.warning(f\"Alpha {alpha_id} è¯·æ±‚è¶…æ—¶ï¼Œé‡è¯• {retry_count + 1}/{max_retries}\")\nretry_count += 1\nif retry_count < max_retries:\n# ä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥\nexponential_wait = 2 ** retry_count\nwait_time = min(exponential_wait, 30) Â # æœ€å¤šç­‰å¾…30ç§’\n# æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ—¶é—´é‡è¯•\nif time.time() - start_time + wait_time > timeout_seconds:\nlogger.warning(f\"Alpha {alpha_id} é‡è¯•ç­‰å¾…ä¼šå¯¼è‡´è¶…æ—¶ï¼Œè·³è¿‡\")\n# è®°å½•ä¸ºæš‚æ—¶å¤±è´¥\nif use_cache:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nreturn pd.DataFrame()\nlogger.info(f\"Alpha {alpha_id} ç­‰å¾… {wait_time} ç§’åé‡è¯•\")\ntime.sleep(wait_time)\ncontinue\nexcept Exception as e:\nlogger.error(f\"è¯·æ±‚Alpha {alpha_id} yearly-statså¤±è´¥: {e}\")\nretry_count += 1\nif retry_count < max_retries:\n# ä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥\nexponential_wait = 2 ** retry_count\nwait_time = min(exponential_wait, 30) Â # æœ€å¤šç­‰å¾…30ç§’\nif time.time() - start_time + wait_time > timeout_seconds:\nlogger.warning(f\"Alpha {alpha_id} é‡è¯•ç­‰å¾…ä¼šå¯¼è‡´è¶…æ—¶ï¼Œè·³è¿‡\")\n# è®°å½•ä¸ºæš‚æ—¶å¤±è´¥\nif use_cache:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nreturn pd.DataFrame()\nlogger.info(f\"Alpha {alpha_id} ç­‰å¾… {wait_time} ç§’åé‡è¯•\")\ntime.sleep(wait_time)\ncontinue\nelse:\n# è®°å½•ä¸ºæš‚æ—¶å¤±è´¥\nif use_cache:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nreturn pd.DataFrame()\nif retry_count >= max_retries:\nlogger.error(f\"Alpha {alpha_id} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæš‚æ—¶æ”¾å¼ƒè¯·æ±‚\")\n# è®°å½•ä¸ºæš‚æ—¶å¤±è´¥ï¼Œä¸ç¼“å­˜ç©ºç»“æœï¼Œå…è®¸åç»­é‡è¯•\nif use_cache:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nlogger.debug(f\"Alpha {alpha_id} æš‚æ—¶å¤±è´¥è®°å½•: {self._temp_failure_count[alpha_id]} æ¬¡\")\nreturn pd.DataFrame()\nif result.status_code != 200:\nlogger.error(f\"Alpha {alpha_id} yearly-stats APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {result.status_code}\")\n# 404è¡¨ç¤ºAlphaä¸å­˜åœ¨ï¼Œè®°å½•ä¸ºæ°¸ä¹…å¤±è´¥ï¼›å…¶ä»–é”™è¯¯è®°å½•ä¸ºæš‚æ—¶å¤±è´¥\nif use_cache:\nif result.status_code == 404:\nself._permanent_failures.add(alpha_id)\nlogger.debug(f\"Alpha {alpha_id} ä¸å­˜åœ¨ï¼Œè®°å½•ä¸ºæ°¸ä¹…å¤±è´¥\")\nelse:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nlogger.debug(f\"Alpha {alpha_id} HTTPé”™è¯¯ï¼Œè®°å½•ä¸ºæš‚æ—¶å¤±è´¥\")\nreturn pd.DataFrame()\ntry:\nstats = result.json()\nexcept Exception as e:\nlogger.error(f\"Alpha {alpha_id} è§£æå“åº”JSONå¤±è´¥: {e}\")\n# JSONè§£æå¤±è´¥é€šå¸¸æ˜¯æš‚æ—¶é—®é¢˜\nif use_cache:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nlogger.debug(f\"Alpha {alpha_id} JSONè§£æå¤±è´¥ï¼Œè®°å½•ä¸ºæš‚æ—¶å¤±è´¥\")\nreturn pd.DataFrame()\nif stats.get(\"records\", 0) == 0:\nlogger.warning(f\"Alpha {alpha_id} æ²¡æœ‰æ‰¾åˆ°å¹´åº¦ç»Ÿè®¡æ•°æ®\")\n# æ²¡æœ‰æ•°æ®è®°å½•é€šå¸¸æ„å‘³ç€è¿™ä¸ªAlphaç¡®å®æ²¡æœ‰å¹´åº¦æ•°æ®ï¼Œè®°å½•ä¸ºæ°¸ä¹…å¤±è´¥\nif use_cache:\nself._permanent_failures.add(alpha_id)\nlogger.debug(f\"Alpha {alpha_id} æ— å¹´åº¦æ•°æ®ï¼Œè®°å½•ä¸ºæ°¸ä¹…å¤±è´¥\")\nreturn pd.DataFrame()\ntry:\ncolumns = [dct[\"name\"] for dct in stats[\"schema\"][\"properties\"]]\nyearly_stats_df = pd.DataFrame(stats[\"records\"], columns=columns)\nlogger.info(f\"Alpha {alpha_id} å¹´åº¦ç»Ÿè®¡æ•°æ®è·å–æˆåŠŸ\")\n# ç¼“å­˜ç»“æœ\nif use_cache:\nself._yearly_stats_cache[alpha_id] = yearly_stats_df\n# æˆåŠŸè·å–æ•°æ®åï¼Œæ¸…ç†å¤±è´¥è®°å½•\nif alpha_id in self._temp_failure_count:\ndel self._temp_failure_count[alpha_id]\nlogger.debug(f\"Alpha {alpha_id} å¹´åº¦ç»Ÿè®¡æ•°æ®å·²ç¼“å­˜\")\nreturn yearly_stats_df\nexcept Exception as e:\nlogger.error(f\"Alpha {alpha_id} æ„å»ºDataFrameå¤±è´¥: {e}\")\n# DataFrameæ„å»ºå¤±è´¥é€šå¸¸æ˜¯æ•°æ®æ ¼å¼é—®é¢˜ï¼Œè®°å½•ä¸ºæš‚æ—¶å¤±è´¥\nif use_cache:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nlogger.debug(f\"Alpha {alpha_id} DataFrameæ„å»ºå¤±è´¥ï¼Œè®°å½•ä¸ºæš‚æ—¶å¤±è´¥\")\nreturn pd.DataFrame()\ndef reset_temp_failures(self, alpha_id: str = None):\n\"\"\"é‡ç½®æš‚æ—¶å¤±è´¥è®°å½•ï¼Œå…è®¸é‡æ–°å°è¯•\nArgs:\nalpha_id (str, optional): æŒ‡å®šAlpha IDé‡ç½®ï¼Œä¸ºNoneæ—¶é‡ç½®æ‰€æœ‰\n\"\"\"\nif alpha_id:\nif alpha_id in self._temp_failure_count:\ndel self._temp_failure_count[alpha_id]\nlogger.info(f\"å·²é‡ç½®Alpha {alpha_id} çš„æš‚æ—¶å¤±è´¥è®°å½•\")\nelse:\nself._temp_failure_count.clear()\nlogger.info(\"å·²é‡ç½®æ‰€æœ‰Alphaçš„æš‚æ—¶å¤±è´¥è®°å½•\")\ndef get_failure_status(self, alpha_id: str) -> dict:\n\"\"\"è·å–Alphaçš„å¤±è´¥çŠ¶æ€ä¿¡æ¯\"\"\"\nreturn {\n'is_permanent_failure': alpha_id in self._permanent_failures,\n'temp_failure_count': self._temp_failure_count.get(alpha_id, 0),\n'is_cached': alpha_id in self._yearly_stats_cache\n}\ndef calculate_annual_metrics(self, pnl_series: pd.Series = None, alpha_id: str = None) -> Dict[str, Dict[str, float]]:\n\"\"\"\nè·å–å¹´åº¦æŒ‡æ ‡ - ä¼˜å…ˆä½¿ç”¨å®˜æ–¹APIï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°PNLè®¡ç®—\n\"\"\"\nif alpha_id:\n# å°è¯•ä½¿ç”¨å®˜æ–¹APIï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰\nyearly_stats = self.get_alpha_yearly_stats(alpha_id, use_cache=True)\nif not yearly_stats.empty:\nreturn self._parse_official_yearly_stats(yearly_stats)\n# å¦‚æœAPIå¤±è´¥ï¼Œå›é€€åˆ°PNLè®¡ç®—ï¼ˆä»…ç”¨äºç”»å›¾ï¼‰\nif pnl_series is not None:\nlogger.warning(f\"Alpha {alpha_id} ä½¿ç”¨PNLæ•°æ®è®¡ç®—å¹´åº¦æŒ‡æ ‡ï¼ˆä»…ä¾›å‚è€ƒï¼‰\")\nreturn self._calculate_from_pnl(pnl_series)\nreturn {}\ndef _parse_official_yearly_stats(self, yearly_stats: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n\"\"\"\nè§£æå®˜æ–¹å¹´åº¦ç»Ÿè®¡æ•°æ®\n\"\"\"\nannual_metrics = {}\nfor _, row in yearly_stats.iterrows():\nyear_str = str(int(row.get('year', 0)))\nif year_str == '0':\ncontinue\n# è§£æå„é¡¹æŒ‡æ ‡ï¼Œå¤„ç†å¯èƒ½çš„å­—æ®µåå˜åŒ–\nmetrics = {\n'return': self._safe_get_metric(row, ['returns', 'return', 'annual_return']) * 100, Â # è½¬æ¢ä¸ºç™¾åˆ†æ¯”\n'sharpe': self._safe_get_metric(row, ['sharpe', 'sharpe_ratio', 'sr']),\n'drawdown': self._safe_get_metric(row, ['drawdown', 'max_drawdown', 'mdd']) * 100, Â # è½¬æ¢ä¸ºç™¾åˆ†æ¯”\n'turnover': self._safe_get_metric(row, ['turnover']) * 100, Â # è½¬æ¢ä¸ºç™¾åˆ†æ¯”\n'fitness': self._safe_get_metric(row, ['fitness']),\n'margin': self._safe_get_metric(row, ['margin']) * 10000, Â # è½¬æ¢ä¸ºä¸‡åˆ†æ¯”\n'longCount': self._safe_get_metric(row, ['longCount', 'long_count']),\n'shortCount': self._safe_get_metric(row, ['shortCount', 'short_count'])\n}\nannual_metrics[year_str] = metrics\nreturn annual_metrics\ndef _safe_get_metric(self, row: pd.Series, possible_names: List[str]) -> float:\n\"\"\"\nå®‰å…¨è·å–æŒ‡æ ‡å€¼ï¼Œå°è¯•å¤šä¸ªå¯èƒ½çš„å­—æ®µå\n\"\"\"\nfor name in possible_names:\nif name in row and pd.notna(row[name]):\nreturn float(row[name])\nreturn 0.0\ndef _calculate_from_pnl(self, pnl_series: pd.Series) -> Dict[str, Dict[str, float]]:\n\"\"\"\nä»PNLæ•°æ®è®¡ç®—å¹´åº¦æŒ‡æ ‡ï¼ˆå›é€€æ–¹æ¡ˆï¼Œä»…ä¾›å‚è€ƒï¼‰\n\"\"\"\nannual_metrics = {}\n# æŒ‰å¹´åˆ†ç»„\nfor year in pnl_series.index.year.unique():\nyear_data = pnl_series[pnl_series.index.year == year]\nif len(year_data) < 10: Â # æ•°æ®ç‚¹å¤ªå°‘è·³è¿‡\ncontinue\n# è®¡ç®—æ”¶ç›Šç‡\nreturns = year_data.pct_change().dropna()\nif len(returns) == 0:\ncontinue\n# è®¡ç®—å„é¡¹æŒ‡æ ‡\nannual_return = (year_data.iloc[-1] / year_data.iloc[0] - 1) * 100 if year_data.iloc[0] != 0 else 0\nvolatility = returns.std() * np.sqrt(252) Â # å¹´åŒ–æ³¢åŠ¨ç‡\nsharpe = returns.mean() / returns.std() * np.sqrt(252) if returns.std() > 0 else 0\n# è®¡ç®—æœ€å¤§å›æ’¤\ncumulative = (1 + returns).cumprod()\nrunning_max = cumulative.expanding().max()\ndrawdown = (cumulative / running_max - 1).min() * 100\n# è®¡ç®—æ¢æ‰‹ç‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰\nturnover = returns.abs().mean() * 252 * 100\nannual_metrics[str(year)] = {\n'return': annual_return,\n'sharpe': sharpe,\n'drawdown': abs(drawdown),\n'turnover': turnover,\n'volatility': volatility,\n'fitness': 0, Â # PNLæ— æ³•è®¡ç®—fitness\n'margin': 0, Â  # PNLæ— æ³•è®¡ç®—margin\n'longCount': 0, Â # PNLæ— æ³•è®¡ç®—longCount\n'shortCount': 0 Â # PNLæ— æ³•è®¡ç®—shortCount\n}\nreturn annual_metrics\nclass MaximalCliqueAnalyzer:\n\"\"\"æœ€å¤§å›¢Alphaåˆ†æå™¨ä¸»ç±»\nè¯¥ç±»æ•´åˆäº†æ•°æ®è·å–ã€ç›¸å…³æ€§è®¡ç®—ã€æœ€å¤§å›¢æ£€æµ‹å’Œå¯è§†åŒ–åŠŸèƒ½ï¼Œ\nä¸ºAlphaç›¸å…³æ€§åˆ†ææä¾›å®Œæ•´çš„è§£å†³æ–¹æ¡ˆã€‚\n\"\"\"\ndef __init__(self, use_server_data: bool = False):\nself.db_manager = DatabaseManager()\nself.correlation_calculator = CorrelationCalculator(self.db_manager, use_server_data)\nself.clique_detector = MaximalCliqueDetector()\nself.pnl_analyzer = PNLAnalyzer(self.db_manager)\nself.use_server_data = use_server_data\nself.session = get_session() if use_server_data else None\n# ç¼“å­˜ç®¡ç†ï¼Œé¿å…é‡å¤è·å–ç›¸åŒæ•°æ®\nself._preloaded_alphas = set()\ndef find_related_alphas(self, alpha_id: str, limit: int = 100) -> List[str]:\n\"\"\"æŸ¥æ‰¾ä¸ç»™å®šalphaç›¸å…³çš„å…¶ä»–alpha\"\"\"\nif self.use_server_data:\n# å½“ä½¿ç”¨æœåŠ¡å™¨æ•°æ®æ—¶ï¼Œå®Œå…¨é¿å…æ•°æ®åº“è°ƒç”¨\nlogger.info(f\"ä½¿ç”¨æœåŠ¡å™¨æ•°æ®æ¨¡å¼ï¼Œé¿å…æ•°æ®åº“è°ƒç”¨\")\nif not self.session:\nlogger.warning(\"æœåŠ¡å™¨sessionæœªåˆå§‹åŒ–ï¼Œè¿”å›å•ä¸ªalpha\")\nreturn [alpha_id]\n# è·å–å½“å‰alphaçš„regionä¿¡æ¯ï¼ˆä»æœåŠ¡å™¨ï¼‰\nalpha_data = self.db_manager.get_alpha_data_from_server(alpha_id, self.session)\nif not alpha_data or not alpha_data.get('region'):\nlogger.warning(f\"æ— æ³•è·å–alpha {alpha_id} çš„regionä¿¡æ¯ï¼Œè¿”å›å•ä¸ªalpha\")\nreturn [alpha_id]\ntarget_region = alpha_data['region']\nlogger.info(f\"Alpha {alpha_id} çš„region: {target_region}\")\n# åœ¨æœåŠ¡å™¨æ•°æ®æ¨¡å¼ä¸‹ï¼Œç”±äºAPIé™åˆ¶å’Œé¿å…æ•°æ®åº“è°ƒç”¨çš„è¦æ±‚\n# ç›´æ¥è¿”å›å•ä¸ªalphaï¼Œç¡®ä¿ä¸ä¼šè§¦å‘ä»»ä½•æ•°æ®åº“æ“ä½œ\nlogger.info(f\"æœåŠ¡å™¨æ•°æ®æ¨¡å¼ï¼šè¿”å›å•ä¸ªalpha {alpha_id}ï¼Œé¿å…æ•°æ®åº“æŸ¥è¯¢\")\nreturn [alpha_id]\n# ä»…åœ¨éæœåŠ¡å™¨æ•°æ®æ¨¡å¼ä¸‹ä½¿ç”¨æ•°æ®åº“\nconn = self.db_manager.get_connection()\nif not conn:\nreturn [alpha_id]\ntry:\nwith conn.cursor() as cursor:\n# è·å–åŒä¸€regionçš„alpha\nsql_region = \"SELECT region FROM alpha_data WHERE id = %s\"\ncursor.execute(sql_region, (alpha_id,))\nregion_result = cursor.fetchone()\nif not region_result:\nreturn [alpha_id]\nregion = region_result['region']\n# æŸ¥æ‰¾åŒregionçš„å…¶ä»–alphaï¼ˆé™åˆ¶æ•°é‡ï¼‰\nsql_related = \"\"\"\nSELECT id FROM alpha_data\nWHERE region = %s AND id != %s\nORDER BY RAND()\nLIMIT %s\n\"\"\"\ncursor.execute(sql_related, (region, alpha_id, limit - 1))\nrelated_results = cursor.fetchall()\nrelated_alphas = [alpha_id] Â # åŒ…å«åŸå§‹alpha\nfor result in related_results:\nrelated_alphas.append(result['id'])\nlogger.info(f\"æ‰¾åˆ° {len(related_alphas)} ä¸ªç›¸å…³alphaï¼ˆregion: {region}ï¼‰\")\nreturn related_alphas\nexcept Exception as e:\nlogger.error(f\"æŸ¥æ‰¾ç›¸å…³alphaå¤±è´¥: {e}\")\nreturn [alpha_id]\nfinally:\nconn.close()\ndef process_alpha_cliques(self, alpha_id: str) -> Tuple[List[List[str]], pd.DataFrame]:\n\"\"\"å¤„ç†alphaçš„å›¢ç»“æ„\"\"\"\n# æŸ¥æ‰¾ç›¸å…³alpha\nrelated_alphas = self.find_related_alphas(alpha_id)\nif len(related_alphas) < 2:\nlogger.info(f\"åªæ‰¾åˆ° {len(related_alphas)} ä¸ªç›¸å…³alphaï¼Œæ— æ³•è®¡ç®—å›¢ç»“æ„\")\nreturn [[alpha_id]], pd.DataFrame()\n# è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ\ncorrelation_matrix = self.correlation_calculator.calculate_correlation_matrix(related_alphas)\nif correlation_matrix.empty:\nreturn [[alpha_id]], correlation_matrix\n# æ£€æµ‹å›¢ç»“æ„\ncliques = self.clique_detector.detect_cliques(correlation_matrix)\nreturn cliques, correlation_matrix\ndef create_pnl_plot(self, clique: List[str]) -> go.Figure:\n\"\"\"åˆ›å»ºPNLæ›²çº¿å›¾\"\"\"\nfig = go.Figure()\ncolors = px.colors.qualitative.Set1\nvalid_alphas = 0\nfor i, alpha_id in enumerate(clique):\n# è·å–PNLæ•°æ®\npnl_series = self.correlation_calculator.get_alpha_pnl_series(alpha_id)\nif pnl_series is None or len(pnl_series) == 0:\nlogger.warning(f\"Alpha {alpha_id} æ— æœ‰æ•ˆPNLæ•°æ®\")\ncontinue\n# è·å–å¹´åº¦ç»Ÿè®¡æ•°æ®å¹¶æ ¼å¼åŒ–ä¸ºæ‚¬åœæ–‡æœ¬ï¼ˆä¼˜åŒ–ï¼šåªåˆ›å»ºä¸€æ¬¡ï¼Œä¸ä¸ºæ¯ä¸ªç‚¹é‡å¤åˆ›å»ºï¼‰\nalpha_hover_text = self.get_alpha_yearly_stats_formatted(alpha_id)\n# å°†æ¢è¡Œç¬¦è½¬æ¢ä¸ºHTMLçš„<br>æ ‡ç­¾ä»¥ç¡®ä¿æ­£ç¡®æ˜¾ç¤º\nformatted_table = alpha_hover_text.replace('\\n', '<br>')\n# åˆ›å»ºä¼˜åŒ–çš„æ‚¬åœæ¨¡æ¿ï¼Œåªæ˜¾ç¤ºAlphaåç§°å’Œå¹´åº¦ç»Ÿè®¡æ•°æ®ï¼Œä¸æ˜¾ç¤ºå…·ä½“çš„æ—¶é—´å’ŒPNLå€¼\nhover_template = f\"<b>Alpha: {alpha_id}</b><br><br><b>å¹´åº¦ç»Ÿè®¡æ•°æ®:</b><br><span style='font-family: monospace; font-size: 11px; line-height: 1.4; white-space: pre;'>{formatted_table}</span><extra></extra>\"\n# æ·»åŠ æ›²çº¿ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨è‡ªå®šä¹‰hovertemplateè€Œä¸æ˜¯ä¸ºæ¯ä¸ªç‚¹åˆ›å»ºtextï¼‰\nfig.add_trace(go.Scatter(\nx=pnl_series.index,\ny=pnl_series.values,\nmode='lines',\nname=alpha_id,\nline=dict(color=colors[i % len(colors)], width=2),\nhovertemplate=hover_template,\nconnectgaps=True Â # è¿æ¥ç¼ºå¤±æ•°æ®ç‚¹\n))\nvalid_alphas += 1\n# æ›´æ–°å¸ƒå±€\ntitle = f'Alpha PNLæ›²çº¿å›¾ ({valid_alphas}/{len(clique)} ä¸ªæœ‰æ•ˆAlpha)'\nfig.update_layout(\ntitle=title,\nxaxis_title='æ—¥æœŸ',\nyaxis_title='PNL',\nhovermode='closest',\nshowlegend=True,\nwidth=600,\nheight=900,\nxaxis=dict(\nshowgrid=True,\ngridwidth=1,\ngridcolor='lightgray'\n),\nyaxis=dict(\nshowgrid=True,\ngridwidth=1,\ngridcolor='lightgray'\n)\n)\nif valid_alphas == 0:\n# å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œæ˜¾ç¤ºæç¤ºä¿¡æ¯\nfig.add_annotation(\ntext=\"æ— æœ‰æ•ˆPNLæ•°æ®å¯æ˜¾ç¤º\",\nxref=\"paper\", yref=\"paper\",\nx=0.5, y=0.5,\nshowarrow=False,\nfont=dict(size=16, color=\"red\")\n)\nreturn fig\ndef get_alpha_yearly_stats_formatted(self, alpha_id: str) -> str:\n\"\"\"è·å–æ ¼å¼åŒ–çš„Alphaå¹´åº¦ç»Ÿè®¡ä¿¡æ¯\"\"\"\n# ç›´æ¥ä½¿ç”¨å·²ç¼“å­˜çš„æ•°æ®ï¼Œé¿å…é‡å¤APIè°ƒç”¨\nyearly_stats_df = self.pnl_analyzer.get_alpha_yearly_stats(alpha_id, use_cache=True)\nif yearly_stats_df.empty or 'year' not in yearly_stats_df.columns:\nreturn \"No yearly statistics data available\"\n# æŒ‰å¹´ä»½æ’åº\nyearly_stats_sorted = yearly_stats_df.sort_values('year')\n# å®šä¹‰è¡¨æ ¼åˆ—é¡ºåºå’Œæ ¼å¼ - æŒ‰ç…§ç”¨æˆ·è¦æ±‚çš„é¡ºåºæ’åˆ—\ntable_columns = [\n('year', 'Year', '', 0, 10),\n('sharpe', 'Sharpe', '', 2, 12),\n('turnover', 'Turnover(%)', '*100', 2, 18),\n('fitness', 'Fitness', '', 2, 15),\n('returns', 'Returns(%)', '*100', 2, 18),\n('drawdown', 'Drawdown(%)', '*100', 2, 18),\n('margin', 'Margin(â€±)', '*10000', 2, 18),\n('longCount', 'LongCnt', '', 0, 15),\n('shortCount', 'ShortCnt', '', 0, 15)\n]\n# æ„å»ºè¡¨æ ¼å¤´éƒ¨\nheader_parts = []\nfor _, display_name, _, _, width in table_columns:\nheader_parts.append(display_name.center(width))\nresult = \"|\".join(header_parts) + \"|\\n\"\n# æ·»åŠ åˆ†éš”çº¿\nseparator_parts = []\nfor _, _, _, _, width in table_columns:\nseparator_parts.append(\"-\" * width)\nresult += \"|\".join(separator_parts) + \"|\\n\"\n# æ·»åŠ æ€»ä½“æ•°æ®è¡Œï¼ˆä»alpha_dataè¡¨æŸ¥è¯¢ï¼‰\nif self.use_server_data and self.session:\nalpha_data = self.db_manager.get_alpha_data_from_server(alpha_id, self.session)\nelse:\nalpha_data = self.db_manager.get_alpha_data(alpha_id)\noverall_parts = []\nfor col_key, _, multiplier, decimals, width in table_columns:\nif col_key == 'year':\nformatted_value = 'all'\nelse:\n# æ˜ å°„åˆ—ååˆ°alpha_dataå­—æ®µ\nfield_mapping = {\n'sharpe': 'sharpe',\n'returns': 'returns',\n'drawdown': 'drawdown',\n'turnover': 'turnover',\n'fitness': 'fitness',\n'margin': 'margin',\n'longCount': 'long_count',\n'shortCount': 'short_count'\n}\nfield_name = field_mapping.get(col_key)\nif alpha_data and field_name and field_name in alpha_data and alpha_data[field_name] is not None:\ntry:\nvalue = alpha_data[field_name]\nfval = float(value)\nif multiplier == '*100':\nfval = fval * 100\nif decimals == 0:\nformatted_value = f\"{int(fval)}%\"\nelse:\nformatted_value = f\"{fval:.{decimals}f}%\"\nelif multiplier == '*10000':\nfval = fval * 10000\nif decimals == 0:\nformatted_value = f\"{int(fval)}â€±\"\nelse:\nformatted_value = f\"{fval:.{decimals}f}â€±\"\nelse:\nif decimals == 0:\nformatted_value = str(int(fval))\nelse:\nformatted_value = f\"{fval:.{decimals}f}\"\nexcept Exception:\nformatted_value = \"NAN\"\nelse:\nformatted_value = \"NAN\"\n# å±…ä¸­å¯¹é½å¹´ä»½ï¼Œå³å¯¹é½æ•°å­—\nif col_key == 'year':\noverall_parts.append(formatted_value.center(width))\nelse:\noverall_parts.append(formatted_value.rjust(width))\nresult += \"|\".join(overall_parts) + \"|\\n\"\n# æ„å»ºæ•°æ®è¡Œ\nfor _, year_row in yearly_stats_sorted.iterrows():\ndata_parts = []\nfor col_key, _, multiplier, decimals, width in table_columns:\n# æŸ¥æ‰¾åŒ¹é…çš„åˆ—åï¼ˆå¤„ç†å¤§å°å†™ä¸æ•æ„Ÿï¼‰\nactual_col = None\nfor col in yearly_stats_df.columns:\nif col.lower() == col_key.lower() or col.lower().replace('_', '') == col_key.lower():\nactual_col = col\nbreak\nif actual_col and actual_col in year_row and not pd.isna(year_row[actual_col]):\nvalue = year_row[actual_col]\ntry:\nif col_key == 'year':\nformatted_value = str(value)\nelse:\nfval = float(value)\nif multiplier == '*100':\nfval = fval * 100\nif decimals == 0:\nformatted_value = f\"{int(fval)}%\"\nelse:\nformatted_value = f\"{fval:.{decimals}f}%\"\nelif multiplier == '*10000':\nfval = fval * 10000\nif decimals == 0:\nformatted_value = f\"{int(fval)}â€±\"\nelse:\nformatted_value = f\"{fval:.{decimals}f}â€±\"\nelse:\nif decimals == 0:\nformatted_value = str(int(fval))\nelse:\nformatted_value = f\"{fval:.{decimals}f}\"\nexcept Exception:\nformatted_value = str(value)\nelse:\nformatted_value = \"-\"\n# å±…ä¸­å¯¹é½å¹´ä»½ï¼Œå³å¯¹é½æ•°å­—\nif col_key == 'year':\ndata_parts.append(formatted_value.center(width))\nelse:\ndata_parts.append(formatted_value.rjust(width))\nresult += \"|\".join(data_parts) + \"|\\n\"\nreturn result\ndef create_annual_metrics_table(self, clique: List[str], progress_callback=None) -> pd.DataFrame:\n\"\"\"åˆ›å»ºå¹´åº¦æŒ‡æ ‡è¡¨æ ¼ - ä½¿ç”¨å®˜æ–¹yearly-stats API\"\"\"\n# é¦–å…ˆé¢„åŠ è½½æ‰€æœ‰alphaçš„å¹´åº¦ç»Ÿè®¡æ•°æ®ï¼Œé¿å…é‡å¤APIè°ƒç”¨\nlogger.info(f\"å¼€å§‹ä¸ºå›¢é¢„åŠ è½½ {len(clique)} ä¸ªAlphaçš„å¹´åº¦ç»Ÿè®¡æ•°æ®\")\nself.preload_yearly_stats(clique, progress_callback)\nall_annual_data = []\ntotal_alphas = len(clique)\nfailed_count = 0\nmax_consecutive_failures = 3 Â # æœ€å¤§è¿ç»­å¤±è´¥æ¬¡æ•°\nfor i, alpha_id in enumerate(clique, 1):\n# å·²ç»é¢„åŠ è½½æ•°æ®ï¼Œä¸éœ€è¦æ›´æ–°è¿›åº¦æç¤º\ntry:\n# ä¼˜å…ˆä½¿ç”¨å®˜æ–¹APIè·å–å¹´åº¦æŒ‡æ ‡ï¼ˆä»ç¼“å­˜è·å–ï¼‰\nlogger.debug(f\"å¼€å§‹ä¸ºAlpha {alpha_id} è·å–å¹´åº¦æŒ‡æ ‡æ•°æ®\")\nannual_metrics = self.pnl_analyzer.calculate_annual_metrics(alpha_id=alpha_id)\n# å¦‚æœå®˜æ–¹APIå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨PNLæ•°æ®ä½œä¸ºå›é€€\nif not annual_metrics:\nlogger.debug(f\"Alpha {alpha_id} å®˜æ–¹APIå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨PNLæ•°æ®\")\npnl_series = self.correlation_calculator.get_alpha_pnl_series(alpha_id)\nif pnl_series is not None and len(pnl_series) > 0:\nannual_metrics = self.pnl_analyzer.calculate_annual_metrics(pnl_series=pnl_series, alpha_id=alpha_id)\nif not annual_metrics:\nlogger.warning(f\"Alpha {alpha_id} æ— æ³•è·å–å¹´åº¦æŒ‡æ ‡æ•°æ®ï¼Œè·³è¿‡\")\nfailed_count += 1\n# å¦‚æœè¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œæç¤ºç”¨æˆ·\nif failed_count >= max_consecutive_failures:\nlogger.warning(f\"è¿ç»­ {failed_count} ä¸ªAlphaè·å–æ•°æ®å¤±è´¥ï¼Œå¯èƒ½å­˜åœ¨ç½‘ç»œé—®é¢˜\")\nfailed_count = 0 Â # é‡ç½®è®¡æ•°å™¨\ncontinue\nelse:\nfailed_count = 0 Â # æˆåŠŸåé‡ç½®å¤±è´¥è®¡æ•°å™¨\nfor year, metrics in annual_metrics.items():\nyear_int = int(year)\n# åªåŒ…å«2013å¹´åŠä»¥åçš„æ•°æ®\nif year_int >= 2013:\n# åˆ›å»ºåŸºç¡€è¡Œæ•°æ®ï¼ŒåªåŒ…å«å¯ä»¥ä»PNLè®¡ç®—çš„æŒ‡æ ‡\nrow = {\n'Alpha': alpha_id,\n'Year': year_int,\n'Sharpe': round(metrics.get('sharpe', 0), 4),\n'Returns (%)': round(metrics.get('return', 0), 2),\n'Drawdown (%)': round(metrics.get('drawdown', 0), 2)\n}\n# åªæœ‰å½“æŒ‡æ ‡å€¼ä¸ä¸º0æ—¶æ‰æ·»åŠ å…¶ä»–æŒ‡æ ‡ï¼ˆé¿å…æ˜¾ç¤ºæ— æ„ä¹‰çš„0å€¼ï¼‰\nturnover = metrics.get('turnover', 0)\nif turnover > 0:\nrow['Turnover (%)'] = round(turnover, 2)\nfitness = metrics.get('fitness', 0)\nif fitness > 0:\nrow['Fitness'] = round(fitness, 4)\nmargin = metrics.get('margin', 0)\nif margin > 0:\nrow['Margin (â€±)'] = round(margin, 2)\nlong_count = metrics.get('longCount', 0)\nif long_count > 0:\nrow['Long Count'] = int(long_count)\nshort_count = metrics.get('shortCount', 0)\nif short_count > 0:\nrow['Short Count'] = int(short_count)\nall_annual_data.append(row)\nexcept Exception as e:\nlogger.error(f\"è·å–Alpha {alpha_id} å¹´åº¦æŒ‡æ ‡å¤±è´¥: {e}\")\ncontinue\ndf = pd.DataFrame(all_annual_data)\n# æŒ‰Alphaå’Œå¹´ä»½æ’åºï¼ˆæ¯ä¸ªalphaçš„å„å¹´æ•°æ®èšé›†åœ¨ä¸€èµ·ï¼‰\nif not df.empty:\ndf = df.sort_values(['Alpha', 'Year'], ascending=[True, True])\ndf = df.reset_index(drop=True)\n# åŠ¨æ€ç¡®å®šåˆ—çš„é¡ºåºï¼ŒæŒ‰ç…§ç”¨æˆ·è¦æ±‚çš„é¡ºåºæ’åˆ—\nbase_columns = ['Alpha', 'Year', 'Sharpe', 'Turnover (%)', 'Fitness', 'Returns (%)', 'Drawdown (%)']\noptional_columns = ['Margin (â€±)', 'Long Count', 'Short Count']\n# æ„å»ºæœ€ç»ˆçš„åˆ—é¡ºåº\ncolumn_order = base_columns + [col for col in optional_columns if col in df.columns]\ndf = df[column_order]\nreturn df\ndef preload_yearly_stats(self, alpha_ids: List[str], progress_callback=None):\n\"\"\"é¢„åŠ è½½å¹´åº¦ç»Ÿè®¡æ•°æ®ï¼Œé¿å…åç»­é‡å¤APIè°ƒç”¨\"\"\"\nlogger.info(f\"å¼€å§‹é¢„åŠ è½½ {len(alpha_ids)} ä¸ªAlphaçš„å¹´åº¦ç»Ÿè®¡æ•°æ®\")\n# è¿‡æ»¤å·²ç»åŠ è½½è¿‡çš„alpha\nalphas_to_load = [alpha_id for alpha_id in alpha_ids if alpha_id not in self._preloaded_alphas]\nif not alphas_to_load:\nlogger.info(\"æ‰€æœ‰Alphaçš„å¹´åº¦ç»Ÿè®¡æ•°æ®å·²é¢„åŠ è½½\")\nreturn\nlogger.info(f\"éœ€è¦åŠ è½½ {len(alphas_to_load)} ä¸ªæ–°Alphaçš„å¹´åº¦ç»Ÿè®¡æ•°æ®\")\ntotal_alphas = len(alphas_to_load)\nfor i, alpha_id in enumerate(alphas_to_load, 1):\nif progress_callback:\nprogress_callback(f\"é¢„åŠ è½½ç¬¬ {i}/{total_alphas} ä¸ªAlphaçš„å¹´åº¦ç»Ÿè®¡æ•°æ®: {alpha_id}\")\ntry:\n# ä½¿ç”¨ç¼“å­˜åŠ è½½å¹´åº¦ç»Ÿè®¡æ•°æ®\nself.pnl_analyzer.get_alpha_yearly_stats(alpha_id, use_cache=True)\nself._preloaded_alphas.add(alpha_id)\n# æ·»åŠ é€‚åº¦å»¶æ—¶ï¼Œé¿å…è¯·æ±‚è¿‡é¢‘\nif i < total_alphas: Â # æœ€åä¸€ä¸ªä¸ç”¨ç­‰å¾…\n# ä½¿ç”¨é€’å¢å»¶æ—¶ï¼Œæœ€åˆ0.2ç§’ï¼Œé€æ­¥å¢åŠ åˆ°æœ€å¥—1ç§’\ndelay = min(0.2 + (i-1) * 0.1, 1.0)\ntime.sleep(delay)\nexcept Exception as e:\nlogger.warning(f\"é¢„åŠ è½½Alpha {alpha_id} å¹´åº¦ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}\")\n# å³ä½¿å¤±è´¥ä¹Ÿæ ‡è®°ä¸ºå·²å°è¯•ï¼Œé¿å…åç»­é‡å¤å°è¯•\nself._preloaded_alphas.add(alpha_id)\ncontinue\nlogger.info(f\"å¹´åº¦ç»Ÿè®¡æ•°æ®é¢„åŠ è½½å®Œæˆ\")\ndef reset_yearly_stats_failures(self, alpha_ids: List[str] = None):\n\"\"\"é‡ç½®å¹´åº¦ç»Ÿè®¡æ•°æ®çš„å¤±è´¥çŠ¶æ€ï¼Œå…è®¸é‡æ–°å°è¯•\nArgs:\nalpha_ids (List[str], optional): æŒ‡å®šè¦é‡ç½®çš„Alpha IDåˆ—è¡¨ï¼Œä¸ºNoneæ—¶é‡ç½®æ‰€æœ‰\n\"\"\"\nif alpha_ids:\nfor alpha_id in alpha_ids:\nself.pnl_analyzer.reset_temp_failures(alpha_id)\n# ä»é¢„åŠ è½½é›†åˆä¸­ç§»é™¤ï¼Œå…è®¸é‡æ–°é¢„åŠ è½½\nself._preloaded_alphas.discard(alpha_id)\nlogger.info(f\"å·²é‡ç½® {len(alpha_ids)} ä¸ªAlphaçš„å¤±è´¥çŠ¶æ€\")\nelse:\nself.pnl_analyzer.reset_temp_failures()\nself._preloaded_alphas.clear()\nlogger.info(\"å·²é‡ç½®æ‰€æœ‰Alphaçš„å¤±è´¥çŠ¶æ€\")\ndef get_failed_alphas_info(self, alpha_ids: List[str]) -> dict:\n\"\"\"è·å–Alphaåˆ—è¡¨ä¸­å¤±è´¥çš„Alphaä¿¡æ¯\"\"\"\nfailed_info = {\n'permanent_failures': [],\n'temp_failures': [],\n'success': []\n}\nfor alpha_id in alpha_ids:\nstatus = self.pnl_analyzer.get_failure_status(alpha_id)\nif status['is_permanent_failure']:\nfailed_info['permanent_failures'].append(alpha_id)\nelif status['temp_failure_count'] > 0:\nfailed_info['temp_failures'].append({\n'alpha_id': alpha_id,\n'failure_count': status['temp_failure_count']\n})\nelif status['is_cached']:\nfailed_info['success'].append(alpha_id)\nreturn failed_info\ndef run_analysis(self, alpha_id: str):\n\"\"\"è¿è¡Œåˆ†æçš„ä¸»å‡½æ•°\"\"\"\nlogger.info(f\"å¼€å§‹åˆ†æalpha: {alpha_id}\")\n# å¤„ç†å›¢ç»“æ„\ncliques, correlation_matrix = self.process_alpha_cliques(alpha_id)\nlogger.info(f\"å‘ç° {len(cliques)} ä¸ªå›¢\")\nresults = []\nfor i, clique in enumerate(cliques):\nlogger.info(f\"å¤„ç†ç¬¬ {i+1} ä¸ªå›¢ï¼ŒåŒ…å« {len(clique)} ä¸ªalpha: {clique}\")\n# åˆ›å»ºPNLå›¾\npnl_fig = self.create_pnl_plot(clique)\n# åˆ›å»ºå¹´åº¦æŒ‡æ ‡è¡¨\nannual_table = self.create_annual_metrics_table(clique)\nresults.append({\n'clique': clique,\n'pnl_figure': pnl_fig,\n'annual_metrics': annual_table\n})\nreturn results\ndef run_batch_analysis(self, alpha_ids: List[str]):\n\"\"\"å¯¹alpha IDåˆ—è¡¨è¿›è¡Œæ‰¹é‡å›¢æ£€æµ‹åˆ†æ\"\"\"\nlogger.info(f\"å¼€å§‹æ‰¹é‡åˆ†æalphaåˆ—è¡¨: {alpha_ids}\")\n# é¢„åŠ è½½æ‰€æœ‰alphaçš„å¹´åº¦ç»Ÿè®¡æ•°æ®ï¼Œé¿å…åç»­é‡å¤APIè°ƒç”¨\nlogger.info(\"å¼€å§‹é¢„åŠ è½½æ‰¹é‡åˆ†æçš„å¹´åº¦ç»Ÿè®¡æ•°æ®\")\nself.preload_yearly_stats(alpha_ids)\n# ç›´æ¥å¯¹æä¾›çš„alphaåˆ—è¡¨è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ\ncorrelation_matrix = self.correlation_calculator.calculate_correlation_matrix(alpha_ids)\nif correlation_matrix.empty:\nlogger.warning(\"æ— æ³•è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ\")\nreturn []\n# æ£€æµ‹å›¢ç»“æ„\ncliques = self.clique_detector.detect_cliques(correlation_matrix)\nlogger.info(f\"å‘ç° {len(cliques)} ä¸ªå›¢\")\nresults = []\nfor i, clique in enumerate(cliques):\nlogger.info(f\"å¤„ç†ç¬¬ {i+1} ä¸ªå›¢ï¼ŒåŒ…å« {len(clique)} ä¸ªalpha: {clique}\")\n# åˆ›å»ºPNLå›¾\npnl_fig = self.create_pnl_plot(clique)\n# åˆ›å»ºå¹´åº¦æŒ‡æ ‡è¡¨\nannual_table = self.create_annual_metrics_table(clique)\nresults.append({\n'clique': clique,\n'pnl_figure': pnl_fig,\n'annual_metrics': annual_table\n})\nreturn results\n# Streamlitç•Œé¢\ndef main(use_server_data: bool = False):\n# è®¾ç½®Streamlité…ç½®\nst.set_page_config(\npage_title=\"Maximal Clique Alpha Analyzer\",\nlayout=\"wide\",\ninitial_sidebar_state=\"expanded\"\n)\nst.title(\"ğŸ” æœ€å¤§å›¢Alphaåˆ†æå·¥å…·\")\nst.markdown(\"\"\"\nåŸºäºå›¾è®ºæœ€å¤§å›¢ç®—æ³•çš„Alphaç›¸å…³æ€§åˆ†æç³»ç»Ÿï¼Œç”¨äºè¯†åˆ«å’Œåˆ†æé«˜åº¦ç›¸å…³çš„Alphaç»„åˆã€‚\n**æ ¸å¿ƒåŠŸèƒ½ï¼š**\n- ğŸ¯ åŸºäºç›¸å…³æ€§é˜ˆå€¼æ„å»ºAlphaå…³è”å›¾\n- ğŸ”— ä½¿ç”¨æœ€å¤§å›¢ç®—æ³•è¯†åˆ«ç›¸å…³Alphaé›†åˆ\n- ğŸ“ˆ äº¤äº’å¼PNLè¡¨ç°å¯è§†åŒ–\n- ğŸ“Š æ‚¬åœæ˜¾ç¤ºè¯¦ç»†æ€§èƒ½æŒ‡æ ‡\n- ğŸ“… å¹´åº¦ç»Ÿè®¡æ•°æ®åˆ†æ\n- ğŸ”„ æ”¯æŒæ•°æ®åº“å’ŒAPIåŒæ•°æ®æº\n\"\"\")\n# ä¾§è¾¹æ é…ç½®\nwith st.sidebar:\nst.header(\"âš™ï¸ é…ç½®é€‰é¡¹\")\n# ç›¸å…³æ€§é˜ˆå€¼è®¾ç½®\ncorrelation_threshold = st.slider(\n\"ç›¸å…³æ€§é˜ˆå€¼\",\nmin_value=0.1,\nmax_value=0.9,\nvalue=0.5,\nstep=0.05,\nhelp=\"ç”¨äºæ„å»ºç›¸å…³æ€§å›¾çš„é˜ˆå€¼ï¼Œè¶Šé«˜è¡¨ç¤ºè¦æ±‚æ›´å¼ºçš„ç›¸å…³æ€§\"\n)\n# ç›¸å…³Alphaæ•°é‡é™åˆ¶\nalpha_limit = st.number_input(\n\"ç›¸å…³Alphaæ•°é‡é™åˆ¶\",\nmin_value=5,\nmax_value=1000,\nvalue=1000,\nhelp=\"æŸ¥æ‰¾ç›¸å…³Alphaçš„æœ€å¤§æ•°é‡\"\n)\nst.markdown(\"---\")\nst.markdown(\"\"\"\n**ä½¿ç”¨è¯´æ˜ï¼š**\n1. è¾“å…¥è¦åˆ†æçš„Alpha ID\n2. è°ƒæ•´ç›¸å…³æ€§é˜ˆå€¼å’Œæ•°é‡é™åˆ¶\n3. ç‚¹å‡»å¼€å§‹åˆ†æ\n4. æŸ¥çœ‹PNLæ›²çº¿å’Œå¹´åº¦æŒ‡æ ‡\n5. é¼ æ ‡æ‚¬åœæŸ¥çœ‹è¯¦ç»†æ•°æ®\n\"\"\")\n# ä¸»ç•Œé¢\ncol1, col2 = st.columns([3, 1])\nwith col1:\nalpha_input = st.text_area(\n\"ğŸ¯ è¯·è¾“å…¥Alpha IDåˆ—è¡¨:\",\nvalue=\"\",\nplaceholder=\"ä¾‹å¦‚: ['9Olpx1e','gVa8e60','bz7gNWm','355ZJ5z','OVjQoo1','ELLJbV1','QvQqQLG']\\næˆ–è€…å•ä¸ªID: alpha_001\",\nhelp=\"è¾“å…¥è¦åˆ†æçš„Alphaæ ‡è¯†ç¬¦åˆ—è¡¨ï¼ˆPythonåˆ—è¡¨æ ¼å¼ï¼‰æˆ–å•ä¸ªAlpha ID\",\nheight=100\n)\nwith col2:\nst.write(\"\")\nst.write(\"\")\nanalyze_button = st.button(\n\"ğŸš€ å¼€å§‹åˆ†æ\",\ntype=\"primary\",\nuse_container_width=True\n)\nif analyze_button and alpha_input:\n# è§£æalpha IDè¾“å…¥\ntry:\n# å°è¯•è§£æä¸ºPythonåˆ—è¡¨\nif alpha_input.strip().startswith('[') and alpha_input.strip().endswith(']'):\nimport ast\nalpha_ids = ast.literal_eval(alpha_input.strip())\nif not isinstance(alpha_ids, list):\nst.error(\"âŒ è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥æœ‰æ•ˆçš„Pythonåˆ—è¡¨æ ¼å¼\")\nst.stop()\nelse:\n# å•ä¸ªalpha ID\nalpha_ids = [alpha_input.strip()]\nexcept Exception as e:\nst.error(f\"âŒ è¾“å…¥æ ¼å¼é”™è¯¯: {str(e)}\")\nst.stop()\n# æ›´æ–°é…ç½®\nConfig.CORRELATION_THRESHOLD = correlation_threshold\nanalyzer = MaximalCliqueAnalyzer(use_server_data=use_server_data)\n# æ˜¾ç¤ºåˆ†æè¿›åº¦\nprogress_bar = st.progress(0)\nstatus_text = st.empty()\ntry:\nstatus_text.text(\"æ­£åœ¨å¤„ç†Alpha IDåˆ—è¡¨...\")\nprogress_bar.progress(10)\n# åˆ›å»ºè¿›åº¦å›è°ƒå‡½æ•°\ndef update_progress(message):\nstatus_text.text(message)\n# åˆ¤æ–­æ˜¯å•ä¸ªalphaè¿˜æ˜¯å¤šä¸ªalpha\nif len(alpha_ids) == 1:\n# å•ä¸ªalphaåˆ†æï¼Œè·³è¿‡å›¢æ£€æµ‹ï¼Œç›´æ¥ç»˜åˆ¶PNLå’Œå¹´åº¦æ•°æ®\nstatus_text.text(f\"æ­£åœ¨åˆ†æAlpha {alpha_ids[0]}...\")\nprogress_bar.progress(30)\n# é¢„åŠ è½½å¹´åº¦ç»Ÿè®¡æ•°æ®ï¼Œé¿å…é‡å¤è°ƒç”¨\nstatus_text.text(f\"é¢„åŠ è½½å¹´åº¦ç»Ÿè®¡æ•°æ®...\")\nanalyzer.preload_yearly_stats(alpha_ids, progress_callback=update_progress)\nprogress_bar.progress(50)\ncliques = [alpha_ids] Â # åªåŒ…å«è¾“å…¥çš„alpha\nstatus_text.text(\"æ­£åœ¨ç”Ÿæˆåˆ†æç»“æœ...\")\nprogress_bar.progress(80)\n# ç”Ÿæˆç»“æœ\nresults = []\ntotal_cliques = len(cliques)\nfor i, clique in enumerate(cliques):\ntry:\nstatus_text.text(f\"æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{total_cliques} ä¸ªå›¢ (åŒ…å« {len(clique)} ä¸ªAlpha)...\")\n# æ›´æ–°è¿›åº¦æ¡\nprogress = 70 + (i / total_cliques) * 25 Â # 70-95%çš„è¿›åº¦\nprogress_bar.progress(int(progress))\n# åˆ›å»ºPNLå›¾\nlogger.info(f\"å¼€å§‹ä¸ºå›¢ {i+1} åˆ›å»ºPNLå›¾\")\npnl_fig = analyzer.create_pnl_plot(clique)\n# åˆ›å»ºå¹´åº¦æŒ‡æ ‡è¡¨ï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰\nlogger.info(f\"å¼€å§‹ä¸ºå›¢ {i+1} è·å–å¹´åº¦æŒ‡æ ‡æ•°æ®\")\nannual_table = analyzer.create_annual_metrics_table(clique, progress_callback=update_progress)\nresults.append({\n'clique': clique,\n'pnl_figure': pnl_fig,\n'annual_metrics': annual_table\n})\nlogger.info(f\"å›¢ {i+1} å¤„ç†å®Œæˆ\")\nexcept Exception as e:\nlogger.error(f\"å¤„ç†å›¢ {i+1} æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\nst.warning(f\"âš ï¸ å›¢ {i+1} å¤„ç†å¤±è´¥ï¼Œå·²è·³è¿‡: {str(e)}\")\ncontinue\nelse:\n# å¤šä¸ªalphaï¼Œç›´æ¥å¯¹åˆ—è¡¨è¿›è¡Œå›¢æ£€æµ‹\n# é¦–å…ˆé¢„åŠ è½½æ‰€æœ‰alphaçš„å¹´åº¦ç»Ÿè®¡æ•°æ®\nstatus_text.text(f\"é¢„åŠ è½½ {len(alpha_ids)} ä¸ªAlphaçš„å¹´åº¦ç»Ÿè®¡æ•°æ®...\")\nanalyzer.preload_yearly_stats(alpha_ids, progress_callback=update_progress)\nprogress_bar.progress(20)\nstatus_text.text(f\"æ­£åœ¨å¯¹ {len(alpha_ids)} ä¸ªAlphaè®¡ç®—ç›¸å…³æ€§çŸ©é˜µ...\")\nprogress_bar.progress(30)\n# è®¡ç®—ç›¸å…³æ€§çŸ©é˜µï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰\ncorrelation_matrix = analyzer.correlation_calculator.calculate_correlation_matrix(\nalpha_ids, progress_callback=update_progress\n)\nstatus_text.text(\"æ­£åœ¨æ£€æµ‹å›¢ç»“æ„...\")\nprogress_bar.progress(60)\n# æ£€æµ‹å›¢ç»“æ„\ncliques = analyzer.clique_detector.detect_cliques(correlation_matrix)\nstatus_text.text(\"æ­£åœ¨ç”Ÿæˆåˆ†æç»“æœ...\")\nprogress_bar.progress(70)\n# ç”Ÿæˆç»“æœ\nresults = []\nfor i, clique in enumerate(cliques):\ntry:\nstatus_text.text(f\"æ­£åœ¨å¤„ç†Alpha {clique[0]}...\")\n# åˆ›å»ºPNLå›¾\nlogger.info(f\"å¼€å§‹ä¸ºAlpha {clique[0]} åˆ›å»ºPNLå›¾\")\npnl_fig = analyzer.create_pnl_plot(clique)\n# åˆ›å»ºå¹´åº¦æŒ‡æ ‡è¡¨ï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰\nlogger.info(f\"å¼€å§‹ä¸ºAlpha {clique[0]} è·å–å¹´åº¦æŒ‡æ ‡æ•°æ®\")\nannual_table = analyzer.create_annual_metrics_table(clique, progress_callback=update_progress)\nresults.append({\n'clique': clique,\n'pnl_figure': pnl_fig,\n'annual_metrics': annual_table\n})\nlogger.info(f\"Alpha {clique[0]} å¤„ç†å®Œæˆ\")\nexcept Exception as e:\nlogger.error(f\"å¤„ç†Alpha {clique[0]} æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\nst.error(f\"âŒ Alpha {clique[0]} å¤„ç†å¤±è´¥: {str(e)}\")\ncontinue\nstatus_text.text(\"åˆ†æå®Œæˆï¼\")\nprogress_bar.progress(100)\n# æ¸…é™¤è¿›åº¦æ˜¾ç¤º\nprogress_bar.empty()\nstatus_text.empty()\nif not results:\nst.warning(\"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„åˆ†æç»“æœ\")\nreturn\nelse:\nst.success(f\"âœ… åˆ†æå®Œæˆï¼å‘ç° {len(results)} ä¸ªå›¢\")\n# æ˜¾ç¤ºç»“æœ\nfor i, result in enumerate(results):\nwith st.expander(f\"ğŸ“Š å›¢ {i+1}: {', '.join(result['clique'])} ({len(result['clique'])} ä¸ªAlpha)\", expanded=True):\n# å›¢ä¿¡æ¯\ncol1, col2, col3 = st.columns(3)\nwith col1:\nst.metric(\"å›¢å¤§å°\", len(result['clique']))\nwith col2:\nst.metric(\"Alphaæ•°é‡\", len([a for a in result['clique'] if a]))\nwith col3:\nannual_years = len(result['annual_metrics']['Year'].unique()) if not result['annual_metrics'].empty else 0\nst.metric(\"è¦†ç›–å¹´ä»½\", annual_years)\n# PNLå›¾è¡¨\nst.subheader(\"ğŸ“ˆ PNLæ›²çº¿å›¾\")\nst.info(\"ğŸ’¡ é¼ æ ‡æ‚¬åœåˆ°PNLæ›²çº¿ä¸Šå¯æŸ¥çœ‹è¯¥Alphaçš„è¯¦ç»†å¹´åº¦ç»Ÿè®¡æ•°æ®\")\n# æ˜¾ç¤ºå›¾è¡¨\nst.plotly_chart(\nresult['pnl_figure'],\nuse_container_width=True,\nkey=f\"pnl_chart_{i}\"\n)\n# å¹´åº¦æŒ‡æ ‡è¡¨\nif not result['annual_metrics'].empty:\nst.subheader(\"ğŸ“… å¹´åº¦æŒ‡æ ‡ç»Ÿè®¡\")\n# æ·»åŠ ç­›é€‰é€‰é¡¹\ncol1, col2 = st.columns(2)\nwith col1:\nselected_years = st.multiselect(\n\"é€‰æ‹©å¹´ä»½\",\noptions=sorted(result['annual_metrics']['Year'].unique(), reverse=True),\ndefault=sorted(result['annual_metrics']['Year'].unique(), reverse=True),\nkey=f\"years_{i}\"\n)\nwith col2:\nselected_alphas = st.multiselect(\n\"é€‰æ‹©Alpha\",\noptions=result['annual_metrics']['Alpha'].unique(),\ndefault=result['annual_metrics']['Alpha'].unique(),\nkey=f\"alphas_{i}\"\n)\n# ç­›é€‰æ•°æ®\nfiltered_data = result['annual_metrics'][\n(result['annual_metrics']['Year'].isin(selected_years)) &\n(result['annual_metrics']['Alpha'].isin(selected_alphas))\n]\nif not filtered_data.empty:\nst.dataframe(\nfiltered_data,\nuse_container_width=True,\nhide_index=True\n)\n# ä¸‹è½½æŒ‰é’®\ncsv = filtered_data.to_csv(index=False)\nst.download_button(\nlabel=\"ğŸ“¥ ä¸‹è½½å¹´åº¦æŒ‡æ ‡æ•°æ®\",\ndata=csv,\nfile_name=f\"annual_metrics_clique_{i+1}.csv\",\nmime=\"text/csv\",\nkey=f\"download_{i}\"\n)\nelse:\nst.info(\"ğŸ“ æ ¹æ®ç­›é€‰æ¡ä»¶æœªæ‰¾åˆ°æ•°æ®\")\nelse:\nst.info(\"ğŸ“ æš‚æ— å¹´åº¦æŒ‡æ ‡æ•°æ®\")\nst.markdown(\"---\")\nexcept Exception as e:\nst.error(f\"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}\")\nlogger.error(f\"åˆ†æé”™è¯¯: {e}\")\n# æ˜¾ç¤ºé”™è¯¯è¯¦æƒ…ï¼ˆä»…åœ¨è°ƒè¯•æ¨¡å¼ä¸‹ï¼‰\nif st.checkbox(\"æ˜¾ç¤ºé”™è¯¯è¯¦æƒ…\", key=\"show_error_details\"):\nst.code(str(e))\nelif analyze_button and not alpha_input:\nst.warning(\"âš ï¸ è¯·è¾“å…¥Alpha ID\")\n# é¡µè„šä¿¡æ¯\nst.markdown(\"---\")\nst.markdown(\"\"\"\n<div style='text-align: center; color: gray;'>\n<small>Maximal Clique Alpha Analyzer v1.0 | åŸºäºå›¾è®ºæœ€å¤§å›¢ç®—æ³•çš„Alphaç›¸å…³æ€§åˆ†æç³»ç»Ÿ</small>\n</div>\n\"\"\", unsafe_allow_html=True)\nif __name__ == \"__main__\":\n# é…ç½®æ•°æ®æºï¼šTrue=ä»æœåŠ¡å™¨è·å–ï¼ŒFalse=ä»æ•°æ®åº“è·å–\nUSE_SERVER_DATA = False Â # ä¿®æ”¹æ­¤å€¼æ¥åˆ‡æ¢æ•°æ®æº\nmain(use_server_data=USE_SERVER_DATA)",
    "å¤ªå¼ºäº†ï¼Œç­‰å¾…è§‚æ‘©ä»£ç \n#========= WORLDQUANT BRAIN CONSULTANT ========== #\n# Alphaâˆ Engine Status: ONLINE [â™¦â™¦â™¦â™¦â™¦â™¦â™¦â™¦â™¦â™¦] 100%\n# sys.setrecursionlimit(Î±âˆ)\n# PnL = âˆ‘(Robustness * Creativity)\n#æ— é™æ¢ç´¢ã€é²æ£’æ€§ä¼˜å…ˆï¼Œåˆ›æ–°æ€§å¢å€¼\n#=================å¥‹è¿›çš„å°å¾=======================#",
    "æ¥å§ï¼Œä½œè€…å¿«ç‚¹åˆ†äº«ä»£ç ï¼Œçœ‹ç€å¾ˆæœŸå¾…ï¼Œç‚¹èµ",
    "å¤§ä½¬æ±‚ä»£ç ",
    "æ„Ÿè°¢å¤§ä½¬çš„åˆ†äº«ï¼Œè¿«ä¸åŠå¾…çš„æƒ³è¯•ä¸‹ï¼Œè¯·é—®ä»£ç è¿˜æ²¡å‘å¸ƒå—ï¼Ÿ",
    "å¤§ä½¬ä»£ç è¯„è®ºåŒºæ²¡å‘",
    "å¤§ä½¬ï¼Œè¯·é—®æ•°æ®åº“æºè¿™è¾¹æ€ä¹ˆå¤„ç†å‘¢ï¼Ÿå¯ä»¥æä¾›ä¸€ä¸‹å»ºåº“å»ºè¡¨çš„sqlè„šæœ¬å—ï¼Ÿ",
    "@\nJA92987\næ•°æ®æºè¿™è¾¹å»ºè®®å¹³æ—¶å®šæœŸä¸‹è½½PNL, ä¸»è¦æ˜¯è·å–PNLè¿™è¾¹ä¼šç¨å¾®æ…¢ä¸€ç‚¹,PNLçš„æ•°æ®é‡æ¯”è¾ƒå¤§,æœåŠ¡å™¨é™åˆ¶ä¹Ÿå¤šä¸€ç‚¹.è·å–year_statusçš„æ¯å¹´æ•°æ®, ä¸€èˆ¬æ˜¯1-5ç§’ä¸‹è½½ä¸€ä¸ª.æ‰€ä»¥æœ€åyear_statusçš„æ•°æ®æˆ‘æ²¡æœ‰ä¿å­˜åˆ°æ•°æ®åº“.æˆ‘è¿™è¾¹æµ‹è¯•çš„æ˜¯,åœ¨PNLå’Œyear_statuséƒ½ä»æœåŠ¡å™¨ä¸‹è½½çš„æƒ…å†µä¸‹, 100å¤šä¸ªalphaè¿›è¡Œè®¡ç®—ä¸ç»˜å›¾, åŸºæœ¬ä¸Š95%ä»¥ä¸Šçš„æ—¶é—´èŠ±åœ¨è·å–æ•°æ®ä¸Š,æ˜¯éœ€è¦å‡ åˆ†é’Ÿåˆ°åå‡ åˆ†é’Ÿçš„æ—¶é—´ä¸ç­‰çš„.\nPNLæ•°æ®åº“å»ºè¡¨è¯­å¥å¦‚ä¸‹:\nCREATE TABLE alpha_pnl (\nalpha_id VARCHAR(255) PRIMARY KEY,\npnl_data JSON,\nregion VARCHAR(50),\ncreate_time DATETIME DEFAULT CURRENT_TIMESTAMP,\nupdate_time DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n);",
    "å¤ªå‰å®³äº†",
    "=========================================MY27687====================================\næ„Ÿè°¢å¤§ä½¬åˆ†äº«ï¼Œå¾ˆä¸é”™çš„å·¥å…· ï¼Œä¹‹å‰è‡ªå·±ç®€å•å®ç°äº†ä¸€ä¸ªæœ€å¤§å›¢ç®—æ³•ï¼Œå¤§ä½¬çš„å·¥å…·æ›´åŠ é½å…¨ï¼Œæ›´åŠ ä¼˜ç§€ï¼Œä¸ºå¤§ä½¬ç‚¹èµï¼Œç¥å¤§ä½¬vf é«˜é«˜ baseå¤šå¤šï¼ï¼ï¼ï¼\n============================æ¯å¤©æäº¤ä¸€ä¸ªå¥½çš„alpha======================================"
  ]
}