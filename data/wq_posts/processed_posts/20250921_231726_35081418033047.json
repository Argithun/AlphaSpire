{
  "title": "一文解锁手机自动化回测, 从此随时随地想跑就跑。---基于QPython(安卓)--附代码",
  "description": "大部分人都会选择云服务器来进行批量回测,如果觉得云服务器贵or前期懒得配置云服务器，更或者假期出去游玩没办法登录云服务器来操作，或许手机端批量自动化回测是一个不错的选择....",
  "post_body": "大部分人都会选择云服务器来进行批量回测,如果觉得云服务器贵or前期懒得配置云服务器，更或者假期出去游玩没办法登录云服务器来操作，或许手机端批量自动化回测是一个不错的选择.\n服务器可看作是一个高配置的电脑主机,而手机则可以是低配的电脑主机,对于回测使用的内存/硬盘/网络带宽完全足够.我选择的时QPYTHON，应用无广告，可开多个py文件进程。\n启动流程: 手机应用商店下载Qpython ,点击进入APP -> 中间选择扩展 -> 选择AIPY ->往下滑动点击Pandas包安装.也可以在扩展界面选择PIP客户端进入手动下载一些pip库,但是支持的没那么多.（基础配置就弄好啦。）->.py文件，选择以QPYTHON打开，点击下面的运行符号。等待即可\n如何查看手机文件的主目录: 点击文件夹最上面的默认分区.我的是/storage/emulated/0/,当文件存储在其他路径时,可能有权限问题,需要让QPython拥有更多权限,一般在手机设置中.由于json写入其他目录不对问题，所以我把状态文件Qpython的文件夹中\n最后附上自动化回测代码，支持自动跑一二三N阶，支持断点续测，可扩展性强，但是函数比较少，可以自己多多添加。应该是支持顾问阶段和用户阶段的，可能需要自己稍微调一下确保代码没问题（有个缺点就是无法拉取超过10000的回测量，之前的手机摔坏了，所以没用这套代码了，下个版本还没调好，有机会再分享）。\n单槽回测/用户阶段记得更改：if len(arr) == 10为if len(arr) == 1。max_post为最大槽位数量，用户阶段需要改为3。\n字段分词以两个空格来分词，所以传入的表达式不要使用两个空格连在一起。\n主函数如下：第一个为文件名称, 以.csv结尾。列名为code\nflow(\"news18-EUR-1-TOP2500-m2-df\", region=\"EUR\", universe=\"TOP2500\",delay=1, neu=\"FAST\")\n其他组合函数命名：ff：代表不做组合，ts_first->对应一阶，group_second->对应二阶，when_third->对应三阶。 完全支持各种顺序组合，更改funcs = [ff, ts_first, group_second, when_third,][:]的顺序或者删减即可。\n代码如下：\nimport requests, logging, os, sys, re\nfrom logging.handlers import RotatingFileHandler\nimport json\nfrom requests.auth import HTTPBasicAuth\nimport pandas as pd\nfrom time import sleep\nimport datetime\nimport pytz\nimport logging\nclass quant:\ndef __init__(self, data_name: str) -> None:\nself.arr = []\nself.count = 0\nself.path = \"/storage/emulated/0/qua/\" if sys.platform == \"linux\"\\\nelse \"C:\n\\\nProject\n\\\nqua\n\\\n\"\nself.path += data_name.split(\"-\")[0]+\"/\"\nos.makedirs(self.path, exist_ok=True)\nself.data_name =self.path+data_name\nself.log_name = self.path+data_name+'-log.txt'\nself.case_name = self.data_name+\"-case.json\"\nself.case_df = self.data_name+\"-df.json\"\nself.result_df = self.data_name+\"-result.csv\"\nself.sess = requests.Session()\nself.save_file: StopIteration = None\ndef login(self,):\n# 用户名和密码文件brain.txt格式: [\"username\", \"password\"]\nwith open(self.path + '../brain.txt') as f:\nself.username, self.password = json.load(f)\nself.sess.auth = HTTPBasicAuth(self.username, self.password)\nresponse = self.sess.post('\nhttps://api.worldquantbrain.com/authentication\n')\nprint(response)\ndef save_result(self, alpha_num, s_time) -> pd.DataFrame:\n# 2025-07-04T22:52:30\narr = []\nprint(alpha_num, s_time)\nfor i in range(0, alpha_num+100, 100):\nprint(i)\nurl = \"\nhttps://api.worldquantbrain.com/users/self/alphas?limit=100&offset=%d\"%(i)\n\\\n+ \"&status=UNSUBMITTED%1FIS_FAIL&dateCreated%3E=\" + s_time  \\\n+ \"-04:00\"\nprint(url)\nresponse = self.sess.get(url)\nprint(response)\nalpha_list = response.json()[\"results\"]\nprint\nif len(alpha_list)==0:\nreturn pd.DataFrame(arr)\nfor alphas_p in alpha_list:\nresult = dict()\nresult[\"id\"] = alphas_p[\"id\"]\nresult[\"code\"] = alphas_p[\"regular\"][\"code\"]\nresult[\"result\"] = \"FAIL\" if  [i.get(\"name\") for i in alphas_p[\"is\"][\"checks\"] if i.get(\"result\") == \"FAIL\"] else \"PASS\"\nLOW_SUB_UNIVERSE_SHARPE = [i for i in alphas_p[\"is\"][\"checks\"] if i[\"name\"] == \"LOW_SUB_UNIVERSE_SHARPE\"][0]\nresult[\"sub\"]=LOW_SUB_UNIVERSE_SHARPE.get(\"value\", -2)-LOW_SUB_UNIVERSE_SHARPE.get(\"limit\", 2)\nCONCENTRATED_WEIGHT: dict = [i for i in alphas_p[\"is\"][\"checks\"] if i[\"name\"] == \"CONCENTRATED_WEIGHT\"][0]\nresult[\"weight\"] = CONCENTRATED_WEIGHT.get(\"limit\", 0) - CONCENTRATED_WEIGHT.get(\"value\", 0)\naplha_is:dict = alphas_p[\"is\"]\ndelete =  [\"startDate\", \"checks\", \"bookSize\"]\nfor ite in delete:\ndel aplha_is[ite]\n# del\nresult.update(aplha_is)\nresult[\"settings\"] = alphas_p[\"settings\"]\narr.append(result)\nreturn pd.DataFrame(arr)\ndef submit_simulations(self, index,  alpha_list, max_post=3):\nif self.count % 80 == 0:\nself.sess.close()\nself.login()\nself.count +=1\nif  alpha_list:\nprint(alpha_list[0])\nif len(alpha_list)==0:\nalpha_list =alpha_list[0]\nfor _ in range(20):\ntry:\nsim1 = None\nsim1 = self.sess.post('\nhttps://api.worldquantbrain.com/simulations\n', json=alpha_list,)\nlocation = sim1.headers['Location'].split(\"/\")[-1]\nbreak\nexcept  Exception as e:\nprint(e, index, sim1)\nsleep(10)\nif _ == 19:\ncfg.log(f\"post ERROR: {index}\")\nreturn\nself.arr.append((index, location))\nself.save_status(index)\ntry:\nprint(self.arr)\nwhile len(self.arr) >= max_post:\nfor index, ip in self.arr:\nurl = \"\nhttps://api.worldquantbrain.com/simulations/\n\" + ip\nsim_progress_resp = self.sess.get(url)\nretry_after_sec = sim_progress_resp.headers.get(\"Retry-After\", 0)\nif retry_after_sec == 0:  # simulation done!模拟完成!\nif (index, ip) in self.arr:\nself.arr.remove((index, ip)) #删除对应的值\nchildren = sim_progress_resp.json().get(\"children\")  # 获取alpha id\nstatus1 = sim_progress_resp.json().get(\"status\")\nif status1 == \"ERROR\":\ncfg.log(f\"status ERROR: {index} {ip}\" )\nprint(index, status1, children)\nsleep(0.1)\nelse:\nsleep(0.25)\nexcept Exception as e:\ncfg.log(e)\nprint(e)\ncfg.log(f\"no location [{index}], sleep for 10 seconds and try next alpha.没有位置，睡10秒然后尝试下一个字母。”\")\nsleep(10)\ndef save_status(self, index):\ncfg.status[\"case\"] = index\ncfg.log(str(cfg.status))\nwith open(self.case_name, \"w\") as f:\nf.write(json.dumps(cfg.status))\ndef sims(self, df: pd.DataFrame, start: int=0, max_post: int=8):\nprint(len(df.index))\narr= []\nstart = cfg.status.get(\"case\",-1)\ni = 0\nstart+=1\nfor i, index in enumerate(df.index[start:], start=start):\nalpha_s =  {\n\"type\": \"REGULAR\",\n\"settings\": df.loc[index][\"settings\"],\n\"regular\": df.loc[index][\"code\"] }\narr.append(alpha_s)\n# 每个槽位的数量,用户阶段需要设置为1\nif len(arr) == 10:\nprint(arr[0])\ncfg.log(f\"index is: {index}\")\ncfg.qua.submit_simulations(index, arr, max_post=max_post)\narr = []\ncfg.qua.submit_simulations(index, arr, max_post=max_post)\ncfg.qua.submit_simulations(i, [], max_post=1)\ndef deal_data(self, df: pd.DataFrame,sharpe: float=0.9,n: int=1, save_file:str=\"\"):\n# 变更sharpe和fitness，按照原始表达式分组。按照fitness+sharpe排序取前n。\nfor a in df.index:\nif df.loc[a][\"sharpe\"] <0:\ndf.iat[a][\"code\"] = \"-\"+df.loc[a][\"code\"]\ndf = df[df[\"longCount\"]+df[\"shortCount\"]>4]\ndf[\"total\"] = abs(df[\"fitness\"] + df[\"sharpe\"])\ndf[\"exp\"] = df[\"code\"].apply(lambda x: x.split(\"  \")[1] if \"  \" in x else x)\ndf[\"op\"] = df[\"code\"].apply(lambda x: x.split(\"(\")[1] if \"(\" in x else x)\ndf.sort_values(by=\"total\", inplace=True,ascending=False)\ndf.to_csv(self.save_file +\".csv\")\ndf = df[(abs(df[\"sharpe\"])>=sharpe) | (abs(df[\"fitness\"]) >= 1)]\ndf = df.groupby([\"exp\", \"op\"]).head(n)\ndf = df[[\"code\", \"settings\"]]\ndf.to_json(self.case_df)\ncfg.status = {}\nreturn df\n#原始序列\ndef ff(df :pd.DataFrame):\nreturn df\n# ts序列\ndef ts_first(df: pd.DataFrame, days:list = [5, 22, 66, 252]) ->pd.DataFrame:\nts_ops = [\"ts_rank\", \"ts_zscore\", \"ts_delta\",  \"ts_sum\", \"ts_delay\",\n\"ts_std_dev\", \"ts_mean\",  \"ts_arg_min\", \"ts_arg_max\",\"ts_scale\", \"ts_quantile\"]\narr = []\nfor i in df.index:\nfor op in ts_ops:\nfor day in days:\narr.append({\"code\": f'{op}({df.loc[i][\"code\"]}, {day})',\n\"settings\": df.loc[i][\"settings\"]})\nreturn pd.DataFrame(arr)\ndef group_second(df: pd.DataFrame,):\ngroups=[\"group_rank\", \"group_zscore\", \"group_scale\", \"group_neutralize\"]\ngps = [\"industry\", \"subindustry\"]\nif df.loc[df.index[0], \"settings\"][\"region\"] in [\"GLB\", \"EUR\",\"ASI\"]:\ngps += [\"group_cartesian_product(country, industry)\",\n\"group_cartesian_product(country, subindustry)\"]\narr = []\nfor i in df.index:\nfor op in groups:\nfor gp in gps:\narr.append({\"code\": f'{op}({df.loc[i][\"code\"]}, {gp})',\n\"settings\": df.loc[i][\"settings\"]})\nreturn pd.DataFrame(arr)\ndef when_third(df:pd.DataFrame,) ->pd.DataFrame:\nopen_events = ['group_rank(ts_std_dev(returns,60),sector)>0.7',\n'ts_mean(volume,5)>ts_mean(volume,60)',\n'ts_zscore(returns,60)>2',\n'ts_std_dev(returns, 5)>ts_std_dev(returns, 20)',\n'returns<0.09',\n'ts_corr(close,volume,5)>0',\n'ts_corr(close,volume,5)<0',\n'returns>-0.09',\n\"abs(returns)<0.10\"]\n# open_events=[\"rank(rp_css_business)>0.8\",\"ts_rank(rp_css_business,22)>0.8\",\n# \"rank(vec_avg(nws3_scores_posnormscr))>0.8\",\n# \"ts_rank(vec_avg(nws3_scores_posnormscr),22)>0.8\",]\narr=  []\nfor i in df.index:\nfor op in open_events:\narr.append({\"code\":f'{op}? {df.loc[i][\"code\"]}:-1',\n\"settings\": df.loc[i][\"settings\"]})\nreturn pd.DataFrame(arr)\ndef cases(func, df, sharpe=1.4, max_post=7, s_time=\"\"):\ncfg.qua.save_file = cfg.qua.data_name + f\"-{func.__name__}.csv\"\narrs = func(df)\nif not cfg.status.get(\"time\"):\nnow = datetime.datetime.now(pytz.timezone('America/New_York'))\ns_time =  now.isoformat().split(\".\")[0]\ncfg.status[\"time\"] = s_time\ncfg.status[\"func\"] = func.__name__\nprint(cfg.status)\ncfg.qua.sims(arrs, start=0,max_post=max_post)\ndf: pd.DataFrame = cfg.qua.save_result(len(arrs), cfg.status.get(\"time\"))\ndf = cfg.qua.deal_data(df, sharpe=sharpe,)\nreturn df\ndef log(name,):\nlogger = logging.getLogger(name)\nformater = logging.Formatter( \"[%(asctime)s] %(message)s\", '%Y-%m-%d %H:%M:%S' )\nlogger.setLevel(logging.DEBUG)\n# if not show:\nfile_log = logging.FileHandler(name, )\nfile_log.setFormatter(formater)\nlogger.addHandler(file_log)\nch = logging.StreamHandler()\nch.setFormatter(formater)\nlogger.addHandler(ch)\nreturn logger\nclass cfg:\nqua: quant = None\nstatus = {}\nlog = None\ndef flow(data_name, settings: dict= {}, region= \"USA\",universe=\"TOP3000\", delay=1,neu=\"SUBINDUSTRY\" ):\ncfg.qua = quant(data_name)\ncfg.log = log(cfg.qua.log_name).info\n# 加载原始文件\ndf = pd.read_csv(cfg.qua.data_name+\".csv\")\n# 状态\nif os.path.exists(cfg.qua.case_name):\nwith open(cfg.qua.case_name, \"r\") as f:\ncfg.status = json.load(f)\n# 存在中间过程文件\nif os.path.exists(cfg.qua.case_df):\ndf = pd.read_json(cfg.qua.case_df)\nif \"settings\" not in df.columns:\ndf[\"code\"] = \"  \" + df[\"code\"] + \"  \"\nsettings = {\n\"instrumentType\": \"EQUITY\",\n\"region\": region,\n\"universe\": universe,\n\"delay\": delay,\n\"decay\": 0,\n\"neutralization\": neu,\n\"truncation\": 0.08,\n\"pasteurization\": \"ON\",\n\"unitHandling\": \"VERIFY\",\n\"nanHandling\": \"ON\",\n\"language\": \"FASTEXPR\",\n\"visualization\": False }\ndf = df.to_dict()\ndf[\"settings\"] ={i:settings for i in list(df[\"code\"].keys())}\ndf = pd.DataFrame(df)\nprint(\"status: \", cfg.status)\ncfg.qua.login()\nfuncs = [ff, ts_first, group_second, when_third,][:] #\nfor i, func  in enumerate(funcs):\nif cfg.status.get(\"func\") and cfg.status.get(\"func\")!= func.__name__:\ncontinue\n# sharpe：用于筛选进入下一阶段的最低要求\ndf = cases(func, df, sharpe=0.7+i*0.2, max_post=8)\ndf: pd.DataFrame = None\nfor i in os.listdir(cfg.qua.path):\nif data_name +\"-df\"  in i:\ncontinue\nif data_name +\".csv\" in i:\ndft = pd.read_csv(cfg.qua.path+i)\ndf = pd.concat([df, dft])\ndf.to_csv(cfg.qua.data_name+\"-all.csv\")\nflow(\"news18-EUR-1-TOP2500-m2-df\", region=\"EUR\", universe=\"TOP2500\",delay=1, neu=\"SUBINDUSTRY\")",
  "post_comments": []
}