{
  "title": "使用图论最大团算法深入分析Alpha相关性，进行Alpha去重及团内Alpha可视化挑选的工具",
  "description": "大家好， 根据华子哥的自相关代码以及毛老师提出的最大团思路, 我做了一个基于图论中 “最大团（Maximal Clique）” 算法的可视化分析工具. 算法思路就不赘述了, 简单来说就是针对0.5以下自相关的alpha,将他们分成团,同一个团内的alpha,...",
  "post_body": "大家好，\n根据华子哥的自相关代码以及毛老师提出的最大团思路, 我做了一个基于图论中 “最大团（Maximal Clique）” 算法的可视化分析工具.\n算法思路就不赘述了, 简单来说就是针对0.5以下自相关的alpha,将他们分成团,同一个团内的alpha, 提交了其中一个以后, 其他的alpha无法通过PPA的自相关检测, 使用可视化工具,方便找出这批alpha里面曲线走势与每年指标综合更优质的选择.当然也可以做成相关性的剪枝, 只对团内部分因子进行进一步的其他处理, 减少无效的同质化操作.\n功能和亮点：\n1. 1.\n利用 networkx 库进行图的构建和最大团查找，能更系统地揭示Alpha间的复杂关联结构。\n2. 2.\n交互式可视化仪表盘 ：基于 Streamlit 和 Plotly 构建了Web界面。你可以：\n- 查看识别出的所有Alpha团。\n- 点击任何一个团，立即看到团内所有Alpha的累积PNL曲线对比图。\n- 获取团内Alpha的年度表现统计（Sharpe、Returns、Drawdown等），快速评估其整体质量。\n- 查看每个Alpha的详细参数（region, neutralization, turnover等）。\n3. 3.\n灵活的数据源支持 ：\n- 支持直接从 WorldQuant Brain API 获取Alpha的PNL和详细数据。\n- 也支持连接本地 MySQL数据库 ，方便进行离线分析和回测。\n4. 4.\n稳健的API交互 ：内置了完整的WQ Brain API会话管理、请求重试和超时处理机制，确保在大量拉取数据时的稳定性和可靠性。\n5. 5.\n一站式分析 ：从数据获取、相关性计算、最大团检测到结果可视化和指标分析，整个流程被整合在一个工具中，实现了端到端的Alpha关联性分析。\n这个工具在实践中可以帮助我们：\n- Alpha去重与筛选 ：当一个“团”被识别出来后，我们可以认为这个团内的Alpha捕捉的是相似的信号。我们可以从中选择一两个最具代表性（例如Sharpe最高、最稳定）的Alpha，而将其他冗余的Alpha存档，从而大大简化我们的Alpha库。\n- 提升组合多样性 ：在构建投资组合时，我们可以有意识地从不同的“团”中各挑选一个代表，这样可以确保我们的最终组合是由一系列低相关的策略构成的，从而提升多样性。\n- 风险暴露分析 ：分析某个“团”内Alpha的共同特征（例如，它们是否都交易某一类股票，或者都使用了某个特定的数据字段），可以帮助我们理解组合潜在的风险暴露。\n- 激发新思路 ：为什么这个“团”里的Alpha表现如此相似？它们背后共同的逻辑是什么？对这些问题的探索，有时能帮助我们提炼出更稳定、更强大的新Alpha因子。 总结\n总而言之， maximal_clique_analyzer.py 是一个将图论算法与量化投研实践相结合的尝试。它提供了一个新颖的视角来审视我们手中的Alpha宝库，帮助我们去粗取精，构建更稳健、更多样化的投资组合。\n这个工具的设计灵感来源于实际工作中的痛点，希望能对大家有所帮助。欢迎各位下载试用、交流想法，也期待大家的宝贵意见和建议！\n实际效果图:\n代码比较长放评论区吧",
  "post_comments": [
    "具体代码如下:\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nMaximal Clique Alpha Analyzer\n最大团Alpha分析工具\n本工具基于图论中的最大团算法，用于分析Alpha之间的相关性结构。\n主要功能包括：\n1. 基于相关性阈值构建Alpha关联图\n2. 使用最大团算法识别高度相关的Alpha组合\n3. 可视化团内Alpha的PNL表现曲线\n4. 提供交互式数据展示和年度指标统计\n5. 支持数据库和服务器API两种数据源\n技术实现：\n- 使用NetworkX库进行图构建和最大团检测\n- 基于Plotly实现交互式数据可视化\n- 通过Streamlit构建Web界面\n- 支持WorldQuant Brain API数据获取\n\"\"\"\nimport pandas as pd\nimport numpy as np\nimport json\nimport logging\nimport pymysql\nfrom typing import List, Dict, Tuple, Optional\nimport networkx as nx\nimport datetime\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport streamlit as st\nimport requests\nimport time\nimport os\nfrom time import sleep\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n# 全局session管理器\nclass SessionManager:\n_instance = None\n_session = None\n_last_login_time = None\n_session_timeout = 3600  # 1小时session超时\ndef __new__(cls):\nif cls._instance is None:\ncls._instance = super(SessionManager, cls).__new__(cls)\nreturn cls._instance\ndef _login(self):\n\"\"\"内部登录方法，减少日志输出\"\"\"\nusername = os.environ.get('wq_username')\npassword = os.environ.get('wq_password')\nretry_strategy = Retry(\ntotal=3,\nbackoff_factor=1,\nstatus_forcelist=[500, 502, 503, 504]\n)\nadapter = HTTPAdapter(max_retries=retry_strategy)\ns = requests.Session()\ns.mount(\"https://\", adapter)\ns.mount(\"http://\", adapter)\ns.auth = (username, password)\nmax_try = 3\nretry = 0\nwhile retry < max_try:\ntry:\nresponse = s.post('\nhttps://api.worldquantbrain.com/authentication\n')\nif response.status_code in [200, 201]:\n# 只在首次登录或重新登录时打印简化信息\nauth_data = response.json()\nuser_id = auth_data.get('user', {}).get('id', '未知')\nlogger.info(f\"Session认证成功，用户ID: {user_id}\")\nself._last_login_time = time.time()\nreturn s\nexcept Exception as e:\nlogger.warning(f\"登录尝试失败: {e}\")\nsleep(2)\nretry += 1\nlogger.error(\"登录失败，达到最大重试次数\")\nreturn None\ndef get_session(self):\n\"\"\"获取session，自动处理复用和重新登录\"\"\"\ncurrent_time = time.time()\n# 检查是否需要重新登录\nif (self._session is None or\nself._last_login_time is None or\ncurrent_time - self._last_login_time > self._session_timeout):\nlogger.info(\"创建新的session或session已过期，重新登录...\")\nself._session = self._login()\nreturn self._session\n# 全局session管理器实例\nsession_manager = SessionManager()\ndef get_session():\n\"\"\"获取WorldQuant Brain session\"\"\"\ns = session_manager.get_session()\nif s:\nreturn s\n# 如果登录失败，返回一个基本的requests session\nsession = requests.Session()\nsession.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'})\nreturn session\n# 配置日志\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\nclass Config:\n\"\"\"配置类\"\"\"\nDB_CONFIG = {\n'host': 'localhost',\n'user': 'root',\n'password': '123456',\n'database': 'worldquant',\n'port': 3306,\n'charset': 'utf8mb4'\n}\nCORRELATION_THRESHOLD = 0.5  # 相关性阈值\nclass DatabaseManager:\n\"\"\"数据库管理类\"\"\"\ndef __init__(self):\nself.config = Config.DB_CONFIG\ndef get_connection(self):\n\"\"\"获取数据库连接\"\"\"\ntry:\nconnection = pymysql.connect(**self.config, cursorclass=pymysql.cursors.DictCursor)\nreturn connection\nexcept Exception as e:\nlogger.error(f\"数据库连接失败: {e}\")\nreturn None\ndef get_pnl_data(self, alpha_id: str) -> Optional[dict]:\n\"\"\"从数据库获取PNL数据\"\"\"\nconn = self.get_connection()\nif not conn:\nreturn None\ntry:\nwith conn.cursor() as cursor:\nsql = \"SELECT pnl_data FROM alpha_pnl WHERE alpha_id = %s\"\ncursor.execute(sql, (alpha_id,))\nresult = cursor.fetchone()\nreturn json.loads(result['pnl_data']) if result else None\nexcept Exception as e:\nlogger.error(f\"获取PNL数据失败 {alpha_id}: {e}\")\nreturn None\nfinally:\nconn.close()\ndef get_alpha_data(self, alpha_id: str) -> Optional[dict]:\n\"\"\"从alpha_data表获取alpha数据\"\"\"\nconn = self.get_connection()\nif not conn:\nreturn None\ntry:\nwith conn.cursor() as cursor:\nsql_data = \"\"\"SELECT * FROM alpha_data WHERE id = %s\"\"\"\ncursor.execute(sql_data, (alpha_id,))\nalpha_data = cursor.fetchone()\nif not alpha_data:\nreturn None\nresult = dict(alpha_data)\n# 处理datetime对象\nfor key, value in result.items():\nif isinstance(value, datetime.datetime):\nresult[key] = value.strftime('%Y-%m-%d %H:%M:%S')\nelif isinstance(value, datetime.date):\nresult[key] = value.strftime('%Y-%m-%d')\n# 设置默认字段\nif 'name' not in result:\nresult['name'] = alpha_id\nif 'neutralization' not in result:\nresult['neutralization'] = 'CROWDING'\nreturn result\nexcept Exception as e:\nlogger.error(f\"获取alpha数据失败 {alpha_id}: {e}\")\nreturn None\nfinally:\nconn.close()\ndef get_alpha_data_from_server(self, alpha_id: str, session, timeout_seconds: int = 3600) -> Optional[dict]:\n\"\"\"从服务器获取alpha数据，支持1小时超时限制\nArgs:\nalpha_id (str): Alpha ID\nsession: 会话对象\ntimeout_seconds (int): 总超时时间（秒），默认3600秒（1小时）\nReturns:\nOptional[dict]: Alpha数据，超时或失败返回None\n\"\"\"\nstart_time = time.time()\ntry:\n# 检查超时\nif time.time() - start_time > timeout_seconds:\nlogger.warning(f\"Alpha {alpha_id} 获取数据超时，跳过\")\nreturn None\n# 使用特定alpha的API端点\nurl = f\"\nhttps://api.worldquantbrain.com/alphas/{alpha_id}\n\"\n# 计算剩余超时时间\nremaining_timeout = timeout_seconds - (time.time() - start_time)\nif remaining_timeout <= 0:\nlogger.warning(f\"Alpha {alpha_id} 剩余时间不足，跳过\")\nreturn None\nresponse = session.get(url, timeout=min(30, remaining_timeout))\nif response.status_code == 404:\nlogger.warning(f\"Alpha {alpha_id} 不存在，跳过\")\nreturn None\nelif response.status_code != 200:\nlogger.error(f\"获取alpha {alpha_id} 失败: {response.status_code}\")\nreturn None\nresult_data = response.json()\nalpha_info = result_data.get('is', {})\nsettings = result_data.get('settings', {})\n# 提取需要的字段\nresult = {\n'id': alpha_id,\n'sharpe': alpha_info.get('sharpe'),\n'returns': alpha_info.get('returns'),\n'drawdown': alpha_info.get('drawdown'),\n'turnover': alpha_info.get('turnover'),\n'fitness': alpha_info.get('fitness'),\n'margin': alpha_info.get('margin'),\n'long_count': alpha_info.get('longCount'),\n'short_count': alpha_info.get('shortCount'),\n'region': settings.get('region'),\n'neutralization': settings.get('neutralization', 'CROWDING'),\n'name': result_data.get('name', alpha_id)\n}\nlogger.info(f\"成功从服务器获取alpha数据: {alpha_id}, region: {result.get('region')}\")\nreturn result\nexcept requests.exceptions.Timeout:\nlogger.warning(f\"Alpha {alpha_id} 请求超时，跳过\")\nreturn None\nexcept Exception as e:\nlogger.error(f\"从服务器获取alpha数据失败 {alpha_id}: {e}\")\nreturn None\nclass CorrelationCalculator:\n\"\"\"相关性计算类\"\"\"\ndef __init__(self, db_manager: DatabaseManager, use_server_data: bool = False):\nself.db_manager = db_manager\nself.use_server_data = use_server_data\nself.session = None\nif use_server_data:\nself.session = get_session()\ndef wait_get(self, url: str, max_retries: int = 10, timeout_seconds: int = 3600):\n\"\"\"\n发送带有重试机制的 GET 请求，直到成功或达到最大重试次数或超时。\n此函数会根据服务器返回的 `Retry-After` 头信息进行等待，并在遇到 401 状态码时重新初始化配置。\nArgs:\nurl (str): 目标 URL。\nmax_retries (int, optional): 最大重试次数，默认为 10。\ntimeout_seconds (int, optional): 总超时时间（秒），默认为 3600（1小时）。\nReturns:\nResponse: 请求的响应对象，如果超时返回None。\n\"\"\"\nstart_time = time.time()\nretries = 0\nwhile retries < max_retries:\n# 检查是否超过总超时时间\nif time.time() - start_time > timeout_seconds:\nlogger.warning(f\"请求超时（{timeout_seconds}秒），跳过: {url}\")\nreturn None\nwhile True:\ntry:\nsimulation_progress = self.session.get(url, timeout=30)\nif simulation_progress.headers.get(\"Retry-After\", 0) == 0:\nbreak\nretry_after = float(simulation_progress.headers[\"Retry-After\"])\n# 检查等待时间是否会导致总超时\nif time.time() - start_time + retry_after > timeout_seconds:\nlogger.warning(f\"等待时间会导致超时，跳过: {url}\")\nreturn None\ntime.sleep(retry_after)\nexcept requests.exceptions.Timeout:\nlogger.warning(f\"请求超时，重试: {url}\")\nbreak\nexcept Exception as e:\nlogger.error(f\"请求异常: {e}\")\nbreak\nif simulation_progress.status_code < 400:\nbreak\nelse:\nwait_time = min(2 ** retries, 60)  # 最大等待60秒\n# 检查等待时间是否会导致总超时\nif time.time() - start_time + wait_time > timeout_seconds:\nlogger.warning(f\"重试等待会导致超时，跳过: {url}\")\nreturn None\ntime.sleep(wait_time)\nretries += 1\nreturn simulation_progress\ndef get_pnl_from_server(self, alpha_id: str, timeout_seconds: int = 3600) -> Optional[dict]:\n\"\"\"从服务器获取PNL数据，支持1小时超时限制\nArgs:\nalpha_id (str): Alpha ID\ntimeout_seconds (int): 总超时时间（秒），默认3600秒（1小时）\nReturns:\nOptional[dict]: PNL数据，超时或失败返回None\n\"\"\"\ntry:\nresponse = self.wait_get(f\"\nhttps://api.worldquantbrain.com/alphas/{alpha_id}/recordsets/pnl\n\", timeout_seconds=timeout_seconds)\nif response is None:\nlogger.error(f\"从服务器获取PNL数据失败 {alpha_id}: 请求超时或网络错误\")\nreturn None\nelif response.status_code == 200:\nreturn response.json()\nelif response.status_code == 404:\nlogger.warning(f\"Alpha {alpha_id} 不存在\")\nreturn None\nelse:\nlogger.error(f\"从服务器获取PNL数据失败 {alpha_id}: HTTP {response.status_code}\")\nreturn None\nexcept Exception as e:\nlogger.error(f\"从服务器获取PNL数据异常 {alpha_id}: {e}\")\nreturn None\ndef get_alpha_returns(self, alpha_id: str, timeout_seconds: int = 3600) -> Optional[pd.Series]:\n\"\"\"获取alpha的收益率序列，支持1小时超时限制\"\"\"\n# 根据配置选择数据源\nif self.use_server_data:\npnl_data = self.get_pnl_from_server(alpha_id, timeout_seconds=timeout_seconds)\nelse:\npnl_data = self.db_manager.get_pnl_data(alpha_id)\nif not pnl_data:\nreturn None\ntry:\nrecords = pnl_data['records']\ncolumns = [\"date\", \"pnl\"] + [f\"col{i}\" for i in range(2, len(records[0]))]\ndf = pd.DataFrame(records, columns=columns)\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.set_index('date').sort_index()\npnl_series = df['pnl']\nreturns = pnl_series.pct_change().dropna()\n# 过滤最近4年数据\ncutoff_date = returns.index.max() - pd.DateOffset(years=4)\nreturns = returns[returns.index > cutoff_date]\nreturn returns\nexcept Exception as e:\nlogger.error(f\"计算收益率失败 {alpha_id}: {e}\")\nreturn None\ndef get_alpha_pnl_series(self, alpha_id: str, timeout_seconds: int = 3600) -> Optional[pd.Series]:\n\"\"\"获取alpha的PNL序列（用于绘图），支持1小时超时限制\"\"\"\n# 根据配置选择数据源\nif self.use_server_data:\npnl_data = self.get_pnl_from_server(alpha_id, timeout_seconds=timeout_seconds)\nelse:\npnl_data = self.db_manager.get_pnl_data(alpha_id)\nif not pnl_data:\nreturn None\ntry:\nrecords = pnl_data['records']\ncolumns = [\"date\", \"pnl\"] + [f\"col{i}\" for i in range(2, len(records[0]))]\ndf = pd.DataFrame(records, columns=columns)\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.set_index('date').sort_index()\nreturn df['pnl']\nexcept Exception as e:\nlogger.error(f\"获取PNL序列失败 {alpha_id}: {e}\")\nreturn None\ndef calculate_correlation_matrix(self, alpha_ids: List[str], progress_callback=None) -> pd.DataFrame:\n\"\"\"计算alpha间的相关性矩阵\"\"\"\nlogger.info(f\"开始计算 {len(alpha_ids)} 个alpha的相关性矩阵\")\nreturns_data = {}\ntotal_alphas = len(alpha_ids)\nfor i, alpha_id in enumerate(alpha_ids, 1):\nif progress_callback:\nprogress_callback(f\"正在获取第 {i}/{total_alphas} 个Alpha的PNL数据: {alpha_id}\")\nreturns = self.get_alpha_returns(alpha_id)\nif returns is not None and len(returns) > 0:\nreturns_data[alpha_id] = returns\nif len(returns_data) < 2:\nlogger.warning(\"有效alpha数量不足，无法计算相关性\")\nreturn pd.DataFrame()\nreturns_df = pd.DataFrame(returns_data)\nreturns_df = returns_df.dropna()\nif returns_df.empty:\nlogger.warning(\"没有重叠的时间序列数据\")\nreturn pd.DataFrame()\ncorrelation_matrix = returns_df.corr()\nlogger.info(f\"相关性矩阵计算完成，维度: {correlation_matrix.shape}\")\nreturn correlation_matrix\nclass MaximalCliqueDetector:\n\"\"\"最大团检测类\"\"\"\ndef __init__(self, correlation_threshold: float = Config.CORRELATION_THRESHOLD):\nself.correlation_threshold = correlation_threshold\ndef build_correlation_graph(self, correlation_matrix: pd.DataFrame) -> nx.Graph:\n\"\"\"根据相关性矩阵构建图\"\"\"\nG = nx.Graph()\nalpha_ids = correlation_matrix.index.tolist()\nG.add_nodes_from(alpha_ids)\nfor i, alpha1 in enumerate(alpha_ids):\nfor j, alpha2 in enumerate(alpha_ids[i+1:], i+1):\ncorrelation = correlation_matrix.loc[alpha1, alpha2]\nif abs(correlation) > self.correlation_threshold:\nG.add_edge(alpha1, alpha2, weight=correlation)\nlogger.info(f\"构建相关性图完成: {G.number_of_nodes()} 个节点, {G.number_of_edges()} 条边\")\nreturn G\ndef find_maximal_cliques(self, G: nx.Graph) -> List[List[str]]:\n\"\"\"寻找不重叠的最大团\"\"\"\n# 获取所有最大团\nall_cliques = list(nx.find_cliques(G))\n# 转换为列表并按大小排序（从大到小）\nall_cliques = [list(clique) for clique in all_cliques if len(clique) > 1]\nall_cliques.sort(key=len, reverse=True)\n# 贪心算法选择不重叠的团\nselected_cliques = []\nused_nodes = set()\nfor clique in all_cliques:\n# 检查这个团是否与已选择的团有重叠\nif not any(node in used_nodes for node in clique):\nselected_cliques.append(clique)\nused_nodes.update(clique)\n# 添加独立节点（没有连接且未被包含在任何团中的节点）\nisolated_nodes = list(nx.isolates(G))\nfor node in isolated_nodes:\nif node not in used_nodes:\nselected_cliques.append([node])\nused_nodes.add(node)\n# 添加剩余未分组的节点作为单独的团\nall_nodes = set(G.nodes())\nremaining_nodes = all_nodes - used_nodes\nfor node in remaining_nodes:\nselected_cliques.append([node])\nlogger.info(f\"发现 {len(selected_cliques)} 个不重叠的团，其中 {len(isolated_nodes)} 个独立节点\")\nreturn selected_cliques\ndef detect_cliques(self, correlation_matrix: pd.DataFrame) -> List[List[str]]:\n\"\"\"检测团结构的主函数\"\"\"\nif correlation_matrix.empty:\nreturn []\nG = self.build_correlation_graph(correlation_matrix)\ncliques = self.find_maximal_cliques(G)\nreturn cliques\nclass PNLAnalyzer:\n\"\"\"PNL分析类 - 使用WorldQuant Brain官方yearly-stats API\"\"\"\ndef __init__(self, db_manager: DatabaseManager):\nself.db_manager = db_manager\nself.brain_api_url = \"\nhttps://api.worldquantbrain.com\n\"\n# 添加年度统计数据缓存，避免重复API调用\nself._yearly_stats_cache = {}\n# 添加永久失败缓存，避免对明确无数据的Alpha重复请求\nself._permanent_failures = set()\n# 添加暂时失败记录，允许后续重试\nself._temp_failure_count = {}\ndef get_alpha_yearly_stats(self, alpha_id: str, timeout_seconds: int = 3600, use_cache: bool = True) -> pd.DataFrame:\n\"\"\"\n直接从WorldQuant Brain API获取年度统计数据，支持1小时超时限制\nArgs:\nalpha_id (str): Alpha ID\ntimeout_seconds (int): 总超时时间（秒），默认3600秒（1小时）\nuse_cache (bool): 是否使用缓存，默认True\nReturns:\npd.DataFrame: 年度统计数据，超时或失败返回空DataFrame\n\"\"\"\n# 检查缓存\nif use_cache and alpha_id in self._yearly_stats_cache:\nlogger.info(f\"Alpha {alpha_id} 使用缓存的年度统计数据\")\nreturn self._yearly_stats_cache[alpha_id]\n# 检查是否为永久失败（明确无数据的Alpha）\nif use_cache and alpha_id in self._permanent_failures:\nlogger.info(f\"Alpha {alpha_id} 已知无年度统计数据，跳过\")\nreturn pd.DataFrame()\n# 检查暂时失败次数，如果失败次数过多，暂时跳过\ntemp_failures = self._temp_failure_count.get(alpha_id, 0)\nif temp_failures >= 3:  # 连续3次暂时失败后，暂停一次\nlogger.info(f\"Alpha {alpha_id} 连续失败{temp_failures}次，暂停请求\")\nreturn pd.DataFrame()\nstart_time = time.time()\ns = get_session()\nmax_retries = 10  # 增加重试次数到10次，提高成功率\nmax_wait_time = 60  # 单次最大等待时间\nretry_count = 0\nwhile retry_count < max_retries:\n# 检查是否超过总超时时间\nelapsed_time = time.time() - start_time\nif elapsed_time > timeout_seconds:\nlogger.warning(f\"Alpha {alpha_id} 获取年度统计数据超时（{timeout_seconds}秒），跳过\")\nreturn pd.DataFrame()\ntry:\n# 计算剩余超时时间\nremaining_timeout = min(30, timeout_seconds - elapsed_time)\nif remaining_timeout <= 0:\nlogger.warning(f\"Alpha {alpha_id} 剩余时间不足，跳过\")\nreturn pd.DataFrame()\nresult = s.get(\nf\"{self.brain_api_url}/alphas/{alpha_id}/recordsets/yearly-stats\",\ntimeout=remaining_timeout\n)\nif \"retry-after\" in result.headers:\nretry_after = float(result.headers[\"Retry-After\"])\n# 优先使用服务器指定的重试时间，服务器最清楚自己的负载情况\nwait_time = min(retry_after, max_wait_time)\n# 检查等待时间是否会导致总超时\nif elapsed_time + wait_time > timeout_seconds:\nlogger.warning(f\"Alpha {alpha_id} 等待时间会导致超时，跳过\")\n# 记录为暂时失败\nif use_cache:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nreturn pd.DataFrame()\nlogger.info(f\"Alpha {alpha_id} 需要等待 {wait_time:.1f} 秒后重试（第 {retry_count + 1}/{max_retries} 次）\")\ntime.sleep(wait_time)\nretry_count += 1\ncontinue\nelse:\nbreak\nexcept requests.exceptions.Timeout:\nlogger.warning(f\"Alpha {alpha_id} 请求超时，重试 {retry_count + 1}/{max_retries}\")\nretry_count += 1\nif retry_count < max_retries:\n# 使用指数退避策略\nexponential_wait = 2 ** retry_count\nwait_time = min(exponential_wait, 30)  # 最多等待30秒\n# 检查是否还有时间重试\nif time.time() - start_time + wait_time > timeout_seconds:\nlogger.warning(f\"Alpha {alpha_id} 重试等待会导致超时，跳过\")\n# 记录为暂时失败\nif use_cache:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nreturn pd.DataFrame()\nlogger.info(f\"Alpha {alpha_id} 等待 {wait_time} 秒后重试\")\ntime.sleep(wait_time)\ncontinue\nexcept Exception as e:\nlogger.error(f\"请求Alpha {alpha_id} yearly-stats失败: {e}\")\nretry_count += 1\nif retry_count < max_retries:\n# 使用指数退避策略\nexponential_wait = 2 ** retry_count\nwait_time = min(exponential_wait, 30)  # 最多等待30秒\nif time.time() - start_time + wait_time > timeout_seconds:\nlogger.warning(f\"Alpha {alpha_id} 重试等待会导致超时，跳过\")\n# 记录为暂时失败\nif use_cache:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nreturn pd.DataFrame()\nlogger.info(f\"Alpha {alpha_id} 等待 {wait_time} 秒后重试\")\ntime.sleep(wait_time)\ncontinue\nelse:\n# 记录为暂时失败\nif use_cache:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nreturn pd.DataFrame()\nif retry_count >= max_retries:\nlogger.error(f\"Alpha {alpha_id} 达到最大重试次数，暂时放弃请求\")\n# 记录为暂时失败，不缓存空结果，允许后续重试\nif use_cache:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nlogger.debug(f\"Alpha {alpha_id} 暂时失败记录: {self._temp_failure_count[alpha_id]} 次\")\nreturn pd.DataFrame()\nif result.status_code != 200:\nlogger.error(f\"Alpha {alpha_id} yearly-stats API请求失败，状态码: {result.status_code}\")\n# 404表示Alpha不存在，记录为永久失败；其他错误记录为暂时失败\nif use_cache:\nif result.status_code == 404:\nself._permanent_failures.add(alpha_id)\nlogger.debug(f\"Alpha {alpha_id} 不存在，记录为永久失败\")\nelse:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nlogger.debug(f\"Alpha {alpha_id} HTTP错误，记录为暂时失败\")\nreturn pd.DataFrame()\ntry:\nstats = result.json()\nexcept Exception as e:\nlogger.error(f\"Alpha {alpha_id} 解析响应JSON失败: {e}\")\n# JSON解析失败通常是暂时问题\nif use_cache:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nlogger.debug(f\"Alpha {alpha_id} JSON解析失败，记录为暂时失败\")\nreturn pd.DataFrame()\nif stats.get(\"records\", 0) == 0:\nlogger.warning(f\"Alpha {alpha_id} 没有找到年度统计数据\")\n# 没有数据记录通常意味着这个Alpha确实没有年度数据，记录为永久失败\nif use_cache:\nself._permanent_failures.add(alpha_id)\nlogger.debug(f\"Alpha {alpha_id} 无年度数据，记录为永久失败\")\nreturn pd.DataFrame()\ntry:\ncolumns = [dct[\"name\"] for dct in stats[\"schema\"][\"properties\"]]\nyearly_stats_df = pd.DataFrame(stats[\"records\"], columns=columns)\nlogger.info(f\"Alpha {alpha_id} 年度统计数据获取成功\")\n# 缓存结果\nif use_cache:\nself._yearly_stats_cache[alpha_id] = yearly_stats_df\n# 成功获取数据后，清理失败记录\nif alpha_id in self._temp_failure_count:\ndel self._temp_failure_count[alpha_id]\nlogger.debug(f\"Alpha {alpha_id} 年度统计数据已缓存\")\nreturn yearly_stats_df\nexcept Exception as e:\nlogger.error(f\"Alpha {alpha_id} 构建DataFrame失败: {e}\")\n# DataFrame构建失败通常是数据格式问题，记录为暂时失败\nif use_cache:\nself._temp_failure_count[alpha_id] = temp_failures + 1\nlogger.debug(f\"Alpha {alpha_id} DataFrame构建失败，记录为暂时失败\")\nreturn pd.DataFrame()\ndef reset_temp_failures(self, alpha_id: str = None):\n\"\"\"重置暂时失败记录，允许重新尝试\nArgs:\nalpha_id (str, optional): 指定Alpha ID重置，为None时重置所有\n\"\"\"\nif alpha_id:\nif alpha_id in self._temp_failure_count:\ndel self._temp_failure_count[alpha_id]\nlogger.info(f\"已重置Alpha {alpha_id} 的暂时失败记录\")\nelse:\nself._temp_failure_count.clear()\nlogger.info(\"已重置所有Alpha的暂时失败记录\")\ndef get_failure_status(self, alpha_id: str) -> dict:\n\"\"\"获取Alpha的失败状态信息\"\"\"\nreturn {\n'is_permanent_failure': alpha_id in self._permanent_failures,\n'temp_failure_count': self._temp_failure_count.get(alpha_id, 0),\n'is_cached': alpha_id in self._yearly_stats_cache\n}\ndef calculate_annual_metrics(self, pnl_series: pd.Series = None, alpha_id: str = None) -> Dict[str, Dict[str, float]]:\n\"\"\"\n获取年度指标 - 优先使用官方API，如果失败则回退到PNL计算\n\"\"\"\nif alpha_id:\n# 尝试使用官方API（使用缓存）\nyearly_stats = self.get_alpha_yearly_stats(alpha_id, use_cache=True)\nif not yearly_stats.empty:\nreturn self._parse_official_yearly_stats(yearly_stats)\n# 如果API失败，回退到PNL计算（仅用于画图）\nif pnl_series is not None:\nlogger.warning(f\"Alpha {alpha_id} 使用PNL数据计算年度指标（仅供参考）\")\nreturn self._calculate_from_pnl(pnl_series)\nreturn {}\ndef _parse_official_yearly_stats(self, yearly_stats: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n\"\"\"\n解析官方年度统计数据\n\"\"\"\nannual_metrics = {}\nfor _, row in yearly_stats.iterrows():\nyear_str = str(int(row.get('year', 0)))\nif year_str == '0':\ncontinue\n# 解析各项指标，处理可能的字段名变化\nmetrics = {\n'return': self._safe_get_metric(row, ['returns', 'return', 'annual_return']) * 100,  # 转换为百分比\n'sharpe': self._safe_get_metric(row, ['sharpe', 'sharpe_ratio', 'sr']),\n'drawdown': self._safe_get_metric(row, ['drawdown', 'max_drawdown', 'mdd']) * 100,  # 转换为百分比\n'turnover': self._safe_get_metric(row, ['turnover']) * 100,  # 转换为百分比\n'fitness': self._safe_get_metric(row, ['fitness']),\n'margin': self._safe_get_metric(row, ['margin']) * 10000,  # 转换为万分比\n'longCount': self._safe_get_metric(row, ['longCount', 'long_count']),\n'shortCount': self._safe_get_metric(row, ['shortCount', 'short_count'])\n}\nannual_metrics[year_str] = metrics\nreturn annual_metrics\ndef _safe_get_metric(self, row: pd.Series, possible_names: List[str]) -> float:\n\"\"\"\n安全获取指标值，尝试多个可能的字段名\n\"\"\"\nfor name in possible_names:\nif name in row and pd.notna(row[name]):\nreturn float(row[name])\nreturn 0.0\ndef _calculate_from_pnl(self, pnl_series: pd.Series) -> Dict[str, Dict[str, float]]:\n\"\"\"\n从PNL数据计算年度指标（回退方案，仅供参考）\n\"\"\"\nannual_metrics = {}\n# 按年分组\nfor year in pnl_series.index.year.unique():\nyear_data = pnl_series[pnl_series.index.year == year]\nif len(year_data) < 10:  # 数据点太少跳过\ncontinue\n# 计算收益率\nreturns = year_data.pct_change().dropna()\nif len(returns) == 0:\ncontinue\n# 计算各项指标\nannual_return = (year_data.iloc[-1] / year_data.iloc[0] - 1) * 100 if year_data.iloc[0] != 0 else 0\nvolatility = returns.std() * np.sqrt(252)  # 年化波动率\nsharpe = returns.mean() / returns.std() * np.sqrt(252) if returns.std() > 0 else 0\n# 计算最大回撤\ncumulative = (1 + returns).cumprod()\nrunning_max = cumulative.expanding().max()\ndrawdown = (cumulative / running_max - 1).min() * 100\n# 计算换手率（简化版本）\nturnover = returns.abs().mean() * 252 * 100\nannual_metrics[str(year)] = {\n'return': annual_return,\n'sharpe': sharpe,\n'drawdown': abs(drawdown),\n'turnover': turnover,\n'volatility': volatility,\n'fitness': 0,  # PNL无法计算fitness\n'margin': 0,   # PNL无法计算margin\n'longCount': 0,  # PNL无法计算longCount\n'shortCount': 0  # PNL无法计算shortCount\n}\nreturn annual_metrics\nclass MaximalCliqueAnalyzer:\n\"\"\"最大团Alpha分析器主类\n该类整合了数据获取、相关性计算、最大团检测和可视化功能，\n为Alpha相关性分析提供完整的解决方案。\n\"\"\"\ndef __init__(self, use_server_data: bool = False):\nself.db_manager = DatabaseManager()\nself.correlation_calculator = CorrelationCalculator(self.db_manager, use_server_data)\nself.clique_detector = MaximalCliqueDetector()\nself.pnl_analyzer = PNLAnalyzer(self.db_manager)\nself.use_server_data = use_server_data\nself.session = get_session() if use_server_data else None\n# 缓存管理，避免重复获取相同数据\nself._preloaded_alphas = set()\ndef find_related_alphas(self, alpha_id: str, limit: int = 100) -> List[str]:\n\"\"\"查找与给定alpha相关的其他alpha\"\"\"\nif self.use_server_data:\n# 当使用服务器数据时，完全避免数据库调用\nlogger.info(f\"使用服务器数据模式，避免数据库调用\")\nif not self.session:\nlogger.warning(\"服务器session未初始化，返回单个alpha\")\nreturn [alpha_id]\n# 获取当前alpha的region信息（从服务器）\nalpha_data = self.db_manager.get_alpha_data_from_server(alpha_id, self.session)\nif not alpha_data or not alpha_data.get('region'):\nlogger.warning(f\"无法获取alpha {alpha_id} 的region信息，返回单个alpha\")\nreturn [alpha_id]\ntarget_region = alpha_data['region']\nlogger.info(f\"Alpha {alpha_id} 的region: {target_region}\")\n# 在服务器数据模式下，由于API限制和避免数据库调用的要求\n# 直接返回单个alpha，确保不会触发任何数据库操作\nlogger.info(f\"服务器数据模式：返回单个alpha {alpha_id}，避免数据库查询\")\nreturn [alpha_id]\n# 仅在非服务器数据模式下使用数据库\nconn = self.db_manager.get_connection()\nif not conn:\nreturn [alpha_id]\ntry:\nwith conn.cursor() as cursor:\n# 获取同一region的alpha\nsql_region = \"SELECT region FROM alpha_data WHERE id = %s\"\ncursor.execute(sql_region, (alpha_id,))\nregion_result = cursor.fetchone()\nif not region_result:\nreturn [alpha_id]\nregion = region_result['region']\n# 查找同region的其他alpha（限制数量）\nsql_related = \"\"\"\nSELECT id FROM alpha_data\nWHERE region = %s AND id != %s\nORDER BY RAND()\nLIMIT %s\n\"\"\"\ncursor.execute(sql_related, (region, alpha_id, limit - 1))\nrelated_results = cursor.fetchall()\nrelated_alphas = [alpha_id]  # 包含原始alpha\nfor result in related_results:\nrelated_alphas.append(result['id'])\nlogger.info(f\"找到 {len(related_alphas)} 个相关alpha（region: {region}）\")\nreturn related_alphas\nexcept Exception as e:\nlogger.error(f\"查找相关alpha失败: {e}\")\nreturn [alpha_id]\nfinally:\nconn.close()\ndef process_alpha_cliques(self, alpha_id: str) -> Tuple[List[List[str]], pd.DataFrame]:\n\"\"\"处理alpha的团结构\"\"\"\n# 查找相关alpha\nrelated_alphas = self.find_related_alphas(alpha_id)\nif len(related_alphas) < 2:\nlogger.info(f\"只找到 {len(related_alphas)} 个相关alpha，无法计算团结构\")\nreturn [[alpha_id]], pd.DataFrame()\n# 计算相关性矩阵\ncorrelation_matrix = self.correlation_calculator.calculate_correlation_matrix(related_alphas)\nif correlation_matrix.empty:\nreturn [[alpha_id]], correlation_matrix\n# 检测团结构\ncliques = self.clique_detector.detect_cliques(correlation_matrix)\nreturn cliques, correlation_matrix\ndef create_pnl_plot(self, clique: List[str]) -> go.Figure:\n\"\"\"创建PNL曲线图\"\"\"\nfig = go.Figure()\ncolors = px.colors.qualitative.Set1\nvalid_alphas = 0\nfor i, alpha_id in enumerate(clique):\n# 获取PNL数据\npnl_series = self.correlation_calculator.get_alpha_pnl_series(alpha_id)\nif pnl_series is None or len(pnl_series) == 0:\nlogger.warning(f\"Alpha {alpha_id} 无有效PNL数据\")\ncontinue\n# 获取年度统计数据并格式化为悬停文本（优化：只创建一次，不为每个点重复创建）\nalpha_hover_text = self.get_alpha_yearly_stats_formatted(alpha_id)\n# 将换行符转换为HTML的<br>标签以确保正确显示\nformatted_table = alpha_hover_text.replace('\\n', '<br>')\n# 创建优化的悬停模板，只显示Alpha名称和年度统计数据，不显示具体的时间和PNL值\nhover_template = f\"<b>Alpha: {alpha_id}</b><br><br><b>年度统计数据:</b><br><span style='font-family: monospace; font-size: 11px; line-height: 1.4; white-space: pre;'>{formatted_table}</span><extra></extra>\"\n# 添加曲线（优化：使用自定义hovertemplate而不是为每个点创建text）\nfig.add_trace(go.Scatter(\nx=pnl_series.index,\ny=pnl_series.values,\nmode='lines',\nname=alpha_id,\nline=dict(color=colors[i % len(colors)], width=2),\nhovertemplate=hover_template,\nconnectgaps=True  # 连接缺失数据点\n))\nvalid_alphas += 1\n# 更新布局\ntitle = f'Alpha PNL曲线图 ({valid_alphas}/{len(clique)} 个有效Alpha)'\nfig.update_layout(\ntitle=title,\nxaxis_title='日期',\nyaxis_title='PNL',\nhovermode='closest',\nshowlegend=True,\nwidth=600,\nheight=900,\nxaxis=dict(\nshowgrid=True,\ngridwidth=1,\ngridcolor='lightgray'\n),\nyaxis=dict(\nshowgrid=True,\ngridwidth=1,\ngridcolor='lightgray'\n)\n)\nif valid_alphas == 0:\n# 如果没有有效数据，显示提示信息\nfig.add_annotation(\ntext=\"无有效PNL数据可显示\",\nxref=\"paper\", yref=\"paper\",\nx=0.5, y=0.5,\nshowarrow=False,\nfont=dict(size=16, color=\"red\")\n)\nreturn fig\ndef get_alpha_yearly_stats_formatted(self, alpha_id: str) -> str:\n\"\"\"获取格式化的Alpha年度统计信息\"\"\"\n# 直接使用已缓存的数据，避免重复API调用\nyearly_stats_df = self.pnl_analyzer.get_alpha_yearly_stats(alpha_id, use_cache=True)\nif yearly_stats_df.empty or 'year' not in yearly_stats_df.columns:\nreturn \"No yearly statistics data available\"\n# 按年份排序\nyearly_stats_sorted = yearly_stats_df.sort_values('year')\n# 定义表格列顺序和格式 - 按照用户要求的顺序排列\ntable_columns = [\n('year', 'Year', '', 0, 10),\n('sharpe', 'Sharpe', '', 2, 12),\n('turnover', 'Turnover(%)', '*100', 2, 18),\n('fitness', 'Fitness', '', 2, 15),\n('returns', 'Returns(%)', '*100', 2, 18),\n('drawdown', 'Drawdown(%)', '*100', 2, 18),\n('margin', 'Margin(‱)', '*10000', 2, 18),\n('longCount', 'LongCnt', '', 0, 15),\n('shortCount', 'ShortCnt', '', 0, 15)\n]\n# 构建表格头部\nheader_parts = []\nfor _, display_name, _, _, width in table_columns:\nheader_parts.append(display_name.center(width))\nresult = \"|\".join(header_parts) + \"|\\n\"\n# 添加分隔线\nseparator_parts = []\nfor _, _, _, _, width in table_columns:\nseparator_parts.append(\"-\" * width)\nresult += \"|\".join(separator_parts) + \"|\\n\"\n# 添加总体数据行（从alpha_data表查询）\nif self.use_server_data and self.session:\nalpha_data = self.db_manager.get_alpha_data_from_server(alpha_id, self.session)\nelse:\nalpha_data = self.db_manager.get_alpha_data(alpha_id)\noverall_parts = []\nfor col_key, _, multiplier, decimals, width in table_columns:\nif col_key == 'year':\nformatted_value = 'all'\nelse:\n# 映射列名到alpha_data字段\nfield_mapping = {\n'sharpe': 'sharpe',\n'returns': 'returns',\n'drawdown': 'drawdown',\n'turnover': 'turnover',\n'fitness': 'fitness',\n'margin': 'margin',\n'longCount': 'long_count',\n'shortCount': 'short_count'\n}\nfield_name = field_mapping.get(col_key)\nif alpha_data and field_name and field_name in alpha_data and alpha_data[field_name] is not None:\ntry:\nvalue = alpha_data[field_name]\nfval = float(value)\nif multiplier == '*100':\nfval = fval * 100\nif decimals == 0:\nformatted_value = f\"{int(fval)}%\"\nelse:\nformatted_value = f\"{fval:.{decimals}f}%\"\nelif multiplier == '*10000':\nfval = fval * 10000\nif decimals == 0:\nformatted_value = f\"{int(fval)}‱\"\nelse:\nformatted_value = f\"{fval:.{decimals}f}‱\"\nelse:\nif decimals == 0:\nformatted_value = str(int(fval))\nelse:\nformatted_value = f\"{fval:.{decimals}f}\"\nexcept Exception:\nformatted_value = \"NAN\"\nelse:\nformatted_value = \"NAN\"\n# 居中对齐年份，右对齐数字\nif col_key == 'year':\noverall_parts.append(formatted_value.center(width))\nelse:\noverall_parts.append(formatted_value.rjust(width))\nresult += \"|\".join(overall_parts) + \"|\\n\"\n# 构建数据行\nfor _, year_row in yearly_stats_sorted.iterrows():\ndata_parts = []\nfor col_key, _, multiplier, decimals, width in table_columns:\n# 查找匹配的列名（处理大小写不敏感）\nactual_col = None\nfor col in yearly_stats_df.columns:\nif col.lower() == col_key.lower() or col.lower().replace('_', '') == col_key.lower():\nactual_col = col\nbreak\nif actual_col and actual_col in year_row and not pd.isna(year_row[actual_col]):\nvalue = year_row[actual_col]\ntry:\nif col_key == 'year':\nformatted_value = str(value)\nelse:\nfval = float(value)\nif multiplier == '*100':\nfval = fval * 100\nif decimals == 0:\nformatted_value = f\"{int(fval)}%\"\nelse:\nformatted_value = f\"{fval:.{decimals}f}%\"\nelif multiplier == '*10000':\nfval = fval * 10000\nif decimals == 0:\nformatted_value = f\"{int(fval)}‱\"\nelse:\nformatted_value = f\"{fval:.{decimals}f}‱\"\nelse:\nif decimals == 0:\nformatted_value = str(int(fval))\nelse:\nformatted_value = f\"{fval:.{decimals}f}\"\nexcept Exception:\nformatted_value = str(value)\nelse:\nformatted_value = \"-\"\n# 居中对齐年份，右对齐数字\nif col_key == 'year':\ndata_parts.append(formatted_value.center(width))\nelse:\ndata_parts.append(formatted_value.rjust(width))\nresult += \"|\".join(data_parts) + \"|\\n\"\nreturn result\ndef create_annual_metrics_table(self, clique: List[str], progress_callback=None) -> pd.DataFrame:\n\"\"\"创建年度指标表格 - 使用官方yearly-stats API\"\"\"\n# 首先预加载所有alpha的年度统计数据，避免重复API调用\nlogger.info(f\"开始为团预加载 {len(clique)} 个Alpha的年度统计数据\")\nself.preload_yearly_stats(clique, progress_callback)\nall_annual_data = []\ntotal_alphas = len(clique)\nfailed_count = 0\nmax_consecutive_failures = 3  # 最大连续失败次数\nfor i, alpha_id in enumerate(clique, 1):\n# 已经预加载数据，不需要更新进度提示\ntry:\n# 优先使用官方API获取年度指标（从缓存获取）\nlogger.debug(f\"开始为Alpha {alpha_id} 获取年度指标数据\")\nannual_metrics = self.pnl_analyzer.calculate_annual_metrics(alpha_id=alpha_id)\n# 如果官方API失败，尝试使用PNL数据作为回退\nif not annual_metrics:\nlogger.debug(f\"Alpha {alpha_id} 官方API失败，尝试使用PNL数据\")\npnl_series = self.correlation_calculator.get_alpha_pnl_series(alpha_id)\nif pnl_series is not None and len(pnl_series) > 0:\nannual_metrics = self.pnl_analyzer.calculate_annual_metrics(pnl_series=pnl_series, alpha_id=alpha_id)\nif not annual_metrics:\nlogger.warning(f\"Alpha {alpha_id} 无法获取年度指标数据，跳过\")\nfailed_count += 1\n# 如果连续失败次数过多，提示用户\nif failed_count >= max_consecutive_failures:\nlogger.warning(f\"连续 {failed_count} 个Alpha获取数据失败，可能存在网络问题\")\nfailed_count = 0  # 重置计数器\ncontinue\nelse:\nfailed_count = 0  # 成功后重置失败计数器\nfor year, metrics in annual_metrics.items():\nyear_int = int(year)\n# 只包含2013年及以后的数据\nif year_int >= 2013:\n# 创建基础行数据，只包含可以从PNL计算的指标\nrow = {\n'Alpha': alpha_id,\n'Year': year_int,\n'Sharpe': round(metrics.get('sharpe', 0), 4),\n'Returns (%)': round(metrics.get('return', 0), 2),\n'Drawdown (%)': round(metrics.get('drawdown', 0), 2)\n}\n# 只有当指标值不为0时才添加其他指标（避免显示无意义的0值）\nturnover = metrics.get('turnover', 0)\nif turnover > 0:\nrow['Turnover (%)'] = round(turnover, 2)\nfitness = metrics.get('fitness', 0)\nif fitness > 0:\nrow['Fitness'] = round(fitness, 4)\nmargin = metrics.get('margin', 0)\nif margin > 0:\nrow['Margin (‱)'] = round(margin, 2)\nlong_count = metrics.get('longCount', 0)\nif long_count > 0:\nrow['Long Count'] = int(long_count)\nshort_count = metrics.get('shortCount', 0)\nif short_count > 0:\nrow['Short Count'] = int(short_count)\nall_annual_data.append(row)\nexcept Exception as e:\nlogger.error(f\"获取Alpha {alpha_id} 年度指标失败: {e}\")\ncontinue\ndf = pd.DataFrame(all_annual_data)\n# 按Alpha和年份排序（每个alpha的各年数据聚集在一起）\nif not df.empty:\ndf = df.sort_values(['Alpha', 'Year'], ascending=[True, True])\ndf = df.reset_index(drop=True)\n# 动态确定列的顺序，按照用户要求的顺序排列\nbase_columns = ['Alpha', 'Year', 'Sharpe', 'Turnover (%)', 'Fitness', 'Returns (%)', 'Drawdown (%)']\noptional_columns = ['Margin (‱)', 'Long Count', 'Short Count']\n# 构建最终的列顺序\ncolumn_order = base_columns + [col for col in optional_columns if col in df.columns]\ndf = df[column_order]\nreturn df\ndef preload_yearly_stats(self, alpha_ids: List[str], progress_callback=None):\n\"\"\"预加载年度统计数据，避免后续重复API调用\"\"\"\nlogger.info(f\"开始预加载 {len(alpha_ids)} 个Alpha的年度统计数据\")\n# 过滤已经加载过的alpha\nalphas_to_load = [alpha_id for alpha_id in alpha_ids if alpha_id not in self._preloaded_alphas]\nif not alphas_to_load:\nlogger.info(\"所有Alpha的年度统计数据已预加载\")\nreturn\nlogger.info(f\"需要加载 {len(alphas_to_load)} 个新Alpha的年度统计数据\")\ntotal_alphas = len(alphas_to_load)\nfor i, alpha_id in enumerate(alphas_to_load, 1):\nif progress_callback:\nprogress_callback(f\"预加载第 {i}/{total_alphas} 个Alpha的年度统计数据: {alpha_id}\")\ntry:\n# 使用缓存加载年度统计数据\nself.pnl_analyzer.get_alpha_yearly_stats(alpha_id, use_cache=True)\nself._preloaded_alphas.add(alpha_id)\n# 添加适度延时，避免请求过频\nif i < total_alphas:  # 最后一个不用等待\n# 使用递增延时，最初0.2秒，逐步增加到最套1秒\ndelay = min(0.2 + (i-1) * 0.1, 1.0)\ntime.sleep(delay)\nexcept Exception as e:\nlogger.warning(f\"预加载Alpha {alpha_id} 年度统计数据失败: {e}\")\n# 即使失败也标记为已尝试，避免后续重复尝试\nself._preloaded_alphas.add(alpha_id)\ncontinue\nlogger.info(f\"年度统计数据预加载完成\")\ndef reset_yearly_stats_failures(self, alpha_ids: List[str] = None):\n\"\"\"重置年度统计数据的失败状态，允许重新尝试\nArgs:\nalpha_ids (List[str], optional): 指定要重置的Alpha ID列表，为None时重置所有\n\"\"\"\nif alpha_ids:\nfor alpha_id in alpha_ids:\nself.pnl_analyzer.reset_temp_failures(alpha_id)\n# 从预加载集合中移除，允许重新预加载\nself._preloaded_alphas.discard(alpha_id)\nlogger.info(f\"已重置 {len(alpha_ids)} 个Alpha的失败状态\")\nelse:\nself.pnl_analyzer.reset_temp_failures()\nself._preloaded_alphas.clear()\nlogger.info(\"已重置所有Alpha的失败状态\")\ndef get_failed_alphas_info(self, alpha_ids: List[str]) -> dict:\n\"\"\"获取Alpha列表中失败的Alpha信息\"\"\"\nfailed_info = {\n'permanent_failures': [],\n'temp_failures': [],\n'success': []\n}\nfor alpha_id in alpha_ids:\nstatus = self.pnl_analyzer.get_failure_status(alpha_id)\nif status['is_permanent_failure']:\nfailed_info['permanent_failures'].append(alpha_id)\nelif status['temp_failure_count'] > 0:\nfailed_info['temp_failures'].append({\n'alpha_id': alpha_id,\n'failure_count': status['temp_failure_count']\n})\nelif status['is_cached']:\nfailed_info['success'].append(alpha_id)\nreturn failed_info\ndef run_analysis(self, alpha_id: str):\n\"\"\"运行分析的主函数\"\"\"\nlogger.info(f\"开始分析alpha: {alpha_id}\")\n# 处理团结构\ncliques, correlation_matrix = self.process_alpha_cliques(alpha_id)\nlogger.info(f\"发现 {len(cliques)} 个团\")\nresults = []\nfor i, clique in enumerate(cliques):\nlogger.info(f\"处理第 {i+1} 个团，包含 {len(clique)} 个alpha: {clique}\")\n# 创建PNL图\npnl_fig = self.create_pnl_plot(clique)\n# 创建年度指标表\nannual_table = self.create_annual_metrics_table(clique)\nresults.append({\n'clique': clique,\n'pnl_figure': pnl_fig,\n'annual_metrics': annual_table\n})\nreturn results\ndef run_batch_analysis(self, alpha_ids: List[str]):\n\"\"\"对alpha ID列表进行批量团检测分析\"\"\"\nlogger.info(f\"开始批量分析alpha列表: {alpha_ids}\")\n# 预加载所有alpha的年度统计数据，避免后续重复API调用\nlogger.info(\"开始预加载批量分析的年度统计数据\")\nself.preload_yearly_stats(alpha_ids)\n# 直接对提供的alpha列表计算相关性矩阵\ncorrelation_matrix = self.correlation_calculator.calculate_correlation_matrix(alpha_ids)\nif correlation_matrix.empty:\nlogger.warning(\"无法计算相关性矩阵\")\nreturn []\n# 检测团结构\ncliques = self.clique_detector.detect_cliques(correlation_matrix)\nlogger.info(f\"发现 {len(cliques)} 个团\")\nresults = []\nfor i, clique in enumerate(cliques):\nlogger.info(f\"处理第 {i+1} 个团，包含 {len(clique)} 个alpha: {clique}\")\n# 创建PNL图\npnl_fig = self.create_pnl_plot(clique)\n# 创建年度指标表\nannual_table = self.create_annual_metrics_table(clique)\nresults.append({\n'clique': clique,\n'pnl_figure': pnl_fig,\n'annual_metrics': annual_table\n})\nreturn results\n# Streamlit界面\ndef main(use_server_data: bool = False):\n# 设置Streamlit配置\nst.set_page_config(\npage_title=\"Maximal Clique Alpha Analyzer\",\nlayout=\"wide\",\ninitial_sidebar_state=\"expanded\"\n)\nst.title(\"🔍 最大团Alpha分析工具\")\nst.markdown(\"\"\"\n基于图论最大团算法的Alpha相关性分析系统，用于识别和分析高度相关的Alpha组合。\n**核心功能：**\n- 🎯 基于相关性阈值构建Alpha关联图\n- 🔗 使用最大团算法识别相关Alpha集合\n- 📈 交互式PNL表现可视化\n- 📊 悬停显示详细性能指标\n- 📅 年度统计数据分析\n- 🔄 支持数据库和API双数据源\n\"\"\")\n# 侧边栏配置\nwith st.sidebar:\nst.header(\"⚙️ 配置选项\")\n# 相关性阈值设置\ncorrelation_threshold = st.slider(\n\"相关性阈值\",\nmin_value=0.1,\nmax_value=0.9,\nvalue=0.5,\nstep=0.05,\nhelp=\"用于构建相关性图的阈值，越高表示要求更强的相关性\"\n)\n# 相关Alpha数量限制\nalpha_limit = st.number_input(\n\"相关Alpha数量限制\",\nmin_value=5,\nmax_value=1000,\nvalue=1000,\nhelp=\"查找相关Alpha的最大数量\"\n)\nst.markdown(\"---\")\nst.markdown(\"\"\"\n**使用说明：**\n1. 输入要分析的Alpha ID\n2. 调整相关性阈值和数量限制\n3. 点击开始分析\n4. 查看PNL曲线和年度指标\n5. 鼠标悬停查看详细数据\n\"\"\")\n# 主界面\ncol1, col2 = st.columns([3, 1])\nwith col1:\nalpha_input = st.text_area(\n\"🎯 请输入Alpha ID列表:\",\nvalue=\"\",\nplaceholder=\"例如: ['9Olpx1e','gVa8e60','bz7gNWm','355ZJ5z','OVjQoo1','ELLJbV1','QvQqQLG']\\n或者单个ID: alpha_001\",\nhelp=\"输入要分析的Alpha标识符列表（Python列表格式）或单个Alpha ID\",\nheight=100\n)\nwith col2:\nst.write(\"\")\nst.write(\"\")\nanalyze_button = st.button(\n\"🚀 开始分析\",\ntype=\"primary\",\nuse_container_width=True\n)\nif analyze_button and alpha_input:\n# 解析alpha ID输入\ntry:\n# 尝试解析为Python列表\nif alpha_input.strip().startswith('[') and alpha_input.strip().endswith(']'):\nimport ast\nalpha_ids = ast.literal_eval(alpha_input.strip())\nif not isinstance(alpha_ids, list):\nst.error(\"❌ 输入格式错误，请输入有效的Python列表格式\")\nst.stop()\nelse:\n# 单个alpha ID\nalpha_ids = [alpha_input.strip()]\nexcept Exception as e:\nst.error(f\"❌ 输入格式错误: {str(e)}\")\nst.stop()\n# 更新配置\nConfig.CORRELATION_THRESHOLD = correlation_threshold\nanalyzer = MaximalCliqueAnalyzer(use_server_data=use_server_data)\n# 显示分析进度\nprogress_bar = st.progress(0)\nstatus_text = st.empty()\ntry:\nstatus_text.text(\"正在处理Alpha ID列表...\")\nprogress_bar.progress(10)\n# 创建进度回调函数\ndef update_progress(message):\nstatus_text.text(message)\n# 判断是单个alpha还是多个alpha\nif len(alpha_ids) == 1:\n# 单个alpha分析，跳过团检测，直接绘制PNL和年度数据\nstatus_text.text(f\"正在分析Alpha {alpha_ids[0]}...\")\nprogress_bar.progress(30)\n# 预加载年度统计数据，避免重复调用\nstatus_text.text(f\"预加载年度统计数据...\")\nanalyzer.preload_yearly_stats(alpha_ids, progress_callback=update_progress)\nprogress_bar.progress(50)\ncliques = [alpha_ids]  # 只包含输入的alpha\nstatus_text.text(\"正在生成分析结果...\")\nprogress_bar.progress(80)\n# 生成结果\nresults = []\ntotal_cliques = len(cliques)\nfor i, clique in enumerate(cliques):\ntry:\nstatus_text.text(f\"正在处理第 {i+1}/{total_cliques} 个团 (包含 {len(clique)} 个Alpha)...\")\n# 更新进度条\nprogress = 70 + (i / total_cliques) * 25  # 70-95%的进度\nprogress_bar.progress(int(progress))\n# 创建PNL图\nlogger.info(f\"开始为团 {i+1} 创建PNL图\")\npnl_fig = analyzer.create_pnl_plot(clique)\n# 创建年度指标表（带进度显示）\nlogger.info(f\"开始为团 {i+1} 获取年度指标数据\")\nannual_table = analyzer.create_annual_metrics_table(clique, progress_callback=update_progress)\nresults.append({\n'clique': clique,\n'pnl_figure': pnl_fig,\n'annual_metrics': annual_table\n})\nlogger.info(f\"团 {i+1} 处理完成\")\nexcept Exception as e:\nlogger.error(f\"处理团 {i+1} 时发生错误: {e}\")\nst.warning(f\"⚠️ 团 {i+1} 处理失败，已跳过: {str(e)}\")\ncontinue\nelse:\n# 多个alpha，直接对列表进行团检测\n# 首先预加载所有alpha的年度统计数据\nstatus_text.text(f\"预加载 {len(alpha_ids)} 个Alpha的年度统计数据...\")\nanalyzer.preload_yearly_stats(alpha_ids, progress_callback=update_progress)\nprogress_bar.progress(20)\nstatus_text.text(f\"正在对 {len(alpha_ids)} 个Alpha计算相关性矩阵...\")\nprogress_bar.progress(30)\n# 计算相关性矩阵（带进度显示）\ncorrelation_matrix = analyzer.correlation_calculator.calculate_correlation_matrix(\nalpha_ids, progress_callback=update_progress\n)\nstatus_text.text(\"正在检测团结构...\")\nprogress_bar.progress(60)\n# 检测团结构\ncliques = analyzer.clique_detector.detect_cliques(correlation_matrix)\nstatus_text.text(\"正在生成分析结果...\")\nprogress_bar.progress(70)\n# 生成结果\nresults = []\nfor i, clique in enumerate(cliques):\ntry:\nstatus_text.text(f\"正在处理Alpha {clique[0]}...\")\n# 创建PNL图\nlogger.info(f\"开始为Alpha {clique[0]} 创建PNL图\")\npnl_fig = analyzer.create_pnl_plot(clique)\n# 创建年度指标表（带进度显示）\nlogger.info(f\"开始为Alpha {clique[0]} 获取年度指标数据\")\nannual_table = analyzer.create_annual_metrics_table(clique, progress_callback=update_progress)\nresults.append({\n'clique': clique,\n'pnl_figure': pnl_fig,\n'annual_metrics': annual_table\n})\nlogger.info(f\"Alpha {clique[0]} 处理完成\")\nexcept Exception as e:\nlogger.error(f\"处理Alpha {clique[0]} 时发生错误: {e}\")\nst.error(f\"❌ Alpha {clique[0]} 处理失败: {str(e)}\")\ncontinue\nstatus_text.text(\"分析完成！\")\nprogress_bar.progress(100)\n# 清除进度显示\nprogress_bar.empty()\nstatus_text.empty()\nif not results:\nst.warning(\"⚠️ 未找到有效的分析结果\")\nreturn\nelse:\nst.success(f\"✅ 分析完成！发现 {len(results)} 个团\")\n# 显示结果\nfor i, result in enumerate(results):\nwith st.expander(f\"📊 团 {i+1}: {', '.join(result['clique'])} ({len(result['clique'])} 个Alpha)\", expanded=True):\n# 团信息\ncol1, col2, col3 = st.columns(3)\nwith col1:\nst.metric(\"团大小\", len(result['clique']))\nwith col2:\nst.metric(\"Alpha数量\", len([a for a in result['clique'] if a]))\nwith col3:\nannual_years = len(result['annual_metrics']['Year'].unique()) if not result['annual_metrics'].empty else 0\nst.metric(\"覆盖年份\", annual_years)\n# PNL图表\nst.subheader(\"📈 PNL曲线图\")\nst.info(\"💡 鼠标悬停到PNL曲线上可查看该Alpha的详细年度统计数据\")\n# 显示图表\nst.plotly_chart(\nresult['pnl_figure'],\nuse_container_width=True,\nkey=f\"pnl_chart_{i}\"\n)\n# 年度指标表\nif not result['annual_metrics'].empty:\nst.subheader(\"📅 年度指标统计\")\n# 添加筛选选项\ncol1, col2 = st.columns(2)\nwith col1:\nselected_years = st.multiselect(\n\"选择年份\",\noptions=sorted(result['annual_metrics']['Year'].unique(), reverse=True),\ndefault=sorted(result['annual_metrics']['Year'].unique(), reverse=True),\nkey=f\"years_{i}\"\n)\nwith col2:\nselected_alphas = st.multiselect(\n\"选择Alpha\",\noptions=result['annual_metrics']['Alpha'].unique(),\ndefault=result['annual_metrics']['Alpha'].unique(),\nkey=f\"alphas_{i}\"\n)\n# 筛选数据\nfiltered_data = result['annual_metrics'][\n(result['annual_metrics']['Year'].isin(selected_years)) &\n(result['annual_metrics']['Alpha'].isin(selected_alphas))\n]\nif not filtered_data.empty:\nst.dataframe(\nfiltered_data,\nuse_container_width=True,\nhide_index=True\n)\n# 下载按钮\ncsv = filtered_data.to_csv(index=False)\nst.download_button(\nlabel=\"📥 下载年度指标数据\",\ndata=csv,\nfile_name=f\"annual_metrics_clique_{i+1}.csv\",\nmime=\"text/csv\",\nkey=f\"download_{i}\"\n)\nelse:\nst.info(\"📝 根据筛选条件未找到数据\")\nelse:\nst.info(\"📝 暂无年度指标数据\")\nst.markdown(\"---\")\nexcept Exception as e:\nst.error(f\"❌ 分析过程中出现错误: {str(e)}\")\nlogger.error(f\"分析错误: {e}\")\n# 显示错误详情（仅在调试模式下）\nif st.checkbox(\"显示错误详情\", key=\"show_error_details\"):\nst.code(str(e))\nelif analyze_button and not alpha_input:\nst.warning(\"⚠️ 请输入Alpha ID\")\n# 页脚信息\nst.markdown(\"---\")\nst.markdown(\"\"\"\n<div style='text-align: center; color: gray;'>\n<small>Maximal Clique Alpha Analyzer v1.0 | 基于图论最大团算法的Alpha相关性分析系统</small>\n</div>\n\"\"\", unsafe_allow_html=True)\nif __name__ == \"__main__\":\n# 配置数据源：True=从服务器获取，False=从数据库获取\nUSE_SERVER_DATA = False  # 修改此值来切换数据源\nmain(use_server_data=USE_SERVER_DATA)",
    "太强了，等待观摩代码\n#========= WORLDQUANT BRAIN CONSULTANT ========== #\n# Alpha∞ Engine Status: ONLINE [♦♦♦♦♦♦♦♦♦♦] 100%\n# sys.setrecursionlimit(α∞)\n# PnL = ∑(Robustness * Creativity)\n#无限探索、鲁棒性优先，创新性增值\n#=================奋进的小徐=======================#",
    "来吧，作者快点分享代码，看着很期待，点赞",
    "大佬求代码",
    "感谢大佬的分享，迫不及待的想试下，请问代码还没发布吗？",
    "大佬代码评论区没呐",
    "大佬，请问数据库源这边怎么处理呢？可以提供一下建库建表的sql脚本吗？",
    "@\nJA92987\n数据源这边建议平时定期下载PNL, 主要是获取PNL这边会稍微慢一点,PNL的数据量比较大,服务器限制也多一点.获取year_status的每年数据, 一般是1-5秒下载一个.所以最后year_status的数据我没有保存到数据库.我这边测试的是,在PNL和year_status都从服务器下载的情况下, 100多个alpha进行计算与绘图, 基本上95%以上的时间花在获取数据上,是需要几分钟到十几分钟的时间不等的.\nPNL数据库建表语句如下:\nCREATE TABLE alpha_pnl (\nalpha_id VARCHAR(255) PRIMARY KEY,\npnl_data JSON,\nregion VARCHAR(50),\ncreate_time DATETIME DEFAULT CURRENT_TIMESTAMP,\nupdate_time DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n);",
    "太厉害了",
    "=========================================MY27687====================================\n感谢大佬分享，很不错的工具 ，之前自己简单实现了一个最大团算法，大佬的工具更加齐全，更加优秀，为大佬点赞，祝大佬vf 高高 base多多！！！！\n============================每天提交一个好的alpha======================================"
  ]
}