{
  "title": "【新人科普，老人必读】最全Product Correlation攻略，理解PC与顾问收入的密切关系",
  "description": "引言：为什么我们要关心 PC？ 降低 PC 是提升收益和保证账号健康的最重要因素之一。   PC的高低直接影响个人短期（base payment)和长期收入 (quarterly payment),   PC的高低间接影响Value Factor，从而再次影响收入  ...",
  "post_body": "引言：为什么我们要关心 PC？\n降低 PC 是提升收益和保证账号健康的最重要因素之一\n。\nPC的高低直接影响个人短期（base payment)和长期收入 (quarterly payment),\nPC的高低间接影响Value Factor，从而再次影响收入\n高PC alpha 无法入库，无法参与比赛，长期提交大量高pc alpha，可能会导致账户封禁\n一、理解Product Correlation\n什么是 Product Correlation（PC）？\nPC 衡量一个 alpha 与平台中其他 顾问提交的alpha 的相关性，其计算公式和自相关类似，为最近四年PNL 每日变化（diff）的皮尔逊相关性系数。\nPC高低与收入的关系\n顾问提交的alpha被平台接收后，会进行多次筛选以决定是否对平台有用，其中最重要的筛选就是PC。当PC大于0.7，则被认为是相同/及其相近的alpha，则该alpha不会被平台采用，因此也不会有机会获得任何weight。在顾问协议中披露的每日base payment 计算公式中也明确提到与base payment与PC的负相关性。通常来说，当pc< 0.5 会被认为是比较独特的因子，从而会获得更高的base payment，也更有机会被基金经理采纳而获得weight，进而获得更高的quarterly payment。\n而更低的PC也意味着更低的SC，组合的correlation 越低，也会进一步提升组合的整体多样性表现，从而获得更好的value factor。从个人经验看，在点塔的时候偶尔提交了更高的pc alpha，相应的该月份的vf就会受到影响。不知道平台在计算vf的时候是否对高pc有额外的惩罚系数。\n举例说明PC高低与收入的实证\nAlpha 1： PC 0.49的SA，获得了58.97的 base payment\nAlpha 2：SAC 期间的一个表现更好的alpha，PC 0.68，只获得了9.69 USD的收入。而7月14日一个0.73的pc 只获得了5.4USD的收入。对不起当时0.9+的VF\n二、如何检查PC并降低PC的实操\n获取PC的代码：论坛有很多，部分同学反馈没有权限，已更新在评论区\nhttps://support.worldquantbrain.com/hc/zh-cn/community/posts/33601626915351-%E7%9B%B4%E6%8E%A5%E8%8E%B7%E5%8F%96prod-correlation%E7%9A%84python%E4%BB%A3%E7%A0%81\n几个注意事项：\n不建议直接只用check submission的接口，会更慢，因为要检查其他的测试比如自相关，而自相关等检测都可以在本地完成，效率更高\n该接口也会限流，甚至影响提交alpha，建议做一些在本地的限流，这样不触发平台限流，从而获得整体的结果最优（代码如下，可以自己调整睡眠时间）\n极少数情况会pc通过但是提交失败，这是因为pc是用的缓存数据，在pc检查和提交之间如果别人交了类似的会在提交时候出现pc不过的情况，但这种情况极少极少。\ns = login()\nstart_time = datetime.now()\npass_pc_ids = []\nlast_request_time = datetime.now()\nmax_sleep_time = 60 * 60 # 最大睡眠时间1小时\ncurrent_sleep_time = 60 # 初始睡眠时间60秒\n定义重连时间间隔\nreconnect_interval = timedelta(hours=3.5)\nfor id in batch_ids:\n# 检查是否需要重新登录\ncurrent_time = datetime.now()\nif current_time - start_time >= reconnect_interval:\ns = login()\nstart_time = current_time\n# 记录请求开始时间\nrequest_start_time = datetime.now()\n# 调用 get_prod_corr\npc = get_prod_corr(s, id)\n# 记录请求结束时间\nrequest_end_time = datetime.now()\nrequest_duration = (request_end_time - request_start_time).total_seconds()\n# 检查相关性\nif pc > 0.7:\nset_alpha_properties(s,id,name_value,selection_desc=name_value,combo_desc=name_value,tags = ['PROD Correlation'])\nelif pc >0.5:\nset_alpha_properties(s,id,name_value,selection_desc=name_value,combo_desc=name_value,tags = ['ace_tag'])\nelse:\nresult = get_simulation_result_json(s,id)\nselection = result['selection']['code']\ntag_value = ['PC 0.5','Ready to Submit']\nname_value = result['name']\nif \"own\" in selection:\ncolor = 'GREEN'\ncount = count+1\nelse:\ncolor = 'None'\ncombo = result['combo']['code']\nwhile len(selection)<100:\nselection = selection + selection\nwhile len(combo)<100:\ncombo = combo + combo set_alpha_properties(s,id,name_value,selection_desc=selection,combo_desc=combo,tags=tag_value,color=color)\npass_pc_ids.append(id)\nprint(id,pc)\n# 计算上次请求到本次请求的时间间隔\ntime_since_last_request = (request_start_time - last_request_time).total_seconds()\n# 如果请求时间明显增加，调整睡眠时间\nif request_duration > current_sleep_time * 1.5:\ncurrent_sleep_time = min(max_sleep_time, current_sleep_time * 2)\nelse:\ncurrent_sleep_time = 60 # 如果请求时间正常，恢复默认睡眠时间\n# 更新上次请求时间\nlast_request_time = request_start_time\n# 等待当前睡眠时间\ntime.sleep(current_sleep_time)\nprint(len(pass_pc_ids))\n三、如何降低 PC？\nGlobal 论坛有一篇外区的帖子，有些内容是很有可取之处的\nReduce Production Correlations. – WorldQuant BRAIN\n避免使用过于常见的 signal\n如单纯的暴力一二三阶，没有任何自己的迭代\n增加创新表达方式\n多尝试非线性组合、二阶交叉项、相对排名等。使用sigmoid，sign power，ts linear window等做一些数学变换\n尝试使用target tvr的几个operator来降低tvr的同时也会变化pnl从而降低pc （但有时会反而提高，因为别人也使用了该方法提交过）\n尝试多种region，unvierse和netralization 方式\n一些高pc的因子，在变换了中性化，unvierse和regioin后会有意想不到的收获，而这个可以代码工程化完成\n多尝试新的数据集\n当有新的数据集，新的unvnierse和新的region的时候是pc最低的时候，如TOPDIV3000，最近新上的数据集等\n使用独特的分组方式\n最常见的是group datafield （如other455），bucket分组\n当然王牌还是有自己的模版和自制数据\n如何快速总结已经提交alpha的PC：\n在alphas菜单中点击submitted，选中select column 勾选product correlation，已经提交alpha的PC一目了然\n最后祝大家新的赛季VF和Genius双高！如果对你有用还请点赞评论！",
  "post_comments": [
    "很有帮助",
    "使用独特的分组这个，感觉有时候还受coverage影响，隐含的把原来的universe和coverage取了一个交集",
    "获取PC代码的帖子没有权限看耶",
    "--------------------------------------------------------------------------------------------------------------------------\n哇哦！！！老顾问表示：“学到了！”\n--------------------------------------------------------------------------------------------------------------------------",
    "右上角搜索框  语言调成中文    太多了",
    "大佬所说的“\n多尝试非线性组合、二阶交叉项、相对排名等。使用sigmoid，sign power，ts linear window等做一些数学变换\n尝试使用target tvr的几个operator来降低tvr的同时也会变化pnl从而降低pc （但有时会反而提高，因为别人也使用了该方法提交过）\n”\n确实时需要一些对操作符的理解，也需要去lab上去查看字段的数据分布情况，才能针对的起到效果，更多的是经验的积累吧，这一点我还差的很远，后续会持续在这个方向努力！\n期待大佬更多的分享！\n==========路漫漫其修远兮，吾将上下而求索==========",
    "深有同感，感觉0.7 -- 0.5 -- 0.3是一个门槛，收入有显著差别\n===================================================================================\n===================Talk is cheap,show me the alpha======================================",
    "感谢楼主分享，要想高收入还是要低prod,高theme\n====================================================================================\n==================纸上得来终觉浅，绝知此事要躬行======================================",
    "#  根据alpha ID 获取 prod correlation\ndef get_prod_corr(session, alpha_id):\nresponse = session.get(f\"https://api.worldquantbrain.com/alphas/{alpha_id}/correlations/prod\")\nif response.status_code == 200 and \"records\" in response.json():\ncolumns = [dct[\"name\"] for dct in response.json()[\"schema\"][\"properties\"]]\nself_corr_df = pd.DataFrame(response.json()[\"records\"], columns=columns)\nif not self_corr_df.empty:\nprint(f'{alpha_id} max: {response.json()[\"max\"]} min: {response.json()[\"min\"]}')\nset_alpha_desc(session, alpha_id, f' max: {response.json()[\"max\"]} min: {response.json()[\"min\"]}')\nreturn self_corr_df\nelse:\nreturn pd.DataFrame(columns=[\"correlation\"])\n# 因为经常第一次获取不到内容，可以调用这个函数来多次获取\ndef get_prod_corr_waiting(session, alpha_id, max_times=3):\nretry_count = 0\nwhile retry_count < max_times:\ntry:\nself_corr_df = get_prod_corr(session, alpha_id)\nif not self_corr_df.empty:\nreturn self_corr_df\nexcept json.JSONDecodeError:\npass\nretry_count += 1\ntime.sleep(60)  # Wait for 60 second before the next retry\nprint(retry_count)\nreturn pd.DataFrame(columns=[\"correlation\"])",
    "对新人很有帮助，平时不太注意pc",
    "感谢分享，确实不错的帖子，仅从科普的角度而言\n本地化自检效率确实很高，不过要看电脑配置和提交的alpha数量，\n我之前提交到400个alpha的时候，每次自检也要花费不少时间，\n不过，这跟电脑配置关系很大，而且10年数据也比5年数据更费资源。\n其实我觉得，控制好自己的手就好，不要点的太频繁，基本上可行的，\n难道你的因子已经多到，需要批量处理了吗？\n还是说平时没有时间处理，集中处理一下？\n如果每提交一次，自检一次，这个频率你的电脑是否能够扛得住？\n如果真能扛得住，不考虑PC的情况，可以采用最大团法，\n先算出来哪个先交，哪个后交，能更大程度上提交因子数量。\n最大团法是什么？Kimi一下，含代码\n----------------------------------------------------------------------------------------------\n凡是发生，皆利于我；愿我所愿，尽是美好\n----------------------------------------------------------------------------------------------",
    "感谢大佬分享，“长期提交大量高pc alpha，可能会导致账户封禁”居然是会封账户，之前都不知道，学到了\n=============================================================================\nThe best time to plant a tree is 20 years ago. The second-best time is now.\n=============================================================================",
    "LR93609\n难得有价值的留言。一些comments\n1. 计算本地自相关不需要10年，只保留截取最近四年即可\n2. 没必要对400个做自相关，只对该region 和delay 下的做计算即可，我的本地mac是8G，大概3-5分钟可以完成100个自相关，还是很快的\n3. pnl可以存在本地csv或者部署数据库，我这个月刚部署了数据库，有新的alpha提交了以后就会对待提交的库更新一次SC的值\n4. 难道你的因子已经多到，需要批量处理了吗？有300+ ready to submit。。。 但只是纸面上过了平台测试和自己的一些stats 限制的测试，提交还是都手动看的。那个最大团毛老师讲过，不缺数量的情况下，对portfolio不是最优解",
    "oth455分组感觉没跑出来过对1阶非oth455做主信号时的alpha的有效提升，我跑了1个月就把这些从二阶中停了，楼主介意分享一下这些分组的有效优化率吗？",
    "感谢大佬分享，我现在也确实对pro corr重视起来了，但是一开始确实有点不适应，但是看了大佬的方法后，豁然开朗了，如果每提交一次，自检一次，这个频率很容易会限流。\n还有学到了0.7 -- 0.5 -- 0.3，pc越低就收入就越高的关系，我之前还是只要小于0.7就交，完全没有想到这一点\n感谢大佬\n=============================================================================\n一分耕耘，一分收获\n=============================================================================",
    "第三部分总结了降低因子相关性的一些实用经验，整体思路值得借鉴，强调避免重复使用过于常见的线性或低阶表达式，而应通过非线性组合、交叉项和数学变换来提升因子的独特性，这是减少“同质化”因子的关键。同时提醒利用不同的region、universe和中性化方式，可以在相同信号的基础上获得差异化表现，这对于降低production correlation很有帮助，引入新数据集和独特分组方式的建议，也体现了在数据维度上寻求突破的重要性。作者如果能够分享一些自己降低pc的实际例子就更好了，比如微调哪个部分降低pc最有效，可以截图一些实际调整的例子出来。",
    "set_alpha_desc这个函数内容是啥？"
  ]
}